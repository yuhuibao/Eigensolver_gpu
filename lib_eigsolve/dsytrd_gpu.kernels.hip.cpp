// This file was generated by gpufort

#include "hip/hip_complex.h"
#include "hip/hip_runtime.h"
#include "hip/math_functions.h"
#include "hip/device_functions.h"
#include <cstdio>

namespace {
// make float
float make_float(short int a) { return static_cast<float>(a); }
float make_float(unsigned short int a) { return static_cast<float>(a); }
float make_float(unsigned int a) { return static_cast<float>(a); }
float make_float(int a) { return static_cast<float>(a); }
float make_float(long int a) { return static_cast<float>(a); }
float make_float(unsigned long int a) { return static_cast<float>(a); }
float make_float(long long int a) { return static_cast<float>(a); }
float make_float(unsigned long long int a) { return static_cast<float>(a); }
float make_float(signed char a) { return static_cast<float>(a); }
float make_float(unsigned char a) { return static_cast<float>(a); }
float make_float(float a) { return static_cast<float>(a); }
float make_float(double a) { return static_cast<float>(a); }
float make_float(long double a) { return static_cast<float>(a); }
float make_float(hipFloatComplex &a) { return static_cast<float>(a.x); }
float make_float(hipDoubleComplex &a) { return static_cast<float>(a.x); }
// make double
double make_double(short int a) { return static_cast<double>(a); }
double make_double(unsigned short int a) { return static_cast<double>(a); }
double make_double(unsigned int a) { return static_cast<double>(a); }
double make_double(int a) { return static_cast<double>(a); }
double make_double(long int a) { return static_cast<double>(a); }
double make_double(unsigned long int a) { return static_cast<double>(a); }
double make_double(long long int a) { return static_cast<double>(a); }
double make_double(unsigned long long int a) { return static_cast<double>(a); }
double make_double(signed char a) { return static_cast<double>(a); }
double make_double(unsigned char a) { return static_cast<double>(a); }
double make_double(float a) { return static_cast<double>(a); }
double make_double(double a) { return static_cast<double>(a); }
double make_double(long double a) { return static_cast<double>(a); }
double make_double(hipFloatComplex &a) { return static_cast<double>(a.x); }
double make_double(hipDoubleComplex &a) { return static_cast<double>(a.x); }
// conjugate complex type
hipFloatComplex conj(hipFloatComplex &c) { return hipConjf(c); }
hipDoubleComplex conj(hipDoubleComplex &z) { return hipConj(z); }

// TODO Add the following functions:
// - sign(x,y) = sign(y) * |x| - sign transfer function
// ...
} // namespace
#define divideAndRoundUp(x, y) ((x) / (y) + ((x) % (y) != 0))

// BEGIN krnl_2b8e8f_0
/* Fortran original:
      ! kernel do(1)<<<*,*>>>
      do j = 33, N
         !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
         d(j) = A(j, j)
      end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): -1-1-1._ ('-1' means not specified)
// - Threads per block (CUDA): -1-1-1._ ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_2b8e8f_0(int n, double *d, const int d_n1, const int d_lb1,
                              double *a, const int a_n1, const int a_n2,
                              const int a_lb1, const int a_lb2) {
#undef _idx_d
#define _idx_d(a) ((a - (d_lb1)))
#undef _idx_a
#define _idx_a(a, b) ((a - (a_lb1)) + a_n1 * (b - (a_lb2)))

  unsigned int j = 33 + threadIdx.x + blockIdx.x * blockDim.x;
  if ((j <= n)) {
    // !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
    d[_idx_d(j)] = a[_idx_a(j, j)];
  }
}

extern "C" void launch_krnl_2b8e8f_0(dim3 *grid, dim3 *block,
                                     const int sharedMem, hipStream_t stream,
                                     int n, double *d, const int d_n1,
                                     const int d_lb1, double *a, const int a_n1,
                                     const int a_n2, const int a_lb1,
                                     const int a_lb2) {
  hipLaunchKernelGGL((krnl_2b8e8f_0), *grid, *block, sharedMem, stream, n, d,
                     d_n1, d_lb1, a, a_n1, a_n2, a_lb1, a_lb2);
}
extern "C" void launch_krnl_2b8e8f_0_auto(const int sharedMem,
                                          hipStream_t stream, int n, double *d,
                                          const int d_n1, const int d_lb1,
                                          double *a, const int a_n1,
                                          const int a_n2, const int a_lb1,
                                          const int a_lb2) {
  const unsigned int krnl_2b8e8f_0_NX = n;

  const unsigned int krnl_2b8e8f_0_blockX = 256;

  const unsigned int krnl_2b8e8f_0_gridX =
      divideAndRoundUp(krnl_2b8e8f_0_NX, krnl_2b8e8f_0_blockX);

  dim3 grid(krnl_2b8e8f_0_gridX);
  dim3 block(krnl_2b8e8f_0_blockX);
  hipLaunchKernelGGL((krnl_2b8e8f_0), grid, block, sharedMem, stream, n, d,
                     d_n1, d_lb1, a, a_n1, a_n2, a_lb1, a_lb2);
}
// END krnl_2b8e8f_0

// BEGIN krnl_37a79c_1
/* Fortran original:
        ! kernel do(1)<<<*,*>>>
        do k = 1, N - 1
           W(k, iw) = 0.d0
        end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): -1-1-1._ ('-1' means not specified)
// - Threads per block (CUDA): -1-1-1._ ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_37a79c_1(double *w, const int w_n1, const int w_n2,
                              const int w_lb1, const int w_lb2, int n, int iw) {
#undef _idx_w
#define _idx_w(a, b) ((a - (w_lb1)) + w_n1 * (b - (w_lb2)))

  unsigned int k = 1 + threadIdx.x + blockIdx.x * blockDim.x;
  if ((k <= (n - 1))) {
    w[_idx_w(k, iw)] = 0.e0;
  }
}

extern "C" void launch_krnl_37a79c_1(dim3 *grid, dim3 *block,
                                     const int sharedMem, hipStream_t stream,
                                     double *w, const int w_n1, const int w_n2,
                                     const int w_lb1, const int w_lb2, int n,
                                     int iw) {
  hipLaunchKernelGGL((krnl_37a79c_1), *grid, *block, sharedMem, stream, w, w_n1,
                     w_n2, w_lb1, w_lb2, n, iw);
}
extern "C" void launch_krnl_37a79c_1_auto(const int sharedMem,
                                          hipStream_t stream, double *w,
                                          const int w_n1, const int w_n2,
                                          const int w_lb1, const int w_lb2,
                                          int n, int iw) {
  const unsigned int krnl_37a79c_1_NX = n;

  const unsigned int krnl_37a79c_1_blockX = 256;

  const unsigned int krnl_37a79c_1_gridX =
      divideAndRoundUp(krnl_37a79c_1_NX, krnl_37a79c_1_blockX);

  dim3 grid(krnl_37a79c_1_gridX);
  dim3 block(krnl_37a79c_1_blockX);
  hipLaunchKernelGGL((krnl_37a79c_1), grid, block, sharedMem, stream, w, w_n1,
                     w_n2, w_lb1, w_lb2, n, iw);
}
// END krnl_37a79c_1

// BEGIN dlarfg_kernel
/* Fortran original:
      implicit none
      integer, value                   :: N
      real(8), device                  :: tau
      real(8), device                  :: e
      real(8), dimension(N), device    :: x

      integer                          :: tid, i, j, nb, istat, laneID
      real(8)                          :: rv1, rv2, rv3, scal, scal2, alphar,
   beta, rsum real(8), shared                  :: xnorm real(8), shared ::
   alpha_s

      tid = threadIdx%x
      laneID = iand(tid, 31)

      if (tid == 1) then
         alpha_s = x(N)
         xnorm = 0.0_8
      endif

      call syncthreads()

      alphar = alpha_s
      rsum = 0.0_8

      nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

      i = tid
      do j = 1, nb

         ! All threads perform their product, zero if out of bounds
         if (i <= N - 1) then
            rv1 = x(i)
            rv1 = rv1*rv1
         else
            rv1 = 0.0_8
         endif

         rsum = rsum + rv1

         i = i + blockDim%x
      end do

      ! Partial sum within warps using shuffle
      rv1 = rsum
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      if (laneID == 1) then
         istat = atomicadd(xnorm, rv1)
      endif

      call syncthreads()

      if (xnorm == 0.0_8) then
         if (tid == 1) then
            tau = 0.0_8
         endif
      else
         if (tid == 1) then
            xnorm = sqrt(xnorm)
            rv1 = abs(alphar)

            ! not taking abs of xnorm
            scal = max(rv1, xnorm)
            scal2 = min(rv1, xnorm)

            if (scal2 .eq. 0.0d0) then
               beta = -sign(scal, alphar)
            else
               beta = -sign(scal*sqrt(1.0d0 + (scal2/scal)**2), alphar)
            endif

            tau = (beta - alphar)/beta

            e = beta ! store beta in e vector
            alpha_s = 1.d0/(alphar - beta) !scaling factor for dscal
         endif

         call syncthreads()

         do i = tid, N, blockDim%x

            if (i <= N - 1) then
               x(i) = alpha_s*x(i)
            elseif (i == N) then
               x(i) = 1.0_8
            endif

         end do

      endif


*/

__global__ void dlarfg_kernel(int n, double tau, double e, double *x,
                              const int x_n1, const int x_lb1) {
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))

  int tid;
  int i;
  int j;
  int nb;
  int istat;
  int laneid;
  double rv1;
  double rv2;
  double rv3;
  double scal;
  double scal2;
  double alphar;
  double beta;
  double rsum;
  // ! TODO could not parse:        real(8), shared                  :: xnorm
  // ! TODO could not parse:        real(8), shared                  :: alpha_s
  __shared__ double xnorm;
  __shared__ double alpha_s;
  tid = threadIdx.x + 1;
  laneid = tid & 31;
  if (tid == 1) {
    alpha_s = x[_idx_x(n)];
    xnorm = 0.0 /*_8*/;
  }
  __syncthreads();
  alphar = alpha_s;
  rsum = 0.0 /*_8*/;
  nb = ceil((float(n) / blockDim.x));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= (n - 1))) {
      rv1 = x[_idx_x(i)];
      rv1 = (rv1 * rv1);

    } else {
      rv1 = 0.0 /*_8*/;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if (laneid == 1) {
    istat = atomicAdd(&xnorm, rv1);
  }
  __syncthreads();
  if (xnorm == 0.0 /*_8*/) {
    if (tid == 0) {
      tau = 0.0 /*_8*/;
    }
  } else {
    if (tid == 1) {
      xnorm = sqrt(xnorm);
      rv1 = abs(alphar);
      // ! not taking abs of xnorm
      scal = max(rv1, xnorm);
      scal2 = min(rv1, xnorm);
      if (scal2 == 0.0e0) {
        if (alphar >= 0) {
          beta = -abs(scal);
        } else {

          beta = abs(scal);
        }

      } else {
        if (alphar >= 0) {
          beta = -abs(scal * sqrt(pow((1.0e0 + (scal2 / scal)), 2)));
        } else {

          beta = abs(scal * sqrt(pow((1.0e0 + (scal2 / scal)), 2)));
        }
      }
      tau = ((beta - alphar) / beta);
      e = beta;
      // ! store beta in e vector
      alpha_s = (1.e0 / (alphar - beta));
      // !scaling factor for dscal
    }

    __syncthreads();
    for (int i = tid; i <= n; i += blockDim.x) {
      if ((i <= (n - 1))) {
        x[_idx_x(i)] = (alpha_s * x[_idx_x(i)]);

      } else if (i == n) {
        x[_idx_x(i)] = 1.0 /*_8*/;
      }

    } // ! TODO could not parse:        endif
  }
}

extern "C" void launch_dlarfg_kernel(dim3 *grid, dim3 *block,
                                     const int sharedMem, hipStream_t stream,
                                     int n, double* tau, double *e, double *x,
                                     const int x_n1, const int x_lb1) {
  hipLaunchKernelGGL((dlarfg_kernel), *grid, *block, sharedMem, stream, n, *tau,
                     *e, x, x_n1, x_lb1);
}
// END dlarfg_kernel

// BEGIN dsyr2_mv_dlarfg_kernel
/* Fortran original:
      implicit none
      integer, value                                      :: N, M, ldv, ldw,
   ldw2 real(8), dimension(1:ldv, 1:M), device, intent(in)  :: V real(8),
   dimension(1:ldw, 1:M), device, intent(in)  :: W real(8), dimension(1:ldw2,
   2), device               :: W2 real(8), dimension(1:N), device :: x real(8),
   device                                     :: tau real(8), device :: e

      integer                                             :: i, j, tx, ty, tid,
   nb, laneid, istat, nBlocks integer, device :: finished integer, shared ::
   nFinished real(8)                                             :: rv real(8)
   :: rv1, rv2, rv3, scal, scal2, alphar, beta, rsum real(8), shared :: xnorm
      real(8), shared                                     :: alpha_s

      tx = threadIdx%x
      ty = threadIdx%y
      i = (blockIdx%x - 1)*blockDim%x + tx
      j = (blockIdx%y - 1)*blockDim%y + ty

      nBlocks = gridDim%x*gridDim%y

      if (i <= N .and. j <= M) then

         rv = -W(N, j)*V(i, j) - V(N, j)*W(i, j)

         ! Update x
         istat = atomicadd(x(i), rv)
      endif

      if (ty == 1) then
         ! Zero out column for dgemv call
         if (i <= N) W2(i, 1) = 0
         ! Zero out workspace for intermediate dgemv results
         if (i <= M) then
            W2(N + i, 1) = 0
            W2(N + i, 2) = 0
         endif
      endif

      call threadfence()

      nFinished = 0
      call syncthreads()
      if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)
      call syncthreads()

      if (nFinished < nBlocks - 1) return

      ! Begin dlarfg work with last block
      if (N == 1) return

      tid = tx + (ty - 1)*blockDim%x
      laneID = iand(tid, 31)

      if (tid == 1) then
         alpha_s = x(N - 1)
         xnorm = 0.0_8
      endif

      call syncthreads()

      alphar = alpha_s
      rsum = 0.0_8

      nb = ceiling(real(N - 1)/blockDim%x*blockDim%y) ! number of blocks down
   column

      i = tid
      do j = 1, nb

         ! All threads perform their product, zero if out of bounds
         if (i <= N - 2) then
            rv1 = x(i)
            rv1 = rv1*rv1
         else
            rv1 = 0.0_8
         endif

         rsum = rsum + rv1

         i = i + blockDim%x*blockDim%y
      end do

      ! Partial sum within warps using shuffle
      rv1 = rsum
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      if (laneID == 1) then
         istat = atomicadd(xnorm, rv1)
      endif

      call syncthreads()

      if (xnorm == 0.0_8) then
         if (tid == 1) then
            tau = 0.0_8
         endif
      else
         if (tid == 1) then
            xnorm = sqrt(xnorm)
            rv1 = abs(alphar)

            ! not taking abs of xnorm
            scal = max(rv1, xnorm)
            scal2 = min(rv1, xnorm)

            if (scal2 .eq. 0.0d0) then
               beta = -sign(scal, alphar)
            else
               beta = -sign(scal*sqrt(1.0d0 + (scal2/scal)**2), alphar)
            endif

            tau = (beta - alphar)/beta

            e = beta ! store beta in e vector
            alpha_s = 1.d0/(alphar - beta) !scaling factor for dscal
         endif

         call syncthreads()

         do i = tid, N - 1, blockDim%x*blockDim%y

            if (i <= N - 2) then
               x(i) = alpha_s*x(i)
            elseif (i == N - 1) then
               x(i) = 1.0_8
            endif

         end do

      endif


*/

__global__ void dsyr2_mv_dlarfg_kernel(
    int n, int m, int ldv, int ldw, int ldw2, double *v, const int v_n1,
    const int v_lb1, const int v_lb2, double *w, const int w_n1,
    const int w_lb1, const int w_lb2, double *w2,
    const int w2_n1, const int w2_lb1, const int w2_lb2,
    double *x, const int x_n1, const int x_lb1, double tau, double e,
    unsigned int finished) {
#undef _idx_v
#define _idx_v(a, b) ((a - (v_lb1)) + v_n1 * (b - (v_lb2)))
#undef _idx_w
#define _idx_w(a, b) ((a - (w_lb1)) + w_n1 * (b - (w_lb2)))
#undef _idx_w2
#define _idx_w2(a, b) ((a - (w2_lb1)) + w2_n1 * (b - (w2_lb2)))
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))

  int i;
  int j;
  int tx;
  int ty;
  int tid;
  int nb;
  int laneid;
  int istat;
  int nblocks;
  __shared__ int nfinished;
  double rv;
  double rv1;
  double rv2;
  double rv3;
  double scal;
  double scal2;
  double alphar;
  double beta;
  double rsum;
  __shared__ double xnorm;
  __shared__ double alpha_s;
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = ((blockIdx.x) * blockDim.x + tx);
  j = ((blockIdx.y) * blockDim.y + ty);
  nblocks = (gridDim.x * gridDim.y);
  if ((i <= n & j <= m)) {
    rv = (-w[_idx_w(n, j)] * v[_idx_v(i, j)] -
          v[_idx_v(n, j)] * w[_idx_w(i, j)]);
    // ! Update x
    istat = atomicAdd(x+_idx_x(i)*8, rv);
  }
  if (ty == 1) {
    // ! Zero out column for dgemv call
    if ((i <= n)) {
      w2[_idx_w2(i, 1)] = 0;

    } // ! Zero out workspace for intermediate dgemv results
    if ((i <= m)) {
      w2[_idx_w2((n + i), 1)] = 0;
      w2[_idx_w2((n + i), 2)] = 0;
    }
  }
  __threadfence();
  nfinished = 0;
  __syncthreads();
  if ((tx + ty) == 2) {
    nfinished = atomicInc(&finished, (nblocks - 1));
  }
  __syncthreads();
  if ((nfinished < (nblocks - 1))) {
    return; // ! Begin dlarfg work with last block
  }
  if (n == 1) {
    return;
  }
  tid = (tx + (ty - 1) * blockDim.x);
  laneid = tid & 31;
  if (tid == 1) {
    alpha_s = x[_idx_x((n - 1))];
    xnorm = 0.0 /*_8*/;
  }
  __syncthreads();
  alphar = alpha_s;
  rsum = 0.0 /*_8*/;
  nb = ceil((float((n - 1)) / blockDim.x * blockDim.y));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= (n - 2))) {
      rv1 = x[_idx_x(i)];
      rv1 = (rv1 * rv1);

    } else {
      rv1 = 0.0 /*_8*/;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x * blockDim.y);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if (laneid == 1) {
    istat = atomicAdd(&xnorm, rv1);
  }
  __syncthreads();
  if (xnorm == 0.0 /*_8*/) {
    if (tid == 1) {
      tau = 0.0 /*_8*/;
    }
  } else {
    if (tid == 1) {
      xnorm = sqrt(xnorm);
      rv1 = abs(alphar);
      // ! not taking abs of xnorm
      scal = max(rv1, xnorm);
      scal2 = min(rv1, xnorm);
      if (scal2 == 0.0e0) {
        if (alphar >= 0) {
          beta = -abs(scal);
        } else {

          beta = abs(scal);
        }

      } else {
        if (alphar >= 0) {
          beta = -abs(scal * sqrt(pow((1.0e0 + (scal2 / scal)), 2)));
        } else {

          beta = abs(scal * sqrt(pow((1.0e0 + (scal2 / scal)), 2)));
        }
      }
      tau = ((beta - alphar) / beta);
      e = beta;
      // ! store beta in e vector
      alpha_s = (1.e0 / (alphar - beta));
      // !scaling factor for dscal
    }
    __syncthreads();
    for (int i = tid; i <= (n - 1); i += (blockDim.x * blockDim.y)) {
      if ((i <= (n - 2))) {
        x[_idx_x(i)] = (alpha_s * x[_idx_x(i)]);

      } else if (i == (n - 1)) {
        x[_idx_x(i)] = 1.0 /*_8*/;
      }
    }
  }
}

extern "C" void launch_dsyr2_mv_dlarfg_kernel(
    dim3 *grid, dim3 *block, const int sharedMem, hipStream_t stream, int n,
    int m, int ldv, int ldw, int ldw2, double *v, const int v_n1,
    const int v_lb1, const int v_lb2, double *w, const int w_n1,
    const int w_lb1, const int w_lb2, double *w2,
    const int w2_n1, const int w2_lb1, const int w2_lb2,
    double *x, const int x_n1, const int x_lb1, double *tau, double *e,
    int *finished) {
  hipLaunchKernelGGL((dsyr2_mv_dlarfg_kernel), *grid, *block, sharedMem, stream,
                     n, m, ldv, ldw, ldw2, v, v_n1, v_lb1, v_lb2, w, w_n1,
                     w_lb1, w_lb2, w2, w2_n1, w2_lb1, w2_lb2, x,
                     x_n1, x_lb1, *tau, *e, *finished);
}
// END dsyr2_mv_dlarfg_kernel

// BEGIN stacked_dgemv_t
/* Fortran original:
      use cudafor
      implicit none
      integer, value                                  :: M, N, ldv, ldw
      real(8), dimension(ldv, M), device, intent(in)  :: V
      real(8), dimension(ldw, M), device, intent(in)  :: W
      real(8), dimension(N), device, intent(in)       :: x
      real(8), dimension(M), device                   :: z1, z2
      !complex(8), dimension(M), device, intent(in)        :: z1, z2

      !real(8), dimension(32), shared                     :: r_s
      !real(8), dimension(32), shared                     :: i_s

      integer :: i, j, tx, ty, istat
      real(8) :: rv1, rv2, xr

      tx = threadIdx%x
      ty = threadIdx%y

      i = (blockIdx%y - 1)*blockDim%y + ty
      j = (blockIdx%x - 1)*blockDim%x + tx

      !if (i > 2*M .or. j > N) return
      if (i > 2*M) return

      xr = x(j)

      if (j > N) then
         rv1 = 0.d0
      else
         if (i > M) then
            rv2 = W(j, i - M)
         else
            rv2 = V(j, i)
         endif

         rv1 = rv2*xr
      endif

      !Partial sum within warps using shuffle
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      if (tx == 1) then
         if (i > M) then
            istat = atomicadd(z2(i - M), rv1)
         else
            istat = atomicadd(z1(i), rv1)
         endif
      endif

      return

*/

__global__ void stacked_dgemv_t(int m, int n, int ldv, int ldw, double *v,
                                const int v_n1, const int v_lb1,
                                const int v_lb2, double *w, const int w_n1,
                                const int w_lb1,
                                const int w_lb2, double *x, const int x_n1,
                                const int x_lb1, double *z1, const int z1_n1,
                                const int z1_lb1, double *z2, const int z2_n1,
                                const int z2_lb1) {
#undef _idx_v
#define _idx_v(a, b) ((a - (v_lb1)) + v_n1 * (b - (v_lb2)))
#undef _idx_w
#define _idx_w(a, b) ((a - (w_lb1)) + w_n1 * (b - (w_lb2)))
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_z1
#define _idx_z1(a) ((a - (z1_lb1)))
#undef _idx_z2
#define _idx_z2(a) ((a - (z2_lb1)))

  // !complex(8), dimension(M), device, intent(in)        :: z1, z2
  // !real(8), dimension(32), shared                     :: r_s
  // !real(8), dimension(32), shared                     :: i_s
  int i;
  int j;
  int tx;
  int ty;
  int istat;
  double rv1;
  double rv2;
  double xr;
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = (blockIdx.y * blockDim.y + ty);
  j = (blockIdx.x * blockDim.x + tx);
  // !if (i > 2*M .or. j > N) return
  if ((i > (2 * m))) {
    return;
  }
  xr = x[_idx_x(j)];
  if ((j > n)) {
    rv1 = 0.e0;

  } else {
    if ((i > m)) {
      rv2 = w[_idx_w(j, (i - m))];

    } else {
      rv2 = v[_idx_v(j, i)];
    }
    rv1 = (rv2 * xr);
  }
  // ! TODO could not parse:        endif
  // !Partial sum within warps using shuffle
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if (tx == 1) {
    if ((i > m)) {
      istat = atomicAdd(z2+_idx_z2((i - m))*8, rv1);

    } else {
      istat = atomicAdd(z1+_idx_z1(i)*8, rv1);
    }
  }
  return;
}

extern "C" void launch_stacked_dgemv_t(
    dim3 *grid, dim3 *block, const int sharedMem, hipStream_t stream, int m,
    int n, int ldv, int ldw, double *v, const int v_n1, const int v_n2,
    const int v_lb1, const int v_lb2, double *w, const int w_n1,
    const int w_lb1, const int w_lb2, double *x, const int x_n1,
    const int x_lb1, double *z1, const int z1_n1, const int z1_lb1, double *z2,
    const int z2_n1, const int z2_lb1) {
  hipLaunchKernelGGL((stacked_dgemv_t), *grid, *block, sharedMem, stream, m, n,
                     ldv, ldw, v, v_n1, v_lb1, v_lb2, w, w_n1,
                     w_lb1, w_lb2, x, x_n1, x_lb1, z1, z1_n1, z1_lb1, z2, z2_n1,
                     z2_lb1);
}
// END stacked_dgemv_t

// BEGIN finish_w_col_kernel
/* Fortran original:
      implicit none
      integer, value                               :: N
      real(8), device                              :: tau
      real(8), dimension(N), device, intent(in)    :: x
      real(8), dimension(N), device                :: y

      integer                                      :: tid, i, j, k, nb, istat,
   laneID real(8)                                      :: rv1, rv2, rsum, mytau

      real(8), shared                              :: alphar
      !real(8), shared                              :: alpha
      real(8)                                      :: alpha

      tid = threadIdx%x
      laneID = iand(tid, 31)

      if (tid == 1) then
         alphar = 0.0_8
      endif

      call syncthreads()

      rsum = 0.0_8
      mytau = tau

      nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

      i = tid
      do j = 1, nb

         ! All threads perform their product, zero if out of bounds
         if (i <= N) then
            rv1 = mytau*y(i)*x(i)
         else
            rv1 = 0.0d0
         endif

         rsum = rsum + rv1

         i = i + blockDim%x

      end do

      ! Partial sum within warps using shuffle
      rv1 = rsum
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      if (laneID == 1) then
         istat = atomicadd(alphar, rv1)
      endif

      call syncthreads()

      alpha = -0.5d0*mytau*alphar

      do i = tid, N, blockDim%x
         y(i) = mytau*y(i) + alpha*x(i) !daxpy
      end do


*/

__global__ void finish_w_col_kernel(int n, double tau, double *x,
                                    const int x_n1, const int x_lb1, double *y,
                                    const int y_n1, const int y_lb1) {
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_y
#define _idx_y(a) ((a - (y_lb1)))

  int tid;
  int i;
  int j;
  int k;
  int nb;
  int istat;
  int laneid;
  double rv1;
  double rv2;
  double rsum;
  double mytau;
  // ! TODO could not parse:        real(8), shared :: alphar !real(8), shared
  // :: alpha
  __shared__ double alphar;
  double alpha;
  tid = threadIdx.x + 1;
  laneid = tid & 31;
  if (tid == 1) {
    alphar = 0.0 /*_8*/;
  }
  __syncthreads();
  rsum = 0.0 /*_8*/;
  mytau = tau;
  nb = ceil((float(n) / blockDim.x));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= n)) {
      rv1 = (mytau * y[_idx_y(i)] * x[_idx_x(i)]);

    } else {
      rv1 = 0.0e0;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if (laneid == 1) {
    istat = atomicAdd(&alphar, rv1);
  }
  __syncthreads();
  alpha = (-0.5e0 * mytau * alphar);
  for (int i = tid; i <= n; i += blockDim.x) {
    y[_idx_y(i)] = (mytau * y[_idx_y(i)] + alpha * x[_idx_x(i)]);
    // !daxpy
  }
}

extern "C" void
launch_finish_w_col_kernel(dim3 *grid, dim3 *block, const int sharedMem,
                           hipStream_t stream, int n, double *tau, double *x,
                           const int x_n1, const int x_lb1, double *y,
                           const int y_n1, const int y_lb1) {
  hipLaunchKernelGGL((finish_w_col_kernel), *grid, *block, sharedMem, stream, n,
                     *tau, x, x_n1, x_lb1, y, y_n1, y_lb1);
}
// END finish_w_col_kernel

// BEGIN stacked_dgemv_n_finish_w
/* Fortran original:
      use cudafor
      implicit none
      integer, value                                     :: M, N, ldv, ldw
      real(8), dimension(ldv, N), device, intent(in)     :: V
      real(8), dimension(ldw, N), device, intent(in)     :: W
      real(8), dimension(N), device, intent(in)          :: z1, z2
      real(8), dimension(M), device                      :: y
      real(8), device                                    :: tau
      real(8), dimension(M), device, intent(in)          :: x
      integer, device                                    :: finished

      integer :: i, j, tx, ty, istat, nBlocks, tid, laneID, nb
      integer, shared :: nFinished
      real(8) :: rv1, rv2, rsum, xr, mytau
      real(8), shared                              :: alphar
      !real(8), shared                              :: alpha
      real(8)                                      :: alpha

      tx = threadIdx%x
      ty = threadIdx%y

      i = (blockIdx%x - 1)*blockDim%x + tx
      j = (blockIdx%y - 1)*blockDim%y + ty

      nBlocks = gridDim%x*gridDim%y

      if (i <= M .and. j <= 2*N) then

         if (j > N) then
            xr = z2(j - N)
            rv2 = V(i, j - N)
         else
            xr = z1(j)
            rv2 = W(i, j)
         endif

         rv1 = -rv2*xr

         istat = atomicadd(y(i), rv1)
      endif

      call threadfence()

      nFinished = 0
      call syncthreads()
      if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)
      call syncthreads()

      if (nFinished < nBlocks - 1) return

      ! Begin finish_W_col work with last block
      tid = threadIdx%x + (threadIdx%y - 1)*blockDim%x
      laneID = iand(tid, 31)

      if (tid == 1) then
         alphar = 0.0_8
      endif

      call syncthreads()

      rsum = 0.0_8
      mytau = tau

      nb = ceiling(real(M)/(blockDim%x*blockDim%y)) ! number of blocks down
   column

      i = tid
      do j = 1, nb

         ! All threads perform their product, zero if out of bounds
         if (i <= M) then
            rv1 = mytau*y(i)*x(i)
         else
            rv1 = 0.0d0
         endif

         rsum = rsum + rv1

         i = i + blockDim%x*blockDim%y

      end do

      ! Partial sum within warps using shuffle
      rv1 = rsum
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      if (laneID == 1) then
         istat = atomicadd(alphar, rv1)
      endif

      call syncthreads()

      alpha = -0.5d0*mytau*alphar

      do i = tid, M, blockDim%x*blockDim%y
         y(i) = mytau*y(i) + alpha*x(i) !daxpy
      end do


*/

__global__ void stacked_dgemv_n_finish_w(
    int m, int n, int ldv, int ldw, double *v, const int v_n1, 
    const int v_lb1, const int v_lb2, double *w, const int w_n1, 
    const int w_lb1, const int w_lb2, double *z1, const int z1_n1,
    const int z1_lb1, double *z2, const int z2_n1, const int z2_lb1, double *y,
    const int y_n1, const int y_lb1, double tau, double *x, const int x_n1,
    const int x_lb1, unsigned int finished) {
#undef _idx_v
#define _idx_v(a, b) ((a - (v_lb1)) + v_n1 * (b - (v_lb2)))
#undef _idx_w
#define _idx_w(a, b) ((a - (w_lb1)) + w_n1 * (b - (w_lb2)))
#undef _idx_z1
#define _idx_z1(a) ((a - (z1_lb1)))
#undef _idx_z2
#define _idx_z2(a) ((a - (z2_lb1)))
#undef _idx_y
#define _idx_y(a) ((a - (y_lb1)))
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))

  int i;
  int j;
  int tx;
  int ty;
  int istat;
  int nblocks;
  int tid;
  int laneid;
  int nb;
  __shared__ int nfinished;
  double rv1;
  double rv2;
  double rsum;
  double xr;
  double mytau;
  __shared__ double alphar;
  double alpha;
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = (blockIdx.x * blockDim.x + tx);
  j = (blockIdx.y * blockDim.y + ty);
  nblocks = (gridDim.x * gridDim.y);
  if ((i <= m & j <= (2 * n))) {
    if ((j > n)) {
      xr = z2[_idx_z2((j - n))];
      rv2 = v[_idx_v(i, (j - n))];

    } else {
      xr = z1[_idx_z1(j)];
      rv2 = w[_idx_w(i, j)];
    }
    rv1 = (-rv2 * xr);
    istat = atomicAdd(y+_idx_y(i)*8, rv1);
  }
  __threadfence();
  nfinished = 0;
  __syncthreads();
  if ((tx + ty) == 2) {
    nfinished = atomicInc(&finished, (nblocks - 1));
  }
  __syncthreads();
  if ((nfinished < (nblocks - 1))) {
    return;
  }
  tid = (threadIdx.x + (threadIdx.y) * blockDim.x) + 1;
  laneid = tid & 31;
  if (tid == 1) {
    alphar = 0.0 /*_8*/;
  }
  __syncthreads();
  rsum = 0.0 /*_8*/;
  mytau = tau;
  nb = ceil((float(m) / (blockDim.x * blockDim.y)));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= m)) {
      rv1 = (mytau * y[_idx_y(i)] * x[_idx_x(i)]);

    } else {
      rv1 = 0.0e0;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x * blockDim.y);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if (laneid == 1) {
    istat = atomicAdd(&alphar, rv1);
  }
  __syncthreads();
  alpha = (-0.5e0 * mytau * alphar);
  for (int i = tid; i <= m; i += (blockDim.x * blockDim.y)) {
    y[_idx_y(i)] = (mytau * y[_idx_y(i)] + alpha * x[_idx_x(i)]);
    // !daxpy
  }
}

extern "C" void launch_stacked_dgemv_n_finish_w(
    dim3 *grid, dim3 *block, const int sharedMem, hipStream_t stream, int m,
    int n, int ldv, int ldw, double *v, const int v_n1,
    const int v_lb1, const int v_lb2, double *w, const int w_n1,
    const int w_lb1, const int w_lb2, double *z1, const int z1_n1,
    const int z1_lb1, double *z2, const int z2_n1, const int z2_lb1, double *y,
    const int y_n1, const int y_lb1, double *tau, double *x, const int x_n1,
    const int x_lb1, int *finished) {
  hipLaunchKernelGGL((stacked_dgemv_n_finish_w), *grid, *block, sharedMem,
                     stream, m, n, ldv, ldw, v, v_n1,v_lb1, v_lb2, w,
                     w_n1, w_lb1, w_lb2, z1, z1_n1, z1_lb1, z2, z2_n1,
                     z2_lb1, y, y_n1, y_lb1, *tau, x, x_n1, x_lb1, *finished);
}
// END stacked_dgemv_n_finish_w
