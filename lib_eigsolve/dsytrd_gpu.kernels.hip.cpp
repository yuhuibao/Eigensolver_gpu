// This file was generated by gpufort

#include "hip/hip_complex.h"
#include "hip/hip_runtime.h"
#include "hip/math_functions.h"
#include <cstdio>

namespace {
// make float
float make_float(short int a) { return static_cast<float>(a); }
float make_float(unsigned short int a) { return static_cast<float>(a); }
float make_float(unsigned int a) { return static_cast<float>(a); }
float make_float(int a) { return static_cast<float>(a); }
float make_float(long int a) { return static_cast<float>(a); }
float make_float(unsigned long int a) { return static_cast<float>(a); }
float make_float(long long int a) { return static_cast<float>(a); }
float make_float(unsigned long long int a) { return static_cast<float>(a); }
float make_float(signed char a) { return static_cast<float>(a); }
float make_float(unsigned char a) { return static_cast<float>(a); }
float make_float(float a) { return static_cast<float>(a); }
float make_float(double a) { return static_cast<float>(a); }
float make_float(long double a) { return static_cast<float>(a); }
float make_float(hipFloatComplex &a) { return static_cast<float>(a.x); }
float make_float(hipDoubleComplex &a) { return static_cast<float>(a.x); }
// make double
double make_double(short int a) { return static_cast<double>(a); }
double make_double(unsigned short int a) { return static_cast<double>(a); }
double make_double(unsigned int a) { return static_cast<double>(a); }
double make_double(int a) { return static_cast<double>(a); }
double make_double(long int a) { return static_cast<double>(a); }
double make_double(unsigned long int a) { return static_cast<double>(a); }
double make_double(long long int a) { return static_cast<double>(a); }
double make_double(unsigned long long int a) { return static_cast<double>(a); }
double make_double(signed char a) { return static_cast<double>(a); }
double make_double(unsigned char a) { return static_cast<double>(a); }
double make_double(float a) { return static_cast<double>(a); }
double make_double(double a) { return static_cast<double>(a); }
double make_double(long double a) { return static_cast<double>(a); }
double make_double(hipFloatComplex &a) { return static_cast<double>(a.x); }
double make_double(hipDoubleComplex &a) { return static_cast<double>(a.x); }
// conjugate complex type
hipFloatComplex conj(hipFloatComplex &c) { return hipConjf(c); }
hipDoubleComplex conj(hipDoubleComplex &z) { return hipConj(z); }

// TODO Add the following functions:
// - sign(x,y) = sign(y) * |x| - sign transfer function
// ...
} // namespace
#define divideAndRoundUp(x, y) ((x) / (y) + ((x) % (y) != 0))
#undef _idx
#undef _idx_a
#undef _idx_a_s
#define _idx(a) ((a - 1))
#define _idx_a(a, b) ((a - 1) + n * (b - 1))

// BEGIN krnl_2b8e8f_0
/* Fortran original:
      ! kernel do(1) <<<*,*>>>
      do j = 33, N
         !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
         d(j) = A(j, j)
      end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): [-1, -1, -1]. ('-1' means not specified)
// - Threads per block (CUDA): [-1, -1, -1]. ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_2b8e8f_0(double *d, double *a, int n) {

  unsigned int j = 33 + threadIdx.x + blockIdx.x * blockDim.x;
  if ((j <= n)) {
    // !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
    d[_idx(j)] = a[_idx_a(j, j)];
  }
}

extern "C" void launch_krnl_2b8e8f_0(dim3 *grid, dim3 *block,
                                     const int sharedMem, hipStream_t stream,
                                     double *d, double *a, int n) {
  hipLaunchKernelGGL((krnl_2b8e8f_0), *grid, *block, sharedMem, stream, d, a,
                     n);
}
extern "C" void launch_krnl_2b8e8f_0_auto(const int sharedMem,
                                          hipStream_t stream, double *d,
                                          double *a, int n) {
  const unsigned int krnl_2b8e8f_0_NX = n;

  const unsigned int krnl_2b8e8f_0_blockX = 256;

  const unsigned int krnl_2b8e8f_0_gridX =
      divideAndRoundUp(krnl_2b8e8f_0_NX, krnl_2b8e8f_0_blockX);

  dim3 grid(krnl_2b8e8f_0_gridX);
  dim3 block(krnl_2b8e8f_0_blockX);
  hipLaunchKernelGGL((krnl_2b8e8f_0), grid, block, sharedMem, stream, d, a, n);
}
// END krnl_2b8e8f_0

// BEGIN krnl_37a79c_1
/* Fortran original:
        ! kernel do(1) <<<*,*>>>
        do k = 1, N - 1
           W(k, iw) = 0.d0
        end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): [-1, -1, -1]. ('-1' means not specified)
// - Threads per block (CUDA): [-1, -1, -1]. ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_37a79c_1(int n, int iw, double *w) {

  unsigned int k = 1 + threadIdx.x + blockIdx.x * blockDim.x;
  if ((k <= (n - 1))) {
    w[_idx_a(k, iw)] = 0.e0;
  }
}

extern "C" void launch_krnl_37a79c_1(dim3 *grid, dim3 *block,
                                     const int sharedMem, hipStream_t stream,
                                     int n, int iw, double *w) {
  hipLaunchKernelGGL((krnl_37a79c_1), *grid, *block, sharedMem, stream, n, iw,
                     w);
}
extern "C" void launch_krnl_37a79c_1_auto(const int sharedMem,
                                          hipStream_t stream, int n, int iw,
                                          double *w) {
  const unsigned int krnl_37a79c_1_NX = (n - 1);

  const unsigned int krnl_37a79c_1_blockX = 256;

  const unsigned int krnl_37a79c_1_gridX =
      divideAndRoundUp(krnl_37a79c_1_NX, krnl_37a79c_1_blockX);

  dim3 grid(krnl_37a79c_1_gridX);
  dim3 block(krnl_37a79c_1_blockX);
  hipLaunchKernelGGL((krnl_37a79c_1), grid, block, sharedMem, stream, n, iw, w);
}
// END krnl_37a79c_1

// BEGIN dsyr2_mv_kernel
/* Fortran original:
      implicit none

      integer, value                                      :: N, M, ldv, ldw,
   ldw2

      real(8), dimension(1:ldv, 1:M), device, intent(in)  :: V

      real(8), dimension(1:ldw, 1:M), device, intent(in)  :: W

      real(8), dimension(1:ldw2, 2), device               :: W2

      real(8), dimension(1:N), device                     :: x

      integer                                             :: i, j, istat

      real(8)                                             :: rv

      i = (blockIdx%x - 1)*blockDim%x + threadIdx%x

      j = (blockIdx%y - 1)*blockDim%y + threadIdx%y

      if (i <= N .and. j <= M) then

         rv = -W(N, j)*V(i, j) - V(N, j)*W(i, j)

         ! Update x

         istat = atomicadd(x(i), rv)

      endif

      if (threadIdx%y == 1) then

         ! Zero out column for zhemv call

         if (i <= N) W2(i, 1) = 0

         ! Zero out workspace for intermediate zgemv results

         if (i <= M) then

            W2(N + i, 1) = 0

            W2(N + i, 2) = 0

         endif

      endif


*/

__global__ void dsyr2_mv_kernel(int n, int m, double *v, int ldv, double *w,
                                int ldw, double *x, double *w2, int ldw2) {
#undef _idx_v
#undef _idx_w
#undef _idx_w2
#define _idx_v(a, b) ((a - 1) + ldv * (b - 1))
#define _idx_w(a, b) ((a - 1) + ldw * (b - 1))
#define _idx_w2(a, b) ((a - 1) + ldw2 * (b - 1))
  int i;
  int j;
  int istat;
  double rv;

  // ! TODO could not parse:        real(8), dimension(1:ldv, 1:m), device,
  // intent(in)  :: v ! TODO could not parse:        real(8), dimension(1:ldw,
  // 1:m), device, intent(in)  :: w ! TODO could not parse:        real(8),
  // dimension(1:ldw2, 2), device               :: w2 ! TODO could not parse:
  // real(8), dimension(1:n), device                     :: x
  i = (blockIdx.x * blockDim.x + threadIdx.x) + 1;
  j = (blockIdx.y * blockDim.y + threadIdx.y) + 1;
  if ((i <= n & j <= m)) {
    rv = (-w[_idx_w(n, j)] * v[_idx_v(i, j)] -
          v[_idx_v(n, j)] * w[_idx_w(i, j)]);
    // ! Update x
    istat = atomicAdd(x+_idx(i), rv);
  }
  if ((threadIdx.y == 0)) {
    // ! Zero out column for zhemv call
    if ((i <= n)) {
      w2[_idx_w2(i, 1)] = 0;
    }
    // ! Zero out workspace for intermediate zgemv results
    if ((i <= m)) {
      w2[_idx_w2((n + i), 1)] = 0;
      w2[_idx_w2((n + i), 2)] = 0;
    }
  }
}

extern "C" void launch_dsyr2_mv_kernel(dim3 *grid, dim3 *block,
                                       const int sharedMem, hipStream_t stream,
                                       int n, int m, double *v, int ldv,
                                       double *w, int ldw, double *x, double *w2,
                                       int ldw2) {
  hipLaunchKernelGGL((dsyr2_mv_kernel), *grid, *block, sharedMem, stream, n, m,
                     v, ldv, w, ldw, x, w2, ldw2);
}
// END dsyr2_mv_kernel

// BEGIN dlarfg_kernel
/* Fortran original:
      implicit none

      integer, value                   :: N

      real(8), device                  :: tau

      real(8), device                  :: e

      real(8), dimension(N), device    :: x

      integer                          :: tid, i, j, nb, istat, laneID

      real(8)                          :: rv1, rv2, rv3, scal, scal2, alphar,
   beta, rsum

      real(8), shared                  :: xnorm

      real(8), shared                  :: alpha_s

      tid = threadIdx%x

      laneID = iand(tid, 31)

      if (tid == 1) then

         alpha_s = x(N)

         xnorm = 0.0_8

      endif

      call syncthreads()

      alphar = alpha_s

      rsum = 0.0_8

      nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

      i = tid

      do j = 1, nb

         ! All threads perform their product, zero if out of bounds

         if (i <= N - 1) then

            rv1 = x(i)

            rv1 = rv1*rv1

         else

            rv1 = 0.0_8

         endif

         rsum = rsum + rv1

         i = i + blockDim%x

      end do

      ! Partial sum within warps using shuffle

      rv1 = rsum

      rv2 = __shfl_down(rv1, 1)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 2)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 4)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 8)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 16)

      rv1 = rv1 + rv2

      if (laneID == 1) then

         istat = atomicadd(xnorm, rv1)

      endif

      call syncthreads()

      if (xnorm == 0.0_8) then

         if (tid == 1) then

            tau = 0.0_8

         endif

      else

         if (tid == 1) then

            xnorm = sqrt(xnorm)

            rv1 = abs(alphar)

            ! not taking abs of xnorm

            scal = max(rv1, xnorm)

            scal2 = min(rv1, xnorm)

            if (scal2 .eq. 0.0d0) then

               beta = -sign(scal, alphar)

            else

               beta = -sign(scal*sqrt(1.0d0 + (scal2/scal)**2), alphar)

            endif

            tau = (beta - alphar)/beta

            e = beta ! store beta in e vector

            alpha_s = 1.d0/(alphar - beta) !scaling factor for dscal

         endif

         call syncthreads()

         do i = tid, N, blockDim%x

            if (i <= N - 1) then

               x(i) = alpha_s*x(i)

            elseif (i == N) then

               x(i) = 1.0_8

            endif

         end do

      endif


*/

__global__ void dlarfg_kernel(int n, double tau, double e, double *x) {
  int tid;
  int i;
  int j;
  int nb;
  int istat;
  int laneid;
  double rv1;
  double rv2;
  double rv3;
  double scal;
  double scal2;
  double alphar;
  double beta;
  double rsum;
  __shared__ double xnorm;
  __shared__ double alpha_s;

  tid = threadIdx.x + 1;
  laneid = tid & 31;
  if ((tid == 1)) {
    alpha_s = x[_idx(n)];
    xnorm = 0.0 /*_8*/;
  }
  __syncthreads();
  alphar = alpha_s;
  rsum = 0.0 /*_8*/;
  nb = ceil((float(n) / blockDim.x));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= (n - 1))) {
      rv1 = x[_idx(i)];
      rv1 = (rv1 * rv1);

    } else {
      rv1 = 0.0 /*_8*/;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if ((laneid == 1)) {
    istat = atomicAdd(&xnorm, rv1);
  }
  __syncthreads();
  if ((xnorm == 0.0 /*_8*/)) {
    if ((tid == 0)) {
      tau = 0.0 /*_8*/;
    }
  } else {
    if ((tid == 1)) {
      xnorm = sqrt(xnorm);
      rv1 = abs(alphar);
      // ! not taking abs of xnorm
      scal = max(rv1, xnorm);
      scal2 = min(rv1, xnorm);
      if ((scal2 == 0.0e0)) {
        beta = -sign(scal, alphar);

      } else {
        beta = -sign((scal * sqrt(pow((1.0e0 + (scal2 / scal)),2))), alphar);
      }
      tau = ((beta - alphar) / beta);
      e = beta;
      // ! store beta in e vector
      alpha_s = (1.e0 / (alphar - beta));
      // !scaling factor for dscal
    }
    __syncthreads();
    for (int i = tid; i <= n; i += blockDim.x) {
      if ((i <= (n - 1))) {
        x[_idx(i)] = (alpha_s * x[_idx(i)]);

      } else if ((i == n)) {
        x[_idx(i)] = 1.0 /*_8*/;
      }

    } // ! TODO could not parse:        endif
  }
}

extern "C" void launch_dlarfg_kernel(dim3 *grid, dim3 *block,
                                     const int sharedMem, hipStream_t stream,
                                     int n, double tau, double e, double *x) {
  hipLaunchKernelGGL((dlarfg_kernel), *grid, *block, sharedMem, stream, n, tau,
                     e, x);
}
// END dlarfg_kernel

// BEGIN dsyr2_mv_dlarfg_kernel
/* Fortran original:
      implicit none

      integer, value                                      :: N, M, ldv, ldw,
   ldw2

      real(8), dimension(1:ldv, 1:M), device, intent(in)  :: V

      real(8), dimension(1:ldw, 1:M), device, intent(in)  :: W

      real(8), dimension(1:ldw2, 2), device               :: W2

      real(8), dimension(1:N), device                     :: x

      real(8), device                                     :: tau

      real(8), device                                     :: e

      integer                                             :: i, j, tx, ty, tid,
   nb, laneid, istat, nBlocks

      integer, device                                     :: finished

      integer, shared                                     :: nFinished

      real(8)                                             :: rv

      real(8)                                             :: rv1, rv2, rv3,
   scal, scal2, alphar, beta, rsum

      real(8), shared                                     :: xnorm

      real(8), shared                                     :: alpha_s

      tx = threadIdx%x

      ty = threadIdx%y

      i = (blockIdx%x - 1)*blockDim%x + tx

      j = (blockIdx%y - 1)*blockDim%y + ty

      nBlocks = gridDim%x*gridDim%y

      if (i <= N .and. j <= M) then

         rv = -W(N, j)*V(i, j) - V(N, j)*W(i, j)

         ! Update x

         istat = atomicadd(x(i), rv)

      endif

      if (ty == 1) then

         ! Zero out column for dgemv call

         if (i <= N) W2(i, 1) = 0

         ! Zero out workspace for intermediate dgemv results

         if (i <= M) then

            W2(N + i, 1) = 0

            W2(N + i, 2) = 0

         endif

      endif

      call threadfence()

      nFinished = 0

      call syncthreads()

      if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)

      call syncthreads()

      if (nFinished < nBlocks - 1) return

      ! Begin dlarfg work with last block

      if (N == 1) return

      tid = tx + (ty - 1)*blockDim%x

      laneID = iand(tid, 31)

      if (tid == 1) then

         alpha_s = x(N - 1)

         xnorm = 0.0_8

      endif

      call syncthreads()

      alphar = alpha_s

      rsum = 0.0_8

      nb = ceiling(real(N - 1)/blockDim%x*blockDim%y) ! number of blocks down
   column

      i = tid

      do j = 1, nb

         ! All threads perform their product, zero if out of bounds

         if (i <= N - 2) then

            rv1 = x(i)

            rv1 = rv1*rv1

         else

            rv1 = 0.0_8

         endif

         rsum = rsum + rv1

         i = i + blockDim%x*blockDim%y

      end do

      ! Partial sum within warps using shuffle

      rv1 = rsum

      rv2 = __shfl_down(rv1, 1)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 2)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 4)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 8)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 16)

      rv1 = rv1 + rv2

      if (laneID == 1) then

         istat = atomicadd(xnorm, rv1)

      endif

      call syncthreads()

      if (xnorm == 0.0_8) then

         if (tid == 1) then

            tau = 0.0_8

         endif

      else

         if (tid == 1) then

            xnorm = sqrt(xnorm)

            rv1 = abs(alphar)

            ! not taking abs of xnorm

            scal = max(rv1, xnorm)

            scal2 = min(rv1, xnorm)

            if (scal2 .eq. 0.0d0) then

               beta = -sign(scal, alphar)

            else

               beta = -sign(scal*sqrt(1.0d0 + (scal2/scal)**2), alphar)

            endif

            tau = (beta - alphar)/beta

            e = beta ! store beta in e vector

            alpha_s = 1.d0/(alphar - beta) !scaling factor for dscal

         endif

         call syncthreads()

         do i = tid, N - 1, blockDim%x*blockDim%y

            if (i <= N - 2) then

               x(i) = alpha_s*x(i)

            elseif (i == N - 1) then

               x(i) = 1.0_8

            endif

         end do

      endif


*/

__global__ void dsyr2_mv_dlarfg_kernel(int n, int m, double *v, int ldv,
                                       double *w, int ldw, double *x, double *w2, int ldw2,
                                       double tau, double e, int finished) {
#undef _idx_v
#undef _idx_w
#undef _idx_w2
#define _idx_v(a, b) ((a - 1) + ldv * (b - 1))
#define _idx_w(a, b) ((a - 1) + ldw * (b - 1))
#define _idx_w2(a, b) ((a - 1) + ldw2 * (b - 1))
  int i;
  int j;
  int tx;
  int ty;
  int tid;
  int nb;
  int laneid;
  int istat;
  int nblocks;
  __shared__ int nfinished;
  double rv;
  double rv1;
  double rv2;
  double rv3;
  double scal;
  double scal2;
  double alphar;
  double beta;
  double rsum;
  __shared__ double xnorm;
  __shared__ double alpha_s;

  // ! TODO could not parse:        real(8), dimension(1:ldv, 1:m), device,
  // intent(in)  :: v ! TODO could not parse:        real(8), dimension(1:ldw,
  // 1:m), device, intent(in)  :: w ! TODO could not parse:        real(8),
  // dimension(1:ldw2, 2), device               :: w2 ! TODO could not parse:
  // real(8), dimension(1:n), device                     :: x
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = ((blockIdx.x) * blockDim.x + tx);
  j = ((blockIdx.y) * blockDim.y + ty);
  nblocks = (gridDim.x * gridDim.y);
  if ((i <= n & j <= m)) {
    rv = (-w[_idx_w(n, j)] * v[_idx_v(i, j)] -
          v[_idx_v(n, j)] * w[_idx_w(i, j)]);
    // ! Update x
    istat = atomicAdd(x+_idx(i), rv);
  }
  if ((ty == 1)) {
    // ! Zero out column for dgemv call
    if ((i <= n)) {
      w2[_idx_w2(i, 1)] = 0;
    }
    // ! Zero out workspace for intermediate dgemv results
    if ((i <= m)) {
      w2[_idx_w2((n + i), 1)] = 0;
      w2[_idx_w2((n + i), 2)] = 0;
    }
  }

__threadfence();
nfinished = 0;
__syncthreads();
if (((tx + ty) == 2)) {
  nfinished = atomicInc(&finished, (nblocks - 1));
}
__syncthreads();
if ((nfinished < (nblocks - 1))) {
  return; // ! Begin dlarfg work with last block
}
if ((n == 1)) {
  return;
}
tid = (tx + ty * blockDim.x);
laneid = tid & 31;
if ((tid == 1)) {
  alpha_s = x[_idx((n - 1))];
  xnorm = 0.0 /*_8*/;
}
__syncthreads();
alphar = alpha_s;
rsum = 0.0 /*_8*/;
nb = ceil((float((n - 1)) / blockDim.x * blockDim.y));
// ! number of blocks down column
i = tid;
for (int j = 1; j <= nb; j += 1) {
  // ! All threads perform their product, zero if out of bounds
  if ((i <= (n - 2))) {
    rv1 = x[_idx(i)];
    rv1 = (rv1 * rv1);

  } else {
    rv1 = 0.0 /*_8*/;
  }
  rsum = (rsum + rv1);
  i = (i + blockDim.x * blockDim.y);

} // ! Partial sum within warps using shuffle
rv1 = rsum;
rv2 = __shfl_down(rv1, 1);
rv1 = (rv1 + rv2);
rv2 = __shfl_down(rv1, 2);
rv1 = (rv1 + rv2);
rv2 = __shfl_down(rv1, 4);
rv1 = (rv1 + rv2);
rv2 = __shfl_down(rv1, 8);
rv1 = (rv1 + rv2);
rv2 = __shfl_down(rv1, 16);
rv1 = (rv1 + rv2);
if ((laneid == 1)) {
  istat = atomicAdd(&xnorm, rv1);
}
__syncthreads();
if ((xnorm == 0.0 /*_8*/)) {
  if ((tid == 1)) {
    tau = 0.0 /*_8*/;
  }
} else {
  if ((tid == 1)) {
    xnorm = sqrt(xnorm);
    rv1 = abs(alphar);
    // ! not taking abs of xnorm
    scal = max(rv1, xnorm);
    scal2 = min(rv1, xnorm);
    if ((scal2 == 0.0e0)) {
      beta = -sign(scal, alphar);

    } else {
      beta = -sign((scal * sqrt(pow((1.0e0 + (scal2 / scal)),2))), alphar);
    }
    tau = ((beta - alphar) / beta);
    e = beta;
    // ! store beta in e vector
    alpha_s = (1.e0 / (alphar - beta));
    // !scaling factor for dscal
  }
  __syncthreads();
  for (int i = tid; i <= (n - 1); i += (blockDim.x * blockDim.y)) {
    if ((i <= (n - 2))) {
      x[_idx(i)] = (alpha_s * x[_idx(i)]);

    } else if ((i == (n - 1))) {
      x[_idx(i)] = 1.0 /*_8*/;
    }
  }
}
}

extern "C" void
launch_dsyr2_mv_dlarfg_kernel(dim3 *grid, dim3 *block, const int sharedMem,
                              hipStream_t stream, int n, int m, double *v,
                              int ldv, double *w, int ldw, double *x, double *w2, int ldw2,
                              double tau, double e, int finished) {
  hipLaunchKernelGGL((dsyr2_mv_dlarfg_kernel), *grid, *block, sharedMem, stream,
                     n, m, v,ldv, w, ldw, x, w2, ldw2, tau, e, finished);
}
// END dsyr2_mv_dlarfg_kernel

// BEGIN stacked_dgemv_t
/* Fortran original:
      use cudafor

      implicit none

      integer, value                                  :: M, N, ldv, ldw

      real(8), dimension(ldv, M), device, intent(in)  :: V

      real(8), dimension(ldw, M), device, intent(in)  :: W

      real(8), dimension(N), device, intent(in)       :: x

      real(8), dimension(M), device                   :: z1, z2

      !complex(8), dimension(M), device, intent(in)        :: z1, z2

      !real(8), dimension(32), shared                     :: r_s

      !real(8), dimension(32), shared                     :: i_s

      integer :: i, j, tx, ty, istat

      real(8) :: rv1, rv2, xr

      tx = threadIdx%x

      ty = threadIdx%y

      i = (blockIdx%y - 1)*blockDim%y + ty

      j = (blockIdx%x - 1)*blockDim%x + tx

      !if (i > 2*M .or. j > N) return

      if (i > 2*M) return

      xr = x(j)

      if (j > N) then

         rv1 = 0.d0

      else

         if (i > M) then

            rv2 = W(j, i - M)

         else

            rv2 = V(j, i)

         endif

         rv1 = rv2*xr

      endif

      !Partial sum within warps using shuffle

      rv2 = __shfl_down(rv1, 1)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 2)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 4)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 8)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 16)

      rv1 = rv1 + rv2

      if (tx == 1) then

         if (i > M) then

            istat = atomicadd(z2(i - M), rv1)

         else

            istat = atomicadd(z1(i), rv1)

         endif

      endif

      return

*/

__global__ void stacked_dgemv_t(int m, int n, int ldv, int ldw, double *v,
                                double *w, double *x, double *z1, double *z2) {
#undef _idx_v
#undef _idx_w
#define _idx_v(a, b) ((a - 1) + ldv * (b - 1))
#define _idx_w(a, b) ((a - 1) + ldw * (b - 1))
#define _idx_w2(a, b) ((a - 1) + ldw2 * (b - 1))
  int i;
  int j;
  int tx;
  int ty;
  int istat;
  double rv1;
  double rv2;
  double xr;

  // !complex(8), dimension(M), device, intent(in)        :: z1, z2
  // !real(8), dimension(32), shared                     :: r_s
  // !real(8), dimension(32), shared                     :: i_s
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = ((blockIdx.y) * blockDim.y + ty);
  j = ((blockIdx.x) * blockDim.x + tx);
  // !if (i > 2*M .or. j > N) return
  if ((i > (2 * m))) {
    return;
  }
  xr = x[_idx(j)];
  if ((j > n)) {
    rv1 = 0.e0;

  } else {
    if ((i > m)) {
      rv2 = w[_idx_w(j, (i - m))];

    } else {
      rv2 = v[_idx_v(j, i)];
    }
    rv1 = (rv2 * xr);
  }
  // ! TODO could not parse:        endif
  // !Partial sum within warps using shuffle
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if ((tx == 1)) {
    if ((i > m)) {
      istat = atomicAdd(z2+_idx((i - m)), rv1);

    } else {
      istat = atomicAdd(z1+_idx(i), rv1);
    }
  }
  return;
}

extern "C" void launch_stacked_dgemv_t(dim3 *grid, dim3 *block,
                                       const int sharedMem, hipStream_t stream,
                                       int m, int n, int ldv, int ldw,
                                       double *v, double *w, double *x,
                                       double *z1, double *z2) {
  hipLaunchKernelGGL((stacked_dgemv_t), *grid, *block, sharedMem, stream, m, n,
                     ldv, ldw, v, w, x, z1, z2);
}
// END stacked_dgemv_t

// BEGIN stacked_dgemv_n
/* Fortran original:
      use cudafor

      implicit none

      integer, value                                     :: M, N, ldv, ldw

      real(8), dimension(ldv, N), device, intent(in)     :: V

      real(8), dimension(ldw, N), device, intent(in)     :: W

      real(8), dimension(N), device, intent(in)          :: z1, z2

      real(8), dimension(M), device                      :: y

      integer :: i, j, tx, ty, istat

      real(8) :: rv1, rv2, xr

      tx = threadIdx%x

      ty = threadIdx%y

      i = (blockIdx%x - 1)*blockDim%x + tx

      j = (blockIdx%y - 1)*blockDim%y + ty

      if (i > M .or. j > 2*N) return

      if (j > N) then

         xr = z2(j - N)

         rv2 = V(i, j - N)

      else

         xr = z1(j)

         rv2 = W(i, j)

      endif

      rv1 = -rv2*xr

      istat = atomicadd(y(i), rv1)

      return


*/

__global__ void stacked_dgemv_n(int m, int n, int ldv, int ldw, double *v,
                                double *w, double *z1, double *z2, double *y) {
#undef _idx_v
#undef _idx_w
#undef _idx_w2
#define _idx_v(a, b) ((a - 1) + ldv * (b - 1))
#define _idx_w(a, b) ((a - 1) + ldw * (b - 1))
#define _idx_w2(a, b) ((a - 1) + ldw2 * (b - 1))
  int i;
  int j;
  int tx;
  int ty;
  int istat;
  double rv1;
  double rv2;
  double xr;

  tx = threadIdx.x;
  ty = threadIdx.y;
  i = ((blockIdx.x) * blockDim.x + tx) + 1;
  j = ((blockIdx.y) * blockDim.y + ty) + 1;
  if ((i > m | j > (2 * n))) {
    return;
  }
  if ((j > n)) {
    xr = z2[_idx((j - n))];
    rv2 = v[_idx_v(i, (j - n))];
  } else {
    xr = z1[_idx(j)];
    rv2 = w[_idx_w(i, j)];
  }
  rv1 = (-rv2 * xr);
  istat = atomicAdd(y+_idx(i), rv1);
  return;
}

extern "C" void launch_stacked_dgemv_n(dim3 *grid, dim3 *block,
                                       const int sharedMem, hipStream_t stream,
                                       int m, int n, int ldv, int ldw,
                                       double *v, double *w, double *z1,
                                       double *z2, double *y) {
  hipLaunchKernelGGL((stacked_dgemv_n), *grid, *block, sharedMem, stream, m, n,
                     ldv, ldw, v, w, z1, z2, y);
}
// END stacked_dgemv_n

// BEGIN finish_w_col_kernel
/* Fortran original:
      implicit none

      integer, value                               :: N

      real(8), device                              :: tau

      real(8), dimension(N), device, intent(in)    :: x

      real(8), dimension(N), device                :: y

      integer                                      :: tid, i, j, k, nb, istat,
   laneID

      real(8)                                      :: rv1, rv2, rsum, mytau

      real(8), shared                              :: alphar

      !real(8), shared                              :: alpha

      real(8)                                      :: alpha

      tid = threadIdx%x

      laneID = iand(tid, 31)

      if (tid == 1) then

         alphar = 0.0_8

      endif

      call syncthreads()

      rsum = 0.0_8

      mytau = tau

      nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

      i = tid

      do j = 1, nb

         ! All threads perform their product, zero if out of bounds

         if (i <= N) then

            rv1 = mytau*y(i)*x(i)

         else

            rv1 = 0.0d0

         endif

         rsum = rsum + rv1

         i = i + blockDim%x

      end do

      ! Partial sum within warps using shuffle

      rv1 = rsum

      rv2 = __shfl_down(rv1, 1)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 2)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 4)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 8)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 16)

      rv1 = rv1 + rv2

      if (laneID == 1) then

         istat = atomicadd(alphar, rv1)

      endif

      call syncthreads()

      alpha = -0.5d0*mytau*alphar

      do i = tid, N, blockDim%x

         y(i) = mytau*y(i) + alpha*x(i) !daxpy

      end do


*/

__global__ void finish_w_col_kernel(int n, double tau, double *x, double *y) {
  int tid;
  int i;
  int j;
  int k;
  int nb;
  int istat;
  int laneid;
  double rv1;
  double rv2;
  double rsum;
  double mytau;
  __shared__ double alphar;
  double alpha;

  // !real(8), shared                              :: alpha
  tid = threadIdx.x + 1;
  laneid = tid & 31;
  if ((tid == 1)) {
    alphar = 0.0 /*_8*/;
  }
  __syncthreads();
  rsum = 0.0 /*_8*/;
  mytau = tau;
  nb = ceil((float(n) / blockDim.x));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= n)) {
      rv1 = (mytau * y[_idx(i)] * x[_idx(i)]);

    } else {
      rv1 = 0.0e0;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if ((laneid == 1)) {
    istat = atomicAdd(&alphar, rv1);
  }
  __syncthreads();
  alpha = (-0.5e0 * mytau * alphar);
  for (int i = tid; i <= n; i += blockDim.x) {
    y[_idx(i)] = (mytau * y[_idx(i)] + alpha * x[_idx(i)]);
    // !daxpy
  }
}

extern "C" void launch_finish_w_col_kernel(dim3 *grid, dim3 *block,
                                           const int sharedMem,
                                           hipStream_t stream, int n,
                                           double tau, double *x, double *y) {
  hipLaunchKernelGGL((finish_w_col_kernel), *grid, *block, sharedMem, stream, n,
                     tau, x, y);
}
// END finish_w_col_kernel

// BEGIN stacked_dgemv_n_finish_w
/* Fortran original:
      use cudafor

      implicit none

      integer, value                                     :: M, N, ldv, ldw

      real(8), dimension(ldv, N), device, intent(in)     :: V

      real(8), dimension(ldw, N), device, intent(in)     :: W

      real(8), dimension(N), device, intent(in)          :: z1, z2

      real(8), dimension(M), device                      :: y

      real(8), device                                    :: tau

      real(8), dimension(M), device, intent(in)          :: x

      integer, device                                    :: finished

      integer :: i, j, tx, ty, istat, nBlocks, tid, laneID, nb

      integer, shared :: nFinished

      real(8) :: rv1, rv2, rsum, xr, mytau

      real(8), shared                              :: alphar

      !real(8), shared                              :: alpha

      real(8)                                      :: alpha

      tx = threadIdx%x

      ty = threadIdx%y

      i = (blockIdx%x - 1)*blockDim%x + tx

      j = (blockIdx%y - 1)*blockDim%y + ty

      nBlocks = gridDim%x*gridDim%y

      if (i <= M .and. j <= 2*N) then

         if (j > N) then

            xr = z2(j - N)

            rv2 = V(i, j - N)

         else

            xr = z1(j)

            rv2 = W(i, j)

         endif

         rv1 = -rv2*xr

         istat = atomicadd(y(i), rv1)

      endif

      call threadfence()

      nFinished = 0

      call syncthreads()

      if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)

      call syncthreads()

      if (nFinished < nBlocks - 1) return

      ! Begin finish_W_col work with last block

      tid = threadIdx%x + (threadIdx%y - 1)*blockDim%x

      laneID = iand(tid, 31)

      if (tid == 1) then

         alphar = 0.0_8

      endif

      call syncthreads()

      rsum = 0.0_8

      mytau = tau

      nb = ceiling(real(M)/(blockDim%x*blockDim%y)) ! number of blocks down
   column

      i = tid

      do j = 1, nb

         ! All threads perform their product, zero if out of bounds

         if (i <= M) then

            rv1 = mytau*y(i)*x(i)

         else

            rv1 = 0.0d0

         endif

         rsum = rsum + rv1

         i = i + blockDim%x*blockDim%y

      end do

      ! Partial sum within warps using shuffle

      rv1 = rsum

      rv2 = __shfl_down(rv1, 1)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 2)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 4)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 8)

      rv1 = rv1 + rv2

      rv2 = __shfl_down(rv1, 16)

      rv1 = rv1 + rv2

      if (laneID == 1) then

         istat = atomicadd(alphar, rv1)

      endif

      call syncthreads()

      alpha = -0.5d0*mytau*alphar

      do i = tid, M, blockDim%x*blockDim%y

         y(i) = mytau*y(i) + alpha*x(i) !daxpy

      end do


*/

__global__ void stacked_dgemv_n_finish_w(int m, int n, int ldv, int ldw,
                                         double *v, double *w, double *z1,
                                         double *z2, double *y, double tau,
                                         double *x, int finished) {
#undef _idx_v
#undef _idx_w
#undef _idx_w2
#define _idx_v(a, b) ((a - 1) + ldv * (b - 1))
#define _idx_w(a, b) ((a - 1) + ldw * (b - 1))
#define _idx_w2(a, b) ((a - 1) + ldw2 * (b - 1))
  int i;
  int j;
  int tx;
  int ty;
  int istat;
  int nblocks;
  int tid;
  int laneid;
  int nb;
  __shared__ int nfinished;
  double rv1;
  double rv2;
  double rsum;
  double xr;
  double mytau;
  __shared__ double alphar;
  double alpha;

  // !real(8), shared                              :: alpha
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = ((blockIdx.x) * blockDim.x + tx);
  j = ((blockIdx.y) * blockDim.y + ty);
  nblocks = (gridDim.x * gridDim.y);
  if ((i <= m & j <= (2 * n))) {
    if ((j > n)) {
      xr = z2[_idx((j - n))];
      rv2 = v[_idx_v(i, (j - n))];

    } else {
      xr = z1[_idx(j)];
      rv2 = w[_idx_w(i, j)];
    }
    rv1 = (-rv2 * xr);
    istat = atomicAdd(y+_idx(i), rv1);
  }
  __threadfence();
  nfinished = 0;
  __syncthreads();
  if (((tx + ty) == 2)) {
    nfinished = atomicInc(&finished, (nblocks - 1));
  }
  __syncthreads();
  if ((nfinished < (nblocks - 1))) {
    return;
  }
  // ! Begin finish_W_col work with last block
  tid = (threadIdx.x + (threadIdx.y) * blockDim.x) + 1;
  laneid = tid & 31;
  if ((tid == 1)) {
    alphar = 0.0 /*_8*/;
  }
  __syncthreads();
  rsum = 0.0 /*_8*/;
  mytau = tau;
  nb = ceil(float(m) / (blockDim.x * blockDim.y));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= m)) {
      rv1 = (mytau * y[_idx(i)] * x[_idx(i)]);

    } else {
      rv1 = 0.0e0;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x * blockDim.y);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if ((laneid == 1)) {
    istat = atomicAdd(&alphar, rv1);
  }
  __syncthreads();
  alpha = (-0.5e0 * mytau * alphar);
  for (int i = tid; i <= m; i += (blockDim.x * blockDim.y)) {
    y[_idx(i)] = (mytau * y[_idx(i)] + alpha * x[_idx(i)]);
    // !daxpy
  }
}

extern "C" void launch_stacked_dgemv_n_finish_w(
    dim3 *grid, dim3 *block, const int sharedMem, hipStream_t stream, int m,
    int n, int ldv, int ldw, double *v, double *w, double *z1, double *z2,
    double *y, double tau, double *x, int finished) {
  hipLaunchKernelGGL((stacked_dgemv_n_finish_w), *grid, *block, sharedMem,
                     stream, m, n, ldv, ldw, v, w, z1, z2, y, tau, x, finished);
}
// END stacked_dgemv_n_finish_w
