! This file was generated by gpufort
          
           
module zhetrd_gpu_kernels
  use hip
  implicit none

 
  interface

    subroutine launch_krnl_2b8e8f_0(grid,&
        block,&
        sharedMem,&
        stream,&
        a,&
        d,&
        n) bind(c, name="launch_krnl_2b8e8f_0")
      use iso_c_binding
      use hip
      implicit none
      type(dim3),intent(IN) :: grid
      type(dim3),intent(IN) :: block
      integer(c_int),intent(IN) :: sharedMem
      type(c_ptr),value,intent(IN) :: stream
      TODO declaration not found :: a
      TODO declaration not found :: d
      integer,value :: n
    end subroutine

    subroutine launch_krnl_2b8e8f_0_auto(sharedMem,&
        stream,&
        a,&
        d,&
        n) bind(c, name="launch_krnl_2b8e8f_0_auto")
      use iso_c_binding
      use hip
      implicit none
      integer(c_int),intent(IN) :: sharedMem
      type(c_ptr),value,intent(IN) :: stream
      TODO declaration not found :: a
      TODO declaration not found :: d
      integer,value :: n
    end subroutine

    subroutine launch_krnl_9c27cb_1(grid,&
        block,&
        sharedMem,&
        stream,&
        iw,&
        n,&
        w) bind(c, name="launch_krnl_9c27cb_1")
      use iso_c_binding
      use hip
      implicit none
      type(dim3),intent(IN) :: grid
      type(dim3),intent(IN) :: block
      integer(c_int),intent(IN) :: sharedMem
      type(c_ptr),value,intent(IN) :: stream
      integer,value :: iw
      integer,value :: n
      TODO declaration not found :: w
    end subroutine

    subroutine launch_krnl_9c27cb_1_auto(sharedMem,&
        stream,&
        iw,&
        n,&
        w) bind(c, name="launch_krnl_9c27cb_1_auto")
      use iso_c_binding
      use hip
      implicit none
      integer(c_int),intent(IN) :: sharedMem
      type(c_ptr),value,intent(IN) :: stream
      integer,value :: iw
      integer,value :: n
      TODO declaration not found :: w
    end subroutine

    subroutine launch_zher2_mv_kernel(grid,&
        block,&
        sharedMem,&
        stream,&
        n,&
        m,&
        ldv,&
        ldw,&
        ldw2) bind(c, name="launch_zher2_mv_kernel")
      use iso_c_binding
      use hip
      implicit none
      type(dim3,,intent(IN, :: grid
      type(dim3,,intent(IN, :: block
      integer(c_int,,intent(IN, :: sharedMem
      type(c_ptr,,value,intent(IN, :: stream
      integer,value :: n
      integer,value :: m
      integer,value :: ldv
      integer,value :: ldw
      integer,value :: ldw2
    end subroutine

    subroutine launch_zlarfg_kernel(grid,&
        block,&
        sharedMem,&
        stream,&
        n,&
        tau,&
        e,&
        x) bind(c, name="launch_zlarfg_kernel")
      use iso_c_binding
      use hip
      implicit none
      type(dim3,,intent(IN, :: grid
      type(dim3,,intent(IN, :: block
      integer(c_int,,intent(IN, :: sharedMem
      type(c_ptr,,value,intent(IN, :: stream
      integer,value :: n
      complex(kind=8),value :: tau
      real(kind=8),value :: e
      type(c_ptr),value :: _x
    end subroutine

    subroutine launch_zher2_mv_zlarfg_kernel(grid,&
        block,&
        sharedMem,&
        stream,&
        n,&
        m,&
        ldv,&
        ldw,&
        ldw2,&
        tau,&
        e,&
        finished) bind(c, name="launch_zher2_mv_zlarfg_kernel")
      use iso_c_binding
      use hip
      implicit none
      type(dim3,,intent(IN, :: grid
      type(dim3,,intent(IN, :: block
      integer(c_int,,intent(IN, :: sharedMem
      type(c_ptr,,value,intent(IN, :: stream
      integer,value :: n
      integer,value :: m
      integer,value :: ldv
      integer,value :: ldw
      integer,value :: ldw2
      complex(kind=8),value :: tau
      real(kind=8),value :: e
      integer,value :: finished
    end subroutine

    subroutine launch_stacked_zgemv_c(grid,&
        block,&
        sharedMem,&
        stream,&
        m,&
        n,&
        ldv,&
        ldw,&
        v,&
        w,&
        x) bind(c, name="launch_stacked_zgemv_c")
      use iso_c_binding
      use hip
      implicit none
      type(dim3,,intent(IN, :: grid
      type(dim3,,intent(IN, :: block
      integer(c_int,,intent(IN, :: sharedMem
      type(c_ptr,,value,intent(IN, :: stream
      integer,value :: m
      integer,value :: n
      integer,value :: ldv
      integer,value :: ldw
      type(c_ptr),value :: _v
      type(c_ptr),value :: _w
      type(c_ptr),value :: _x
    end subroutine

    subroutine launch_stacked_zgemv_n(grid,&
        block,&
        sharedMem,&
        stream,&
        m,&
        n,&
        ldv,&
        ldw,&
        v,&
        w,&
        z1,&
        z2) bind(c, name="launch_stacked_zgemv_n")
      use iso_c_binding
      use hip
      implicit none
      type(dim3,,intent(IN, :: grid
      type(dim3,,intent(IN, :: block
      integer(c_int,,intent(IN, :: sharedMem
      type(c_ptr,,value,intent(IN, :: stream
      integer,value :: m
      integer,value :: n
      integer,value :: ldv
      integer,value :: ldw
      type(c_ptr),value :: _v
      type(c_ptr),value :: _w
      type(c_ptr),value :: _z1
      type(c_ptr),value :: _z2
    end subroutine

    subroutine launch_finish_w_col_kernel(grid,&
        block,&
        sharedMem,&
        stream,&
        n,&
        tau,&
        x,&
        y) bind(c, name="launch_finish_w_col_kernel")
      use iso_c_binding
      use hip
      implicit none
      type(dim3,,intent(IN, :: grid
      type(dim3,,intent(IN, :: block
      integer(c_int,,intent(IN, :: sharedMem
      type(c_ptr,,value,intent(IN, :: stream
      integer,value :: n
      complex(kind=8),value :: tau
      type(c_ptr),value :: _x
      type(c_ptr),value :: _y
    end subroutine

    subroutine launch_stacked_zgemv_n_finish_w(grid,&
        block,&
        sharedMem,&
        stream,&
        m,&
        n,&
        ldv,&
        ldw,&
        v,&
        w,&
        z1,&
        z2,&
        tau,&
        x,&
        y2,&
        finished) bind(c, name="launch_stacked_zgemv_n_finish_w")
      use iso_c_binding
      use hip
      implicit none
      type(dim3,,intent(IN, :: grid
      type(dim3,,intent(IN, :: block
      integer(c_int,,intent(IN, :: sharedMem
      type(c_ptr,,value,intent(IN, :: stream
      integer,value :: m
      integer,value :: n
      integer,value :: ldv
      integer,value :: ldw
      type(c_ptr),value :: _v
      type(c_ptr),value :: _w
      type(c_ptr),value :: _z1
      type(c_ptr),value :: _z2
      complex(kind=8),value :: tau
      type(c_ptr),value :: _x
      type(c_ptr),value :: _y2
      integer,value :: finished
    end subroutine

  end interface

  contains

    subroutine launch_krnl_2b8e8f_0_cpu(sharedMem,&
        stream,&
        a,&
        d,&
        n)
      use iso_c_binding
      use hip
      implicit none
      integer(c_int),intent(IN) :: sharedMem
      type(c_ptr),value,intent(IN) :: stream
      TODO declaration not found :: a
      TODO declaration not found :: d
      integer,value :: n
      integer :: j
      do j = 33, N
      !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
         d(j) = A(j, j)
      end do

    end subroutine

    subroutine launch_krnl_9c27cb_1_cpu(sharedMem,&
        stream,&
        iw,&
        n,&
        w)
      use iso_c_binding
      use hip
      implicit none
      integer(c_int),intent(IN) :: sharedMem
      type(c_ptr),value,intent(IN) :: stream
      integer,value :: iw
      integer,value :: n
      TODO declaration not found :: w
      integer :: k
      do k = 1, N - 1
         W(k, iw) = dcmplx(0, 0)
      end do

    end subroutine

    subroutine launch_zher2_mv_kernel_cpu(n,&
        m,&
        ldv,&
        ldw,&
        ldw2)
      use iso_c_binding
      use hip
      implicit none
      integer,value :: n
      integer,value :: m
      integer,value :: ldv
      integer,value :: ldw
      integer,value :: ldw2
      integer :: i
      integer :: j
      integer :: istat
      complex(kind=8) :: val
      real(kind=8) :: rv
      real(kind=8) :: iv
            implicit none

            integer, value                                        :: N, M, ldv, ldw, ldw2

            complex(8), dimension(1:ldv, 1:M), device, intent(in) :: V

            complex(8), dimension(1:ldw, 1:M), device, intent(in) :: W

            complex(8), dimension(1:ldw2, 2), device               :: W2

            !DIR$ IGNORE_TKR x

            real(8), dimension(1:2*N), device                     :: x

            integer                                               :: i, j, istat

            complex(8)                                            :: val

            real(8)                                               :: rv, iv

            i = (blockIdx%x - 1)*blockDim%x + threadIdx%x

            j = (blockIdx%y - 1)*blockDim%y + threadIdx%y

            if (i <= N .and. j <= M) then

               val = -conjg(W(N, j))*V(i, j) - conjg(V(N, j))*W(i, j)

               rv = dble(val)

               iv = dimag(val)

               ! Zero out imaginary part on diagonal

               if (i == N) then

                  iv = 0.d0

               endif

               ! Update x

               istat = atomicadd(x(2*i - 1), rv)

               istat = atomicadd(x(2*i), iv)

            endif

            if (threadIdx%y == 1) then

               ! Zero out column for zhemv call

               if (i <= N) W2(i, 1) = 0

               ! Zero out workspace for intermediate zgemv results

               if (i <= M) then

                  W2(N + i, 1) = 0

                  W2(N + i, 2) = 0

               endif

            endif


    end subroutine

    subroutine launch_zlarfg_kernel_cpu(n,&
        tau,&
        e,&
        _x)
      use iso_c_binding
      use hip
      implicit none
      integer,value :: n
      complex(kind=8),value :: tau
      real(kind=8),value :: e
      type(c_ptr),value :: _x
            complex(8),target :: x()
      integer :: tid
      integer :: i
      integer :: j
      integer :: nb
      integer :: istat
      integer :: laneid
      real(kind=8) :: rv1
      real(kind=8) :: rv2
      real(kind=8) :: rv3
      real(kind=8) :: scal
      real(kind=8) :: invscal
      real(kind=8) :: alphar
      real(kind=8) :: alphai
      real(kind=8) :: beta
      real(kind=8) :: rsum
      real(kind=8) :: isum
      complex(kind=8) :: cv1
      real(kind=8) :: xnorm
      complex(kind=8) :: alpha_s
      CALL hipCheck(hipMemcpy(c_loc(x),_x,C_SIZEOF(x),hipMemcpyDeviceToHost))
            implicit none

            integer, value                   :: N

            complex(8), device               :: tau

            real(8), device                  :: e

            complex(8), dimension(N), device :: x

            integer                          :: tid, i, j, nb, istat, laneID

            real(8)                          :: rv1, rv2, rv3, scal, invscal, alphar, alphai, beta, rsum, isum

            complex(8)                       :: cv1

            real(8), shared                  :: xnorm

            complex(8), shared               :: alpha_s

            tid = threadIdx%x

            laneID = iand(tid, 31)

            if (tid == 1) then

               alpha_s = x(N)

               xnorm = 0.0_8

            endif

            call syncthreads()

            alphar = dble(alpha_s)

            alphai = dimag(alpha_s)

            rsum = 0.0_8

            nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

            i = tid

            do j = 1, nb

               ! All threads perform their product, zero if out of bounds

               if (i <= N - 1) then

                  cv1 = x(i)

                  rv2 = dble(cv1); rv3 = dimag(cv1)

                  rv1 = rv2*rv2 + rv3*rv3

               else

                  rv1 = 0.0_8

               endif

               rsum = rsum + rv1

               i = i + blockDim%x

            end do

            ! Partial sum within warps using shuffle

            rv1 = rsum

            rv2 = __shfl_down(rv1, 1)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 2)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 4)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 8)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 16)

            rv1 = rv1 + rv2

            if (laneID == 1) then

               istat = atomicadd(xnorm, rv1)

            endif

            call syncthreads()

            if (xnorm == 0.0_8 .and. alphai == 0.0_8) then

               if (tid == 1) then

                  tau = 0.0_8

               endif

            else

               if (tid == 1) then

                  xnorm = sqrt(xnorm)

                  rv1 = abs(alphar)

                  rv2 = abs(alphai)

                  ! not taking abs of xnorm

                  scal = max(rv1, rv2, xnorm)

                  invscal = 1.d0/scal

                  rv1 = rv1*invscal

                  rv2 = rv2*invscal

                  xnorm = xnorm*invscal

                  beta = -sign(scal*sqrt(rv1*rv1 + rv2*rv2 + xnorm*xnorm), alphar)

                  tau = dcmplx((beta - alphar)/beta, -alphai/beta)

                  !zladiv

                  rv1 = dble(alpha_s - beta)

                  rv2 = dimag(alpha_s - beta)

                  if (abs(rv2) .lt. abs(rv1)) then

                     xnorm = rv2/rv1

                     invscal = 1.d0/(rv1 + rv2*xnorm)

                     alpha_s = dcmplx(invscal, -xnorm*invscal)

                  else

                     xnorm = rv1/rv2

                     invscal = 1.d0/(rv2 + rv1*xnorm)

                     alpha_s = dcmplx(xnorm*invscal, -invscal)

                  endif

                  e = beta ! store beta in e vector

               endif

               call syncthreads()

               do i = tid, N, blockDim%x

                  cv1 = x(i)

                  if (i <= N - 1) then

                     cv1 = alpha_s*cv1

                  elseif (i == N) then

                     !x(i) = 1.0_8

                     cv1 = dcmplx(1.0_8, 0.0_8)

                  endif

                  x(i) = cv1

               end do

            endif

      CALL hipCheck(hipMemcpy(_x,c_loc(x),C_SIZEOF(x),hipMemcpyHostToDevice))

    end subroutine

    subroutine launch_zher2_mv_zlarfg_kernel_cpu(n,&
        m,&
        ldv,&
        ldw,&
        ldw2,&
        tau,&
        e,&
        finished)
      use iso_c_binding
      use hip
      implicit none
      integer,value :: n
      integer,value :: m
      integer,value :: ldv
      integer,value :: ldw
      integer,value :: ldw2
      complex(kind=8),value :: tau
      real(kind=8),value :: e
      integer,value :: finished
      integer :: i
      integer :: j
      integer :: tx
      integer :: ty
      integer :: tid
      integer :: nb
      integer :: laneid
      integer :: istat
      integer :: nblocks
      integer :: nfinished
      complex(kind=8) :: val
      real(kind=8) :: rv
      real(kind=8) :: iv
      real(kind=8) :: rv1
      real(kind=8) :: rv2
      real(kind=8) :: rv3
      real(kind=8) :: scal
      real(kind=8) :: invscal
      real(kind=8) :: alphar
      real(kind=8) :: alphai
      real(kind=8) :: beta
      real(kind=8) :: rsum
      real(kind=8) :: isum
      complex(kind=8) :: cv1
      real(kind=8) :: xnorm
      complex(kind=8) :: alpha_s
            implicit none

            integer, value                                        :: N, M, ldv, ldw, ldw2

            complex(8), dimension(1:ldv, 1:M), device, intent(in) :: V

            complex(8), dimension(1:ldw, 1:M), device, intent(in) :: W

            complex(8), dimension(1:ldw2, 2), device              :: W2

            !DIR$ IGNORE_TKR x

            real(8), dimension(1:2*N), device                     :: x

            complex(8), dimension(1:N), device                    :: x2

            complex(8), device                                    :: tau

            real(8), device                                       :: e

            integer                                               :: i, j, tx, ty, tid, nb, laneid, istat, nBlocks

            integer, device                                       :: finished

            integer, shared                                       :: nFinished

            complex(8)                                            :: val

            real(8)                                               :: rv, iv

            real(8)                                               :: rv1, rv2, rv3, scal, invscal, alphar, alphai, beta, rsum, isum

            complex(8)                                            :: cv1

            real(8), shared                                       :: xnorm

            complex(8), shared                                    :: alpha_s

            tx = threadIdx%x

            ty = threadIdx%y

            i = (blockIdx%x - 1)*blockDim%x + tx

            j = (blockIdx%y - 1)*blockDim%y + ty

            nBlocks = gridDim%x*gridDim%y

            !if (i > N .or. j > M) return

            if (i <= N .and. j <= M) then

               val = -conjg(W(N, j))*V(i, j) - conjg(V(N, j))*W(i, j)

               rv = dble(val)

               iv = dimag(val)

               ! Zero out imaginary part on diagonal

               if (i == N) then

                  iv = 0.d0

               endif

               ! Update x

               istat = atomicadd(x(2*i - 1), rv)

               istat = atomicadd(x(2*i), iv)

            endif

            if (ty == 1) then

               ! Zero out column for zhemv call

               if (i <= N) W2(i, 1) = 0

               ! Zero out workspace for intermediate zgemv results

               if (i <= M) then

                  W2(N + i, 1) = 0

                  W2(N + i, 2) = 0

               endif

            endif

            call threadfence()

            nFinished = 0

            call syncthreads()

            if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)

            call syncthreads()

            if (nFinished < nBlocks - 1) return

            ! Begin zlarfg work with last block

            if (N == 1) return

            tid = tx + (ty - 1)*blockDim%x

            laneID = iand(tid, 31)

            if (tid == 1) then

               alpha_s = x2(N - 1)

               xnorm = 0.0_8

            endif

            call syncthreads()

            alphar = dble(alpha_s)

            alphai = dimag(alpha_s)

            rsum = 0.0_8

            nb = ceiling(real(N - 1)/(blockDim%x*blockDim%y)) ! number of blocks down column

            i = tid

            do j = 1, nb

               ! All threads perform their product, zero if out of bounds

               if (i <= N - 2) then

                  cv1 = x2(i)

                  rv2 = dble(cv1); rv3 = dimag(cv1)

                  rv1 = rv2*rv2 + rv3*rv3

               else

                  rv1 = 0.0_8

               endif

               rsum = rsum + rv1

               i = i + blockDim%x*blockDim%y

            end do

            ! Partial sum within warps using shuffle

            rv1 = rsum

            rv2 = __shfl_down(rv1, 1)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 2)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 4)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 8)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 16)

            rv1 = rv1 + rv2

            if (laneID == 1) then

               istat = atomicadd(xnorm, rv1)

            endif

            call syncthreads()

            if (xnorm == 0.0_8 .and. alphai == 0.0_8) then

               if (tid == 1) then

                  tau = 0.0_8

               endif

            else

               if (tid == 1) then

                  xnorm = sqrt(xnorm)

                  rv1 = abs(alphar)

                  rv2 = abs(alphai)

                  ! not taking abs of xnorm

                  scal = max(rv1, rv2, xnorm)

                  invscal = 1.d0/scal

                  rv1 = rv1*invscal

                  rv2 = rv2*invscal

                  xnorm = xnorm*invscal

                  beta = -sign(scal*sqrt(rv1*rv1 + rv2*rv2 + xnorm*xnorm), alphar)

                  tau = dcmplx((beta - alphar)/beta, -alphai/beta)

                  !zladiv

                  rv1 = dble(alpha_s - beta)

                  rv2 = dimag(alpha_s - beta)

                  if (abs(rv2) .lt. abs(rv1)) then

                     xnorm = rv2/rv1

                     invscal = 1.d0/(rv1 + rv2*xnorm)

                     alpha_s = dcmplx(invscal, -xnorm*invscal)

                  else

                     xnorm = rv1/rv2

                     invscal = 1.d0/(rv2 + rv1*xnorm)

                     alpha_s = dcmplx(xnorm*invscal, -invscal)

                  endif

                  e = beta ! store beta in e vector

               endif

               call syncthreads()

               do i = tid, N - 1, blockDim%x*blockDim%y

                  cv1 = x2(i)

                  if (i <= N - 2) then

                     cv1 = alpha_s*cv1

                  elseif (i == N - 1) then

                     !x(i) = 1.0_8

                     cv1 = dcmplx(1.0_8, 0.0_8)

                  endif

                  x2(i) = cv1

               end do

            endif


    end subroutine

    subroutine launch_stacked_zgemv_c_cpu(m,&
        n,&
        ldv,&
        ldw,&
        _v,&
        _w,&
        _x)
      use iso_c_binding
      use hip
      implicit none
      integer,value :: m
      integer,value :: n
      integer,value :: ldv
      integer,value :: ldw
      type(c_ptr),value :: _v
      type(c_ptr),value :: _w
      type(c_ptr),value :: _x
            complex(8), M), intent(in)  ,target :: v()
            complex(8), M), intent(in)  ,target :: w()
            complex(8), intent(in)       ,target :: x()
      integer :: i
      integer :: j
      integer :: tx
      integer :: ty
      integer :: istat
      complex(kind=8) :: val
      real(kind=8) :: rv1
      real(kind=8) :: rv2
      real(kind=8) :: iv1
      real(kind=8) :: iv2
      real(kind=8) :: xr
      real(kind=8) :: xi
      CALL hipCheck(hipMemcpy(c_loc(v),_v,C_SIZEOF(v),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(w),_w,C_SIZEOF(w),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(x),_x,C_SIZEOF(x),hipMemcpyDeviceToHost))
            use cudafor

            implicit none

            integer, value                                     :: M, N, ldv, ldw

            complex(8), dimension(ldv, M), device, intent(in)  :: V

            complex(8), dimension(ldw, M), device, intent(in)  :: W

            complex(8), dimension(N), device, intent(in)       :: x

            !DIR$ IGNORE_TKR z1, z2

            real(8), dimension(2*M), device                    :: z1, z2

            !complex(8), dimension(M), device, intent(in)        :: z1, z2

            !real(8), dimension(32), shared                     :: r_s

            !real(8), dimension(32), shared                     :: i_s

            integer :: i, j, tx, ty, istat

            complex(8) :: val

            real(8) :: rv1, rv2, iv1, iv2, xr, xi

            tx = threadIdx%x

            ty = threadIdx%y

            i = (blockIdx%y - 1)*blockDim%y + ty

            j = (blockIdx%x - 1)*blockDim%x + tx

            !if (i > 2*M .or. j > N) return

            if (i > 2*M) return

            val = x(j)

            xr = dble(val); xi = dimag(val)

            if (j > N) then

               !val = dcmplx(0,0)

               rv1 = 0.d0; iv1 = 0.d0

            else

               if (i > M) then

                  val = W(j, i - M)

               else

                  val = V(j, i)

               endif

               rv2 = dble(val); iv2 = dimag(val)

               rv1 = rv2*xr + iv2*xi

               iv1 = rv2*xi - iv2*xr

            endif

            !Partial sum within warps using shuffle

            rv2 = __shfl_down(rv1, 1)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 2)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 4)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 8)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 16)

            rv1 = rv1 + rv2

            !if (tx == 1) then

            !r_s(ty + k*blockDim%y) = rv1

            !r_s(ty) = rv1

            !endif

            !Partial sum within warps using shuffle

            iv2 = __shfl_down(iv1, 1)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 2)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 4)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 8)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 16)

            iv1 = iv1 + iv2

            !if (tx == 1) then

            !i_s(ty + k*blockDim%y) = iv1

            !i_s(ty) = iv1

            !endif

            !call syncthreads()

            !if (ty == 1 .and. i+tx-1 <= 2*M) then

            !  if (i+tx-1 > M) then

            !    istat = atomicadd(z2(2*(i+tx-1-M) - 1), r_s(tx))

            !    istat = atomicadd(z2(2*(i+tx-1-M)), i_s(tx))

            !  else

            !    istat = atomicadd(z1(2*(i+tx-1) - 1), r_s(tx))

            !    istat = atomicadd(z1(2*(i+tx-1)), i_s(tx))

            !  endif

            !endif

            if (tx == 1) then

               if (i > M) then

                  istat = atomicadd(z2(2*(i - M) - 1), rv1)

                  istat = atomicadd(z2(2*(i - M)), iv1)

               else

                  istat = atomicadd(z1(2*i - 1), rv1)

                  istat = atomicadd(z1(2*i), iv1)

               endif

            endif

            return
      CALL hipCheck(hipMemcpy(_v,c_loc(v),C_SIZEOF(v),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_w,c_loc(w),C_SIZEOF(w),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_x,c_loc(x),C_SIZEOF(x),hipMemcpyHostToDevice))

    end subroutine

    subroutine launch_stacked_zgemv_n_cpu(m,&
        n,&
        ldv,&
        ldw,&
        _v,&
        _w,&
        _z1,&
        _z2)
      use iso_c_binding
      use hip
      implicit none
      integer,value :: m
      integer,value :: n
      integer,value :: ldv
      integer,value :: ldw
      type(c_ptr),value :: _v
      type(c_ptr),value :: _w
      type(c_ptr),value :: _z1
      type(c_ptr),value :: _z2
            complex(8), N), intent(in)  ,target :: v()
            complex(8), N), intent(in)  ,target :: w()
            complex(8), intent(in)       ,target :: z1()
            complex(8), intent(in)       ,target :: z2()
      integer :: i
      integer :: j
      integer :: tx
      integer :: ty
      integer :: istat
      complex(kind=8) :: val1
      complex(kind=8) :: val2
      real(kind=8) :: rv1
      real(kind=8) :: rv2
      real(kind=8) :: iv1
      real(kind=8) :: iv2
      real(kind=8) :: xr
      real(kind=8) :: xi
      CALL hipCheck(hipMemcpy(c_loc(v),_v,C_SIZEOF(v),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(w),_w,C_SIZEOF(w),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(z1),_z1,C_SIZEOF(z1),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(z2),_z2,C_SIZEOF(z2),hipMemcpyDeviceToHost))
            use cudafor

            implicit none

            integer, value                                     :: M, N, ldv, ldw

            complex(8), dimension(ldv, N), device, intent(in)  :: V

            complex(8), dimension(ldw, N), device, intent(in)  :: W

            complex(8), dimension(N), device, intent(in)       :: z1, z2

            !DIR$ IGNORE_TKR y

            real(8), dimension(2*M), device                    :: y

            integer :: i, j, tx, ty, istat

            complex(8) :: val1, val2

            real(8) :: rv1, rv2, iv1, iv2, xr, xi

            tx = threadIdx%x

            ty = threadIdx%y

            i = (blockIdx%x - 1)*blockDim%x + tx

            j = (blockIdx%y - 1)*blockDim%y + ty

            if (i > M .or. j > 2*N) return

            if (j > N) then

               val1 = z2(j - N)

               val2 = V(i, j - N)

            else

               val1 = z1(j)

               val2 = W(i, j)

            endif

            xr = dble(val1); xi = dimag(val1)

            rv2 = dble(val2); iv2 = dimag(val2)

            rv1 = -rv2*xr + iv2*xi

            iv1 = -rv2*xi - iv2*xr

            istat = atomicadd(y(2*i - 1), rv1)

            istat = atomicadd(y(2*i), iv1)

            return

      CALL hipCheck(hipMemcpy(_v,c_loc(v),C_SIZEOF(v),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_w,c_loc(w),C_SIZEOF(w),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_z1,c_loc(z1),C_SIZEOF(z1),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_z2,c_loc(z2),C_SIZEOF(z2),hipMemcpyHostToDevice))

    end subroutine

    subroutine launch_finish_w_col_kernel_cpu(n,&
        tau,&
        _x,&
        _y)
      use iso_c_binding
      use hip
      implicit none
      integer,value :: n
      complex(kind=8),value :: tau
      type(c_ptr),value :: _x
      type(c_ptr),value :: _y
            complex(8), intent(in) ,target :: x()
            complex(8),target :: y()
      integer :: tid
      integer :: i
      integer :: j
      integer :: k
      integer :: nb
      integer :: istat
      integer :: laneid
      real(kind=8) :: rv1
      real(kind=8) :: rv2
      real(kind=8) :: iv1
      real(kind=8) :: iv2
      real(kind=8) :: rsum
      real(kind=8) :: isum
      complex(kind=8) :: val
      complex(kind=8) :: cv1
      complex(kind=8) :: mytau
      real(kind=8) :: alphar
      real(kind=8) :: alphai
      complex(kind=8) :: alpha
      CALL hipCheck(hipMemcpy(c_loc(x),_x,C_SIZEOF(x),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(y),_y,C_SIZEOF(y),hipMemcpyDeviceToHost))
            implicit none

            integer, value                               :: N

            complex(8), device                           :: tau

            complex(8), dimension(N), device, intent(in) :: x

            complex(8), dimension(N), device             :: y

            integer                                      :: tid, i, j, k, nb, istat, laneID

            real(8)                                      :: rv1, rv2, iv1, iv2, rsum, isum

            complex(8)                                   :: val, cv1, mytau

            real(8), shared                              :: alphar, alphai

            !complex(8), shared                          :: alpha

            complex(8)                                   :: alpha

            tid = threadIdx%x

            laneID = iand(tid, 31)

            if (tid == 1) then

               alphar = 0.0_8

               alphai = 0.0_8

            endif

            call syncthreads()

            rsum = 0.0_8

            isum = 0.0_8

            mytau = tau

            nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

            i = tid

            do j = 1, nb

               ! All threads perform their product, zero if out of bounds

               if (i <= N) then

                  val = dconjg(mytau*y(i))*x(i)

               else

                  val = dcmplx(0., 0.)

               endif

               rv1 = dble(val); iv1 = dimag(val)

               rsum = rsum + rv1

               isum = isum + iv1

               i = i + blockDim%x

            end do

            ! Partial sum within warps using shuffle

            rv1 = rsum

            rv2 = __shfl_down(rv1, 1)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 2)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 4)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 8)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 16)

            rv1 = rv1 + rv2

            iv1 = isum

            iv2 = __shfl_down(iv1, 1)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 2)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 4)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 8)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 16)

            iv1 = iv1 + iv2

            if (laneID == 1) then

               istat = atomicadd(alphar, rv1)

               istat = atomicadd(alphai, iv1)

            endif

            call syncthreads()

            alpha = -dcmplx(0.5, 0.0)*mytau*dcmplx(alphar, alphai)

            do i = tid, N, blockDim%x

               y(i) = mytau*y(i) + alpha*x(i) !zaxpy

            end do

      CALL hipCheck(hipMemcpy(_x,c_loc(x),C_SIZEOF(x),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_y,c_loc(y),C_SIZEOF(y),hipMemcpyHostToDevice))

    end subroutine

    subroutine launch_stacked_zgemv_n_finish_w_cpu(m,&
        n,&
        ldv,&
        ldw,&
        _v,&
        _w,&
        _z1,&
        _z2,&
        tau,&
        _x,&
        _y2,&
        finished)
      use iso_c_binding
      use hip
      implicit none
      integer,value :: m
      integer,value :: n
      integer,value :: ldv
      integer,value :: ldw
      type(c_ptr),value :: _v
      type(c_ptr),value :: _w
      type(c_ptr),value :: _z1
      type(c_ptr),value :: _z2
      complex(kind=8),value :: tau
      type(c_ptr),value :: _x
      type(c_ptr),value :: _y2
      integer,value :: finished
            complex(8), N), intent(in)  ,target :: v()
            complex(8), N), intent(in)  ,target :: w()
            complex(8), intent(in)       ,target :: z1()
            complex(8), intent(in)       ,target :: z2()
            complex(8), intent(in)       ,target :: x()
            complex(8),target :: y2()
      integer :: i
      integer :: j
      integer :: tx
      integer :: ty
      integer :: istat
      integer :: nblocks
      integer :: tid
      integer :: laneid
      integer :: nb
      integer :: nfinished
      complex(kind=8) :: val1
      complex(kind=8) :: val2
      complex(kind=8) :: mytau
      complex(kind=8) :: alpha
      real(kind=8) :: rv1
      real(kind=8) :: rv2
      real(kind=8) :: iv1
      real(kind=8) :: iv2
      real(kind=8) :: xr
      real(kind=8) :: xi
      real(kind=8) :: rsum
      real(kind=8) :: isum
      real(kind=8) :: alphar
      real(kind=8) :: alphai
      CALL hipCheck(hipMemcpy(c_loc(v),_v,C_SIZEOF(v),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(w),_w,C_SIZEOF(w),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(z1),_z1,C_SIZEOF(z1),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(z2),_z2,C_SIZEOF(z2),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(x),_x,C_SIZEOF(x),hipMemcpyDeviceToHost))
      CALL hipCheck(hipMemcpy(c_loc(y2),_y2,C_SIZEOF(y2),hipMemcpyDeviceToHost))
            use cudafor

            implicit none

            integer, value                                     :: M, N, ldv, ldw

            complex(8), dimension(ldv, N), device, intent(in)  :: V

            complex(8), dimension(ldw, N), device, intent(in)  :: W

            complex(8), dimension(N), device, intent(in)       :: z1, z2

            !DIR$ IGNORE_TKR y

            real(8), dimension(2*M), device                    :: y

            complex(8), device                                 :: tau

            complex(8), dimension(M), device, intent(in)       :: x

            complex(8), dimension(M), device                   :: y2

            integer, device                                    :: finished

            integer :: i, j, tx, ty, istat, nBlocks, tid, laneID, nb

            integer, shared :: nFinished

            complex(8) :: val1, val2, mytau, alpha

            real(8) :: rv1, rv2, iv1, iv2, xr, xi, rsum, isum

            real(8), shared :: alphar, alphai

            tx = threadIdx%x

            ty = threadIdx%y

            i = (blockIdx%x - 1)*blockDim%x + tx

            j = (blockIdx%y - 1)*blockDim%y + ty

            nBlocks = gridDim%x*gridDim%y

            if (i <= M .and. j <= 2*N) then

               if (j > N) then

                  val1 = z2(j - N)

                  val2 = V(i, j - N)

               else

                  val1 = z1(j)

                  val2 = W(i, j)

               endif

               xr = dble(val1); xi = dimag(val1)

               rv2 = dble(val2); iv2 = dimag(val2)

               rv1 = -rv2*xr + iv2*xi

               iv1 = -rv2*xi - iv2*xr

               istat = atomicadd(y(2*i - 1), rv1)

               istat = atomicadd(y(2*i), iv1)

            endif

            call threadfence()

            nFinished = 0

            call syncthreads()

            if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)

            call syncthreads()

            if (nFinished < nBlocks - 1) return

            ! Begin finish_W_col work with last block

            tid = threadIdx%x + (threadIdx%y - 1)*blockDim%x

            laneID = iand(tid, 31)

            if (tid == 1) then

               alphar = 0.0_8

               alphai = 0.0_8

            endif

            call syncthreads()

            rsum = 0.0_8

            isum = 0.0_8

            mytau = tau

            nb = ceiling(real(M)/(blockDim%x*blockDim%y)) ! number of blocks down column

            i = tid

            do j = 1, nb

               ! All threads perform their product, zero if out of bounds

               if (i <= M) then

                  val1 = dconjg(mytau*y2(i))*x(i)

               else

                  val1 = dcmplx(0., 0.)

               endif

               rv1 = dble(val1); iv1 = dimag(val1)

               rsum = rsum + rv1

               isum = isum + iv1

               i = i + blockDim%x*blockDim%y

            end do

            ! Partial sum within warps using shuffle

            rv1 = rsum

            rv2 = __shfl_down(rv1, 1)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 2)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 4)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 8)

            rv1 = rv1 + rv2

            rv2 = __shfl_down(rv1, 16)

            rv1 = rv1 + rv2

            iv1 = isum

            iv2 = __shfl_down(iv1, 1)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 2)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 4)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 8)

            iv1 = iv1 + iv2

            iv2 = __shfl_down(iv1, 16)

            iv1 = iv1 + iv2

            if (laneID == 1) then

               istat = atomicadd(alphar, rv1)

               istat = atomicadd(alphai, iv1)

            endif

            call syncthreads()

            alpha = -dcmplx(0.5, 0.0)*mytau*dcmplx(alphar, alphai)

            do i = tid, M, blockDim%x*blockDim%y

               y2(i) = mytau*y2(i) + alpha*x(i) !zaxpy

            end do

      CALL hipCheck(hipMemcpy(_v,c_loc(v),C_SIZEOF(v),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_w,c_loc(w),C_SIZEOF(w),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_z1,c_loc(z1),C_SIZEOF(z1),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_z2,c_loc(z2),C_SIZEOF(z2),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_x,c_loc(x),C_SIZEOF(x),hipMemcpyHostToDevice))
      CALL hipCheck(hipMemcpy(_y2,c_loc(y2),C_SIZEOF(y2),hipMemcpyHostToDevice))

    end subroutine


end module zhetrd_gpu_kernels