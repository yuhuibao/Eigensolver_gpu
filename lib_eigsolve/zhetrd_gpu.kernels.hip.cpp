// This file was generated by gpufort

#include "hip/hip_complex.h"
#include "hip/hip_runtime.h"
#include "hip/math_functions.h"
#include <cstdio>

namespace {
// make float
float make_float(short int a) { return static_cast<float>(a); }
float make_float(unsigned short int a) { return static_cast<float>(a); }
float make_float(unsigned int a) { return static_cast<float>(a); }
float make_float(int a) { return static_cast<float>(a); }
float make_float(long int a) { return static_cast<float>(a); }
float make_float(unsigned long int a) { return static_cast<float>(a); }
float make_float(long long int a) { return static_cast<float>(a); }
float make_float(unsigned long long int a) { return static_cast<float>(a); }
float make_float(signed char a) { return static_cast<float>(a); }
float make_float(unsigned char a) { return static_cast<float>(a); }
float make_float(float a) { return static_cast<float>(a); }
float make_float(double a) { return static_cast<float>(a); }
float make_float(long double a) { return static_cast<float>(a); }
float make_float(hipFloatComplex &a) { return static_cast<float>(a.x); }
float make_float(hipDoubleComplex &a) { return static_cast<float>(a.x); }
// make double
double make_double(short int a) { return static_cast<double>(a); }
double make_double(unsigned short int a) { return static_cast<double>(a); }
double make_double(unsigned int a) { return static_cast<double>(a); }
double make_double(int a) { return static_cast<double>(a); }
double make_double(long int a) { return static_cast<double>(a); }
double make_double(unsigned long int a) { return static_cast<double>(a); }
double make_double(long long int a) { return static_cast<double>(a); }
double make_double(unsigned long long int a) { return static_cast<double>(a); }
double make_double(signed char a) { return static_cast<double>(a); }
double make_double(unsigned char a) { return static_cast<double>(a); }
double make_double(float a) { return static_cast<double>(a); }
double make_double(double a) { return static_cast<double>(a); }
double make_double(long double a) { return static_cast<double>(a); }
double make_double(hipFloatComplex &a) { return static_cast<double>(a.x); }
__device__ double make_double(hipDoubleComplex &a) { return static_cast<double>(a.x); }
// conjugate complex type
hipFloatComplex conj(hipFloatComplex &c) { return hipConjf(c); }
__device__ hipDoubleComplex conj(hipDoubleComplex &z) { return hipConj(z); }
__device__ double dimag(hipDoubleComplex &a) { return static_cast<double>(a.y); }

// TODO Add the following functions:
// - sign(x,y) = sign(y) * |x| - sign transfer function
// ...
} // namespace
#define divideAndRoundUp(x, y) ((x) / (y) + ((x) % (y) != 0))

// BEGIN krnl_2b8e8f_0
/* Fortran original:
      ! kernel do(1)<<<*,*>>>
      do j = 33, N
         !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
         d(j) = A(j, j)
      end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): -1-1-1._ ('-1' means not specified)
// - Threads per block (CUDA): -1-1-1._ ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_2b8e8f_0(hipDoubleComplex *a,
                              const int a_n1,
                              const int a_n2,
                              const int a_lb1,
                              const int a_lb2,
                              double *d,
                              const int d_n1,
                              const int d_lb1,
                              int n) {
#undef _idx_a
#define _idx_a(a, b) ((a - (a_lb1)) + a_n1 * (b - (a_lb2)))
#undef _idx_d
#define _idx_d(a) ((a - (d_lb1)))

  unsigned int j = 33 + threadIdx.x + blockIdx.x * blockDim.x;
  if ((j <= n)) {
    // !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
    d[_idx_d(j)] = make_double(a[_idx_a(j, j)]);
  }
}

extern "C" void launch_krnl_2b8e8f_0(dim3 *grid,
                                     dim3 *block,
                                     const int sharedMem,
                                     hipStream_t stream,
                                     hipDoubleComplex *a,
                                     const int a_n1,
                                     const int a_n2,
                                     const int a_lb1,
                                     const int a_lb2,
                                     double *d,
                                     const int d_n1,
                                     const int d_lb1,
                                     int n) {
  hipLaunchKernelGGL((krnl_2b8e8f_0), *grid, *block, sharedMem, stream, a, a_n1, a_n2, a_lb1, a_lb2, d, d_n1, d_lb1, n);
}
extern "C" void launch_krnl_2b8e8f_0_auto(const int sharedMem,
                                          hipStream_t stream,
                                          hipDoubleComplex *a,
                                          const int a_n1,
                                          const int a_n2,
                                          const int a_lb1,
                                          const int a_lb2,
                                          double *d,
                                          const int d_n1,
                                          const int d_lb1,
                                          int n) {
  const unsigned int krnl_2b8e8f_0_NX = n;

  const unsigned int krnl_2b8e8f_0_blockX = 256;

  const unsigned int krnl_2b8e8f_0_gridX = divideAndRoundUp(krnl_2b8e8f_0_NX, krnl_2b8e8f_0_blockX);

  dim3 grid(krnl_2b8e8f_0_gridX);
  dim3 block(krnl_2b8e8f_0_blockX);
  hipLaunchKernelGGL((krnl_2b8e8f_0), grid, block, sharedMem, stream, a, a_n1, a_n2, a_lb1, a_lb2, d, d_n1, d_lb1, n);
}
// END krnl_2b8e8f_0

// BEGIN krnl_9c27cb_1
/* Fortran original:
        ! kernel do(1)<<<*,*>>>
        do k = 1, N - 1
           W(k, iw) = dcmplx(0, 0)
        end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): -1-1-1._ ('-1' means not specified)
// - Threads per block (CUDA): -1-1-1._ ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_9c27cb_1(int iw, hipDoubleComplex *w, const int w_n1, const int w_n2, const int w_lb1, const int w_lb2, int n) {
#undef _idx_w
#define _idx_w(a, b) ((a - (w_lb1)) + w_n1 * (b - (w_lb2)))

  unsigned int k = 1 + threadIdx.x + blockIdx.x * blockDim.x;
  if ((k <= (n - 1))) {
    w[_idx_w(k, iw)] = make_hipDoubleComplex(0, 0);
  }
}

extern "C" void launch_krnl_9c27cb_1(dim3 *grid,
                                     dim3 *block,
                                     const int sharedMem,
                                     hipStream_t stream,
                                     int iw,
                                     hipDoubleComplex *w,
                                     const int w_n1,
                                     const int w_n2,
                                     const int w_lb1,
                                     const int w_lb2,
                                     int n) {
  hipLaunchKernelGGL((krnl_9c27cb_1), *grid, *block, sharedMem, stream, iw, w, w_n1, w_n2, w_lb1, w_lb2, n);
}
extern "C" void launch_krnl_9c27cb_1_auto(const int sharedMem,
                                          hipStream_t stream,
                                          int iw,
                                          hipDoubleComplex *w,
                                          const int w_n1,
                                          const int w_n2,
                                          const int w_lb1,
                                          const int w_lb2,
                                          int n) {
  const unsigned int krnl_9c27cb_1_NX = (n - 1);

  const unsigned int krnl_9c27cb_1_blockX = 256;

  const unsigned int krnl_9c27cb_1_gridX = divideAndRoundUp(krnl_9c27cb_1_NX, krnl_9c27cb_1_blockX);

  dim3 grid(krnl_9c27cb_1_gridX);
  dim3 block(krnl_9c27cb_1_blockX);
  hipLaunchKernelGGL((krnl_9c27cb_1), grid, block, sharedMem, stream, iw, w, w_n1, w_n2, w_lb1, w_lb2, n);
}
// END krnl_9c27cb_1

// BEGIN zlarfg_kernel
/* Fortran original:
      implicit none
      integer, value                   :: N
      complex(8), device               :: tau
      real(8), device                  :: e
      complex(8), dimension(N), device :: x

      integer                          :: tid, i, j, nb, istat, laneID
      real(8)                          :: rv1, rv2, rv3, scal, invscal, alphar, alphai, beta, rsum, isum
      complex(8)                       :: cv1
      real(8), shared                  :: xnorm
      complex(8), shared               :: alpha_s

      tid = threadIdx%x
      laneID = iand(tid, 31)

      if (tid == 1) then
         alpha_s = x(N)
         xnorm = 0.0_8
      endif

      call syncthreads()

      alphar = dble(alpha_s)
      alphai = dimag(alpha_s)
      rsum = 0.0_8

      nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

      i = tid
      do j = 1, nb

         ! All threads perform their product, zero if out of bounds
         if (i <= N - 1) then
            cv1 = x(i)
            rv2 = dble(cv1); rv3 = dimag(cv1)
            rv1 = rv2*rv2 + rv3*rv3
         else
            rv1 = 0.0_8
         endif

         rsum = rsum + rv1

         i = i + blockDim%x
      end do

      ! Partial sum within warps using shuffle
      rv1 = rsum
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      if (laneID == 1) then
         istat = atomicadd(xnorm, rv1)
      endif

      call syncthreads()

      if (xnorm == 0.0_8 .and. alphai == 0.0_8) then
         if (tid == 1) then
            tau = 0.0_8
         endif
      else
         if (tid == 1) then
            xnorm = sqrt(xnorm)

            rv1 = abs(alphar)
            rv2 = abs(alphai)
            ! not taking abs of xnorm
            scal = max(rv1, rv2, xnorm)
            invscal = 1.d0/scal

            rv1 = rv1*invscal
            rv2 = rv2*invscal
            xnorm = xnorm*invscal

            beta = -sign(scal*sqrt(rv1*rv1 + rv2*rv2 + xnorm*xnorm), alphar)

            tau = dcmplx((beta - alphar)/beta, -alphai/beta)

            !zladiv
            rv1 = dble(alpha_s - beta)
            rv2 = dimag(alpha_s - beta)

            if (abs(rv2) .lt. abs(rv1)) then
               xnorm = rv2/rv1
               invscal = 1.d0/(rv1 + rv2*xnorm)
               alpha_s = dcmplx(invscal, -xnorm*invscal)
            else
               xnorm = rv1/rv2
               invscal = 1.d0/(rv2 + rv1*xnorm)
               alpha_s = dcmplx(xnorm*invscal, -invscal)
            endif

            e = beta ! store beta in e vector
         endif

         call syncthreads()

         do i = tid, N, blockDim%x
            cv1 = x(i)

            if (i <= N - 1) then
               cv1 = alpha_s*cv1
            elseif (i == N) then
               !x(i) = 1.0_8
               cv1 = dcmplx(1.0_8, 0.0_8)
            endif

            x(i) = cv1
         end do

      endif


*/

__global__ void zlarfg_kernel(int n, hipDoubleComplex tau, double e, hipDoubleComplex *x, const int x_n1, const int x_lb1) {
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))

  int tid;
  int i;
  int j;
  int nb;
  int istat;
  int laneid;
  double rv1;
  double rv2;
  double rv3;
  double scal;
  double invscal;
  double alphar;
  double alphai;
  double beta;
  double rsum;
  double isum;
  hipDoubleComplex cv1;
  __shared__ double xnorm;             /* Fortran qualifiers: SHARED */
  __shared__ hipDoubleComplex alpha_s; /* Fortran qualifiers: SHARED */
  __shared__ hipDoubleComplex tmp0;
  tid = threadIdx.x + 1;
  laneid = tid & 31;
  if (tid == 1) {
    alpha_s = x[_idx_x(n)];
    xnorm = 0.0 /*_8*/;
  }
  __syncthreads();
  alphar = make_double(alpha_s);
  alphai = dimag(alpha_s);
  rsum = 0.0 /*_8*/;
  nb = ceil((float(n) / blockDim.x));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= (n - 1))) {
      cv1 = x[_idx_x(i)];
      rv2 = make_double(cv1);
      rv3 = dimag(cv1);
      rv1 = (rv2 * rv2 + rv3 * rv3);

    } else {
      rv1 = 0.0 /*_8*/;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if (laneid == 1) {
    istat = atomicAdd(&xnorm, rv1);
  }
  __syncthreads();
  if (xnorm == 0.0 /*_8*/ && alphai == 0.0 /*_8*/) {
    if (tid == 1) {
      tau = make_hipDoubleComplex(0.0,0.0) /*_8*/;
    }
  }else {
    if (tid == 1) {
    xnorm = sqrt(xnorm);
    rv1 = abs(alphar);
    rv2 = abs(alphai);
    // ! not taking abs of xnorm
    scal = max(max(rv1, rv2), xnorm);
    invscal = (1.e0 / scal);
    rv1 = (rv1 * invscal);
    rv2 = (rv2 * invscal);
    xnorm = (xnorm * invscal);
    if (alphar >= 0){
        beta = -abs(scal * sqrt(rv1 * rv1 + rv2 * rv2 + xnorm * xnorm));
    } else {
       beta = abs(scal * sqrt(rv1 * rv1 + rv2 * rv2 + xnorm * xnorm)); 
    }
    tau = make_hipDoubleComplex(((beta - alphar) / beta), (-alphai / beta));
    // !zladiv
    tmp0 = alpha_s - make_hipDoubleComplex(beta,0);
    rv1 = make_double(tmp0);
    rv2 = dimag(tmp0);
    if ((abs(rv2) < abs(rv1))) {
      xnorm = (rv2 / rv1);
      invscal = (1.e0 / (rv1 + rv2 * xnorm));
      alpha_s = make_hipDoubleComplex(invscal, (-xnorm * invscal));

    } else {
      xnorm = (rv1 / rv2);
      invscal = (1.e0 / (rv2 + rv1 * xnorm));
      alpha_s = make_hipDoubleComplex((xnorm * invscal), -invscal);
    }
    e = beta;
    // ! store beta in e vector
  }
  __syncthreads(); 
  
  for (int i = tid; i <= n; i += blockDim.x) {
    cv1 = x[_idx_x(i)];
    if ((i <= (n - 1))) {
      cv1 = (alpha_s * cv1);

    } else if (i == n){
      // !x(i) = 1.0_8
      cv1 = make_hipDoubleComplex(1.0 /*_8*/, 0.0 /*_8*/);
    }
    x[_idx_x(i)] = cv1;

  } 
}
}

extern "C" void launch_zlarfg_kernel(dim3 *grid,
                                     dim3 *block,
                                     const int sharedMem,
                                     hipStream_t stream,
                                     int n,
                                     hipDoubleComplex tau,
                                     double e,
                                     hipDoubleComplex *x,
                                     const int x_n1,
                                     const int x_lb1) {
  hipLaunchKernelGGL((zlarfg_kernel), *grid, *block, sharedMem, stream, n, tau, e, x, x_n1, x_lb1);
}
// END zlarfg_kernel

// BEGIN zher2_mv_zlarfg_kernel
/* Fortran original:
      implicit none
      integer, value                                        :: N, M, ldv, ldw, ldw2
      complex(8), dimension(1:ldv, 1:M), device, intent(in) :: V
      complex(8), dimension(1:ldw, 1:M), device, intent(in) :: W
      complex(8), dimension(1:ldw2, 2), device              :: W2
      !DIR$ IGNORE_TKR x
      real(8), dimension(1:2*N), device                     :: x
      complex(8), dimension(1:N), device                    :: x2
      complex(8), device                                    :: tau
      real(8), device                                       :: e

      integer                                               :: i, j, tx, ty, tid, nb, laneid, istat, nBlocks
      integer, device                                       :: finished
      integer, shared                                       :: nFinished
      complex(8)                                            :: val
      real(8)                                               :: rv, iv
      real(8)                                               :: rv1, rv2, rv3, scal, invscal, alphar, alphai, beta, rsum, isum
      complex(8)                                            :: cv1
      real(8), shared                                       :: xnorm
      complex(8), shared                                    :: alpha_s

      tx = threadIdx%x
      ty = threadIdx%y
      i = (blockIdx%x - 1)*blockDim%x + tx
      j = (blockIdx%y - 1)*blockDim%y + ty

      nBlocks = gridDim%x*gridDim%y

      !if (i > N .or. j > M) return
      if (i <= N .and. j <= M) then

         val = -conjg(W(N, j))*V(i, j) - conjg(V(N, j))*W(i, j)
         rv = dble(val)
         iv = dimag(val)

         ! Zero out imaginary part on diagonal
         if (i == N) then
            iv = 0.d0
         endif

         ! Update x
         istat = atomicadd(x(2*i - 1), rv)
         istat = atomicadd(x(2*i), iv)
      endif

      if (ty == 1) then
         ! Zero out column for zhemv call
         if (i <= N) W2(i, 1) = 0
         ! Zero out workspace for intermediate zgemv results
         if (i <= M) then
            W2(N + i, 1) = 0
            W2(N + i, 2) = 0
         endif
      endif

      call threadfence()

      nFinished = 0
      call syncthreads()
      if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)
      call syncthreads()

      if (nFinished < nBlocks - 1) return

      ! Begin zlarfg work with last block
      if (N == 1) return

      tid = tx + (ty - 1)*blockDim%x
      laneID = iand(tid, 31)

      if (tid == 1) then
         alpha_s = x2(N - 1)
         xnorm = 0.0_8
      endif

      call syncthreads()

      alphar = dble(alpha_s)
      alphai = dimag(alpha_s)
      rsum = 0.0_8

      nb = ceiling(real(N - 1)/(blockDim%x*blockDim%y)) ! number of blocks down column

      i = tid
      do j = 1, nb

         ! All threads perform their product, zero if out of bounds
         if (i <= N - 2) then
            cv1 = x2(i)
            rv2 = dble(cv1); rv3 = dimag(cv1)
            rv1 = rv2*rv2 + rv3*rv3
         else
            rv1 = 0.0_8
         endif

         rsum = rsum + rv1

         i = i + blockDim%x*blockDim%y
      end do

      ! Partial sum within warps using shuffle
      rv1 = rsum
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      if (laneID == 1) then
         istat = atomicadd(xnorm, rv1)
      endif

      call syncthreads()

      if (xnorm == 0.0_8 .and. alphai == 0.0_8) then
         if (tid == 1) then
            tau = 0.0_8
         endif
      else
         if (tid == 1) then
            xnorm = sqrt(xnorm)

            rv1 = abs(alphar)
            rv2 = abs(alphai)
            ! not taking abs of xnorm
            scal = max(rv1, rv2, xnorm)
            invscal = 1.d0/scal

            rv1 = rv1*invscal
            rv2 = rv2*invscal
            xnorm = xnorm*invscal

            beta = -sign(scal*sqrt(rv1*rv1 + rv2*rv2 + xnorm*xnorm), alphar)

            tau = dcmplx((beta - alphar)/beta, -alphai/beta)

            !zladiv
            rv1 = dble(alpha_s - beta)
            rv2 = dimag(alpha_s - beta)

            if (abs(rv2) .lt. abs(rv1)) then
               xnorm = rv2/rv1
               invscal = 1.d0/(rv1 + rv2*xnorm)
               alpha_s = dcmplx(invscal, -xnorm*invscal)
            else
               xnorm = rv1/rv2
               invscal = 1.d0/(rv2 + rv1*xnorm)
               alpha_s = dcmplx(xnorm*invscal, -invscal)
            endif

            e = beta ! store beta in e vector
         endif

         call syncthreads()

         do i = tid, N - 1, blockDim%x*blockDim%y
            cv1 = x2(i)

            if (i <= N - 2) then
               cv1 = alpha_s*cv1
            elseif (i == N - 1) then
               !x(i) = 1.0_8
               cv1 = dcmplx(1.0_8, 0.0_8)
            endif

            x2(i) = cv1
         end do

      endif


*/

__global__ void zher2_mv_zlarfg_kernel(int n,
                                       int m,
                                       int ldv,
                                       int ldw,
                                       int ldw2,
                                       hipDoubleComplex *v,
                                       const int v_n1,
                                       const int v_lb1,
                                       const int v_lb2,
                                       hipDoubleComplex *w,
                                       const int w_n1,
                                       const int w_lb1,
                                       const int w_lb2,
                                       hipDoubleComplex *w2,
                                       const int w2_n1,
                                       const int w2_lb1,
                                       const int w2_lb2,
                                       double *x,
                                       const int x_n1,
                                       const int x_lb1,
                                       hipDoubleComplex *x2,
                                       const int x2_n1,
                                       const int x2_lb1,
                                       hipDoubleComplex tau,
                                       double e,
                                       unsigned int finished) {
#undef _idx_v
#define _idx_v(a, b) ((a - (v_lb1)) + v_n1 * (b - (v_lb2)))
#undef _idx_w
#define _idx_w(a, b) ((a - (w_lb1)) + w_n1 * (b - (w_lb2)))
#undef _idx_w2
#define _idx_w2(a, b) ((a - (w2_lb1)) + w2_n1 * (b - (w2_lb2)))
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_x2
#define _idx_x2(a) ((a - (x2_lb1)))

  // !DIR$ IGNORE_TKR x
  int i;
  int j;
  int tx;
  int ty;
  int tid;
  int nb;
  int laneid;
  int istat;
  int nblocks;
  __shared__ int nfinished; /* Fortran qualifiers: SHARED */
  hipDoubleComplex val;
  double rv;
  double iv;
  double rv1;
  double rv2;
  double rv3;
  double scal;
  double invscal;
  double alphar;
  double alphai;
  double beta;
  double rsum;
  double isum;
  hipDoubleComplex cv1;
  __shared__ double xnorm;             /* Fortran qualifiers: SHARED */
  __shared__ hipDoubleComplex alpha_s; /* Fortran qualifiers: SHARED */
  __shared__ hipDoubleComplex tmp0;
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = ((blockIdx.x) * blockDim.x + tx);
  j = ((blockIdx.y) * blockDim.y + ty);
  nblocks = (gridDim.x * gridDim.y);
  // !if (i > N .or. j > M) return
  if ((i <= n && j <= m)) {
    val = -conj(w[_idx_w(n, j)]) * v[_idx_v(i, j)] - conj(v[_idx_v(n, j)]) * w[_idx_w(i, j)];
    rv = make_double(val);
    iv = dimag(val);
    // ! Zero out imaginary part on diagonal
    if (i == n) {
      iv = 0.e0;
    }
    // ! Update x
    istat = atomicAdd(x + _idx_x((2 * i - 1))*16, rv);
    istat = atomicAdd(x + _idx_x((2 * i))*16, iv);
  }
  if (ty == 1) {
    // ! Zero out column for zhemv call
    if ((i <= n)) {
      w2[_idx_w2(i, 1)] = make_hipDoubleComplex(0,0);

    } // ! Zero out workspace for intermediate zgemv results
    if ((i <= m)) {
      w2[_idx_w2((n + i), 1)] = make_hipDoubleComplex(0,0);
      w2[_idx_w2((n + i), 2)] = make_hipDoubleComplex(0,0);
    }
  }
  __threadfence();
  nfinished = 0;
  __syncthreads();
  if ((tx + ty) == 2) {
    nfinished = atomicInc(&finished, (nblocks - 1));
  }
  __syncthreads(); 
  if ((nfinished < (nblocks - 1))) {
    return; // ! Begin dlarfg work with last block
  }
  if (n == 1) {
    return;
  }
  tid = (tx + (ty - 1) * blockDim.x);
  laneid = tid & 31;
  if (tid == 1) {
    alpha_s = x2[_idx_x2((n - 1))];
    xnorm = 0.0 /*_8*/;
  }
  __syncthreads();
  alphar = make_double(alpha_s);
  alphai = dimag(alpha_s);
  rsum = 0.0 /*_8*/;
  nb = ceil(float(n - 1) / (blockDim.x * blockDim.y));
  // ! number of blocks down column
  i = tid;
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= (n - 2))) {
      cv1 = x2[_idx_x2(i)];
      rv2 = make_double(cv1);
      rv3 = dimag(cv1);
      rv1 = (rv2 * rv2 + rv3 * rv3);

    } else {
      rv1 = 0.0 /*_8*/;
    }
    rsum = (rsum + rv1);
    i = (i + blockDim.x * blockDim.y);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  if (laneid == 1) {
    istat = atomicAdd(&xnorm, rv1);
  }
  __syncthreads();
  if ((xnorm == 0.0 /*_8*/ & alphai == 0.0 /*_8*/)) {
    if (tid == 1) {
      tau = make_hipDoubleComplex(0.0,0.0) /*_8*/;
    }
  }else {
    if (tid == 1) {
    xnorm = sqrt(xnorm);
    rv1 = abs(alphar);
    rv2 = abs(alphai);
    // ! not taking abs of xnorm
    scal = max(max(rv1, rv2), xnorm);
    invscal = (1.e0 / scal);
    rv1 = (rv1 * invscal);
    rv2 = (rv2 * invscal);
    xnorm = (xnorm * invscal);
    if (alphar >= 0){
        beta = -abs(scal * sqrt(rv1 * rv1 + rv2 * rv2 + xnorm * xnorm));
    } else {
       beta = abs(scal * sqrt(rv1 * rv1 + rv2 * rv2 + xnorm * xnorm)); 
    }
    tau = make_hipDoubleComplex(((beta - alphar) / beta), (-alphai / beta));
    // !zladiv
    tmp0 = alpha_s - make_hipDoubleComplex(beta,0);
    rv1 = make_double(tmp0);
    rv2 = dimag(tmp0);
    if ((abs(rv2) < abs(rv1))) {
      xnorm = (rv2 / rv1);
      invscal = (1.e0 / (rv1 + rv2 * xnorm));
      alpha_s = make_hipDoubleComplex(invscal, (-xnorm * invscal));

    } else {
      xnorm = (rv1 / rv2);
      invscal = (1.e0 / (rv2 + rv1 * xnorm));
      alpha_s = make_hipDoubleComplex((xnorm * invscal), -invscal);
    }
    e = beta;
    // ! store beta in e vector
  }
  __syncthreads();
  for (int i = tid; i <= (n - 1); i += (blockDim.x * blockDim.y)) {
    cv1 = x2[_idx_x2(i)];
    if ((i <= (n - 2))) {
      cv1 = (alpha_s * cv1);

    } else if (i == (n - 1)) {
      // !x(i) = 1.0_8
      cv1 = make_hipDoubleComplex(1.0 /*_8*/, 0.0 /*_8*/);
    }
    x2[_idx_x2(i)] = cv1;

  } 
}
                                       }
extern "C" void launch_zher2_mv_zlarfg_kernel(dim3 *grid,
                                              dim3 *block,
                                              const int sharedMem,
                                              hipStream_t stream,
                                              int n,
                                              int m,
                                              int ldv,
                                              int ldw,
                                              int ldw2,
                                              hipDoubleComplex *v,
                                              const int v_n1,
                                              const int v_lb1,
                                              const int v_lb2,
                                              hipDoubleComplex *w,
                                              const int w_n1,
                                              const int w_lb1,
                                              const int w_lb2,
                                              hipDoubleComplex *w2,
                                              const int w2_n1,
                                              const int w2_lb1,
                                              const int w2_lb2,
                                              double *x,
                                              const int x_n1,
                                              const int x_lb1,
                                              hipDoubleComplex *x2,
                                              const int x2_n1,
                                              const int x2_lb1,
                                              hipDoubleComplex tau,
                                              double e,
                                              unsigned int finished) {
  hipLaunchKernelGGL((zher2_mv_zlarfg_kernel),
                     *grid,
                     *block,
                     sharedMem,
                     stream,
                     n,
                     m,
                     ldv,
                     ldw,
                     ldw2,
                     v,
                     v_n1,
                     v_lb1,
                     v_lb2,
                     w,
                     w_n1,
                     w_lb1,
                     w_lb2,
                     w2,
                     w2_n1,
                     w2_lb1,
                     w2_lb2,
                     x,
                     x_n1,
                     x_lb1,
                     x2,
                     x2_n1,
                     x2_lb1,
                     tau,
                     e,
                     finished);
}
// END zher2_mv_zlarfg_kernel

// BEGIN stacked_zgemv_c
/* Fortran original:
      use cudafor
      implicit none
      integer, value                                     :: M, N, ldv, ldw
      complex(8), dimension(ldv, M), device, intent(in)  :: V
      complex(8), dimension(ldw, M), device, intent(in)  :: W
      complex(8), dimension(N), device, intent(in)       :: x
      !DIR$ IGNORE_TKR z1, z2
      real(8), dimension(2*M), device                    :: z1, z2
      !complex(8), dimension(M), device, intent(in)        :: z1, z2

      !real(8), dimension(32), shared                     :: r_s
      !real(8), dimension(32), shared                     :: i_s

      integer :: i, j, tx, ty, istat
      complex(8) :: val
      real(8) :: rv1, rv2, iv1, iv2, xr, xi

      tx = threadIdx%x
      ty = threadIdx%y

      i = (blockIdx%y - 1)*blockDim%y + ty
      j = (blockIdx%x - 1)*blockDim%x + tx

      !if (i > 2*M .or. j > N) return
      if (i > 2*M) return

      val = x(j)
      xr = dble(val); xi = dimag(val)

      if (j > N) then
         !val = dcmplx(0,0)
         rv1 = 0.d0; iv1 = 0.d0
      else
         if (i > M) then
            val = W(j, i - M)
         else
            val = V(j, i)
         endif

         rv2 = dble(val); iv2 = dimag(val)

         rv1 = rv2*xr + iv2*xi
         iv1 = rv2*xi - iv2*xr
      endif

      !Partial sum within warps using shuffle
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      !if (tx == 1) then
      !r_s(ty + k*blockDim%y) = rv1
      !r_s(ty) = rv1
      !endif

      !Partial sum within warps using shuffle
      iv2 = __shfl_down(iv1, 1)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 2)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 4)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 8)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 16)
      iv1 = iv1 + iv2

      !if (tx == 1) then
      !i_s(ty + k*blockDim%y) = iv1
      !i_s(ty) = iv1
      !endif

      !call syncthreads()

      !if (ty == 1 .and. i+tx-1 <= 2*M) then
      !  if (i+tx-1 > M) then
      !    istat = atomicadd(z2(2*(i+tx-1-M) - 1), r_s(tx))
      !    istat = atomicadd(z2(2*(i+tx-1-M)), i_s(tx))
      !  else
      !    istat = atomicadd(z1(2*(i+tx-1) - 1), r_s(tx))
      !    istat = atomicadd(z1(2*(i+tx-1)), i_s(tx))
      !  endif
      !endif

      if (tx == 1) then
         if (i > M) then
            istat = atomicadd(z2(2*(i - M) - 1), rv1)
            istat = atomicadd(z2(2*(i - M)), iv1)
         else
            istat = atomicadd(z1(2*i - 1), rv1)
            istat = atomicadd(z1(2*i), iv1)
         endif
      endif

      return

*/

__global__ void stacked_zgemv_c(int m,
                                int n,
                                int ldv,
                                int ldw,
                                hipDoubleComplex *v,
                                const int v_n1,
                                const int v_lb1,
                                const int v_lb2,
                                hipDoubleComplex *w,
                                const int w_n1,
                                const int w_lb1,
                                const int w_lb2,
                                hipDoubleComplex *x,
                                const int x_n1,
                                const int x_lb1,
                                double *z1,
                                const int z1_n1,
                                const int z1_lb1,
                                double *z2,
                                const int z2_n1,
                                const int z2_lb1) {
#undef _idx_v
#define _idx_v(a, b) ((a - (v_lb1)) + v_n1 * (b - (v_lb2)))
#undef _idx_w
#define _idx_w(a, b) ((a - (w_lb1)) + w_n1 * (b - (w_lb2)))
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_z1
#define _idx_z1(a) ((a - (z1_lb1)))
#undef _idx_z2
#define _idx_z2(a) ((a - (z2_lb1)))

  // !DIR$ IGNORE_TKR z1, z2
  // !complex(8), dimension(M), device, intent(in)        :: z1, z2
  // !real(8), dimension(32), shared                     :: r_s
  // !real(8), dimension(32), shared                     :: i_s
  int i;
  int j;
  int tx;
  int ty;
  int istat;
  hipDoubleComplex val;
  double rv1;
  double rv2;
  double iv1;
  double iv2;
  double xr;
  double xi;
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = ((blockIdx.y) * blockDim.y + ty);
  j = ((blockIdx.x) * blockDim.x + tx);
  // !if (i > 2*M .or. j > N) return
  if ((i > (2 * m))) {
    return;
  }
  val = x[_idx_x(j)];
  xr = make_double(val);
  xi = dimag(val);
  if ((j > n)) {
    // !val = dcmplx(0,0)
    rv1 = 0.e0;
    iv1 = 0.e0;

  } else{
        if ((i > m)) {
    val = w[_idx_w(j, (i - m))];

  } else {
    val = v[_idx_v(j, i)];
  }
  rv2 = make_double(val);
  iv2 = dimag(val);
  
  rv1 = (rv2 * xr + iv2 * xi);
  iv1 = (rv2 * xi - iv2 * xr);
  }
  // ! TODO could not parse:        endif
  // !Partial sum within warps using shuffle
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  // !if (tx == 1) then
  // !r_s(ty + k*blockDim%y) = rv1
  // !r_s(ty) = rv1
  // !endif
  // !Partial sum within warps using shuffle
  iv2 = __shfl_down(iv1, 1);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 2);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 4);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 8);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 16);
  iv1 = (iv1 + iv2);
  // !if (tx == 1) then
  // !i_s(ty + k*blockDim%y) = iv1
  // !i_s(ty) = iv1
  // !endif
  // !call __syncthreads()
  // !if (ty == 1 .and. i+tx-1 <= 2*M) then
  // !  if (i+tx-1 > M) then
  // !    istat = atomicAdd(z2(2*(i+tx-1-M) - 1), r_s(tx))
  // !    istat = atomicAdd(z2(2*(i+tx-1-M)), i_s(tx))
  // !  else
  // !    istat = atomicAdd(z1(2*(i+tx-1) - 1), r_s(tx))
  // !    istat = atomicAdd(z1(2*(i+tx-1)), i_s(tx))
  // !  endif
  // !endif
  if (tx == 1) {
    if ((i > m)) {
      istat = atomicAdd(z2 + _idx_z2((2 * (i - m) - 1))*8, rv1);
      istat = atomicAdd(z2 + _idx_z2((2 * (i - m)))*8, iv1);

    } else {
      istat = atomicAdd(z1 + _idx_z1((2 * i - 1))*8, rv1);
      istat = atomicAdd(z1 + _idx_z1((2 * i))*8, iv1);
    }
  }
  return;
}

extern "C" void launch_stacked_zgemv_c(dim3 *grid,
                                       dim3 *block,
                                       const int sharedMem,
                                       hipStream_t stream,
                                       int m,
                                       int n,
                                       int ldv,
                                       int ldw,
                                       hipDoubleComplex *v,
                                       const int v_n1,
                                       const int v_n2,
                                       const int v_lb1,
                                       const int v_lb2,
                                       hipDoubleComplex *w,
                                       const int w_n1,
                                       const int w_n2,
                                       const int w_lb1,
                                       const int w_lb2,
                                       hipDoubleComplex *x,
                                       const int x_n1,
                                       const int x_lb1,
                                       double *z1,
                                       const int z1_n1,
                                       const int z1_lb1,
                                       double *z2,
                                       const int z2_n1,
                                       const int z2_lb1) {
  hipLaunchKernelGGL((stacked_zgemv_c),
                     *grid,
                     *block,
                     sharedMem,
                     stream,
                     m,
                     n,
                     ldv,
                     ldw,
                     v,
                     v_n1,
                     
                     v_lb1,
                     v_lb2,
                     w,
                     w_n1,
                     
                     w_lb1,
                     w_lb2,
                     x,
                     x_n1,
                     x_lb1,
                     z1,
                     z1_n1,
                     z1_lb1,
                     z2,
                     z2_n1,
                     z2_lb1);
}
// END stacked_zgemv_c


// BEGIN finish_w_col_kernel
/* Fortran original:
      implicit none
      integer, value                               :: N
      complex(8), device                           :: tau
      complex(8), dimension(N), device, intent(in) :: x
      complex(8), dimension(N), device             :: y

      integer                                      :: tid, i, j, k, nb, istat, laneID
      real(8)                                      :: rv1, rv2, iv1, iv2, rsum, isum
      complex(8)                                   :: val, cv1, mytau

      real(8), shared                              :: alphar, alphai
      !complex(8), shared                          :: alpha
      complex(8)                                   :: alpha

      tid = threadIdx%x
      laneID = iand(tid, 31)

      if (tid == 1) then
         alphar = 0.0_8
         alphai = 0.0_8
      endif

      call syncthreads()

      rsum = 0.0_8
      isum = 0.0_8
      mytau = tau

      nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

      i = tid
      do j = 1, nb

         ! All threads perform their product, zero if out of bounds
         if (i <= N) then
            val = dconjg(mytau*y(i))*x(i)
         else
            val = dcmplx(0., 0.)
         endif

         rv1 = dble(val); iv1 = dimag(val)

         rsum = rsum + rv1
         isum = isum + iv1

         i = i + blockDim%x

      end do

      ! Partial sum within warps using shuffle
      rv1 = rsum
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      iv1 = isum
      iv2 = __shfl_down(iv1, 1)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 2)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 4)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 8)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 16)
      iv1 = iv1 + iv2

      if (laneID == 1) then
         istat = atomicadd(alphar, rv1)
         istat = atomicadd(alphai, iv1)
      endif

      call syncthreads()

      alpha = -dcmplx(0.5, 0.0)*mytau*dcmplx(alphar, alphai)

      do i = tid, N, blockDim%x
         y(i) = mytau*y(i) + alpha*x(i) !zaxpy
      end do


*/

__global__ void finish_w_col_kernel(int n,
                                    hipDoubleComplex tau,
                                    hipDoubleComplex *x,
                                    const int x_n1,
                                    const int x_lb1,
                                    hipDoubleComplex *y,
                                    const int y_n1,
                                    const int y_lb1) {
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_y
#define _idx_y(a) ((a - (y_lb1)))

  int tid;
  int i;
  int j;
  int k;
  int nb;
  int istat;
  int laneid;
  double rv1;
  double rv2;
  double iv1;
  double iv2;
  double rsum;
  double isum;
  hipDoubleComplex val;
  hipDoubleComplex cv1;
  hipDoubleComplex mytau;
  hipDoubleComplex tmp0;
  __shared__ double alphar; /* Fortran qualifiers: SHARED */
  __shared__ double alphai; /* Fortran qualifiers: SHARED */
  // !complex(8), shared                          :: alpha
  hipDoubleComplex alpha;
  tid = threadIdx.x + 1;
  laneid = tid & 31;
  if (tid == 1) {
    alphar = 0.0 /*_8*/;
    alphai = 0.0 /*_8*/;
  }
  __syncthreads();
  rsum = 0.0 /*_8*/;
  isum = 0.0 /*_8*/;
  mytau = tau;
  nb = ceil((float(n) / blockDim.x));
  // ! number of blocks down column
  i = tid;
  // ! TODO could not parse:        do j = 1, nb
  // ! All threads perform their product, zero if out of bounds
  // ! TODO could not parse:           if (i <= n) then
  
  // ! TODO could not parse:           else
  // ! TODO could not parse:              val = dcmplx(0., 0.)
  // ! TODO could not parse:           endif
  
  // ! TODO could not parse:        end do
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= n)) {
      tmp0 = mytau * y[_idx_y(i)];
      val = conj(tmp0) * x[_idx_x(i)];

    } else {
      val = make_hipDoubleComplex(0, 0);
    }
    rv1 = make_double(val);
  iv1 = dimag(val);
  rsum = (rsum + rv1);
  isum = (isum + iv1);
  i = (i + blockDim.x);

  } // ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  iv1 = isum;
  iv2 = __shfl_down(iv1, 1);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 2);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 4);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 8);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 16);
  iv1 = (iv1 + iv2);
  if (laneid == 1) {
    istat = atomicAdd(&alphar, rv1);
    istat = atomicAdd(&alphai, iv1);
  }
  __syncthreads();
  alpha = (-make_hipDoubleComplex(0.5, 0.0) * mytau * make_hipDoubleComplex(alphar, alphai));
  for (int i = tid; i <= n; i += blockDim.x) {
    y[_idx_y(i)] = (mytau * y[_idx_y(i)] + alpha * x[_idx_x(i)]);
    // !zaxpy
  }
}

extern "C" void launch_finish_w_col_kernel(dim3 *grid,
                                           dim3 *block,
                                           const int sharedMem,
                                           hipStream_t stream,
                                           int n,
                                           hipDoubleComplex tau,
                                           hipDoubleComplex *x,
                                           const int x_n1,
                                           const int x_lb1,
                                           hipDoubleComplex *y,
                                           const int y_n1,
                                           const int y_lb1) {
  hipLaunchKernelGGL((finish_w_col_kernel), *grid, *block, sharedMem, stream, n, tau, x, x_n1, x_lb1, y, y_n1, y_lb1);
}
// END finish_w_col_kernel

// BEGIN stacked_zgemv_n_finish_w
/* Fortran original:
      use cudafor
      implicit none
      integer, value                                     :: M, N, ldv, ldw
      complex(8), dimension(ldv, N), device, intent(in)  :: V
      complex(8), dimension(ldw, N), device, intent(in)  :: W
      complex(8), dimension(N), device, intent(in)       :: z1, z2
      !DIR$ IGNORE_TKR y
      real(8), dimension(2*M), device                    :: y
      complex(8), device                                 :: tau
      complex(8), dimension(M), device, intent(in)       :: x
      complex(8), dimension(M), device                   :: y2
      integer, device                                    :: finished

      integer :: i, j, tx, ty, istat, nBlocks, tid, laneID, nb
      integer, shared :: nFinished
      complex(8) :: val1, val2, mytau, alpha
      real(8) :: rv1, rv2, iv1, iv2, xr, xi, rsum, isum
      real(8), shared :: alphar, alphai

      tx = threadIdx%x
      ty = threadIdx%y

      i = (blockIdx%x - 1)*blockDim%x + tx
      j = (blockIdx%y - 1)*blockDim%y + ty

      nBlocks = gridDim%x*gridDim%y

      if (i <= M .and. j <= 2*N) then
         if (j > N) then
            val1 = z2(j - N)
            val2 = V(i, j - N)
         else
            val1 = z1(j)
            val2 = W(i, j)
         endif
         xr = dble(val1); xi = dimag(val1)
         rv2 = dble(val2); iv2 = dimag(val2)

         rv1 = -rv2*xr + iv2*xi
         iv1 = -rv2*xi - iv2*xr

         istat = atomicadd(y(2*i - 1), rv1)
         istat = atomicadd(y(2*i), iv1)
      endif

      call threadfence()

      nFinished = 0
      call syncthreads()
      if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)
      call syncthreads()

      if (nFinished < nBlocks - 1) return

      ! Begin finish_W_col work with last block
      tid = threadIdx%x + (threadIdx%y - 1)*blockDim%x
      laneID = iand(tid, 31)

      if (tid == 1) then
         alphar = 0.0_8
         alphai = 0.0_8
      endif

      call syncthreads()

      rsum = 0.0_8
      isum = 0.0_8
      mytau = tau

      nb = ceiling(real(M)/(blockDim%x*blockDim%y)) ! number of blocks down column

      i = tid
      do j = 1, nb

         ! All threads perform their product, zero if out of bounds
         if (i <= M) then
            val1 = dconjg(mytau*y2(i))*x(i)
         else
            val1 = dcmplx(0., 0.)
         endif

         rv1 = dble(val1); iv1 = dimag(val1)

         rsum = rsum + rv1
         isum = isum + iv1

         i = i + blockDim%x*blockDim%y

      end do

      ! Partial sum within warps using shuffle
      rv1 = rsum
      rv2 = __shfl_down(rv1, 1)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 2)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 4)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 8)
      rv1 = rv1 + rv2
      rv2 = __shfl_down(rv1, 16)
      rv1 = rv1 + rv2

      iv1 = isum
      iv2 = __shfl_down(iv1, 1)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 2)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 4)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 8)
      iv1 = iv1 + iv2
      iv2 = __shfl_down(iv1, 16)
      iv1 = iv1 + iv2

      if (laneID == 1) then
         istat = atomicadd(alphar, rv1)
         istat = atomicadd(alphai, iv1)
      endif

      call syncthreads()

      alpha = -dcmplx(0.5, 0.0)*mytau*dcmplx(alphar, alphai)

      do i = tid, M, blockDim%x*blockDim%y
         y2(i) = mytau*y2(i) + alpha*x(i) !zaxpy
      end do


*/

__global__ void stacked_zgemv_n_finish_w(int m,
                                         int n,
                                         int ldv,
                                         int ldw,
                                         hipDoubleComplex *v,
                                         const int v_n1,
                                         const int v_lb1,
                                         const int v_lb2,
                                         hipDoubleComplex *w,
                                         const int w_n1,
                                         const int w_lb1,
                                         const int w_lb2,
                                         hipDoubleComplex *z1,
                                         const int z1_n1,
                                         const int z1_lb1,
                                         hipDoubleComplex *z2,
                                         const int z2_n1,
                                         const int z2_lb1,
                                         double *y,
                                         const int y_n1,
                                         const int y_lb1,
                                         hipDoubleComplex tau,
                                         hipDoubleComplex *x,
                                         const int x_n1,
                                         const int x_lb1,
                                         hipDoubleComplex *y2,
                                         const int y2_n1,
                                         const int y2_lb1,
                                         unsigned int finished) {
#undef _idx_v
#define _idx_v(a, b) ((a - (v_lb1)) + v_n1 * (b - (v_lb2)))
#undef _idx_w
#define _idx_w(a, b) ((a - (w_lb1)) + w_n1 * (b - (w_lb2)))
#undef _idx_z1
#define _idx_z1(a) ((a - (z1_lb1)))
#undef _idx_z2
#define _idx_z2(a) ((a - (z2_lb1)))
#undef _idx_y
#define _idx_y(a) ((a - (y_lb1)))
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_y2
#define _idx_y2(a) ((a - (y2_lb1)))

  // !DIR$ IGNORE_TKR y
  int i;
  int j;
  int tx;
  int ty;
  int istat;
  int nblocks;
  int tid;
  int laneid;
  int nb;
  __shared__ int nfinished; /* Fortran qualifiers: SHARED */
  hipDoubleComplex val1;
  hipDoubleComplex val2;
  hipDoubleComplex mytau;
  hipDoubleComplex alpha;
  hipDoubleComplex tmp0;
  double rv1;
  double rv2;
  double iv1;
  double iv2;
  double xr;
  double xi;
  double rsum;
  double isum;
  __shared__ double alphar; /* Fortran qualifiers: SHARED */
  __shared__ double alphai; /* Fortran qualifiers: SHARED */
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  i = ((blockIdx.x) * blockDim.x + tx);
  j = ((blockIdx.y) * blockDim.y + ty);
  nblocks = (gridDim.x * gridDim.y);
  if ((i <= m && j <= (2 * n))) {
    if ((j > n)) {
      val1 = z2[_idx_z2((j - n))];
      val2 = v[_idx_v(i, (j - n))];

    } else {
      val1 = z1[_idx_z1(j)];
      val2 = w[_idx_w(i, j)];
    }
    xr = make_double(val1);
    xi = dimag(val1);
    rv2 = make_double(val2);
    iv2 = dimag(val2);
    rv1 = (-rv2 * xr + iv2 * xi);
    iv1 = (-rv2 * xi - iv2 * xr);
    istat = atomicAdd(y + _idx_y((2 * i - 1))*8, rv1);
    istat = atomicAdd(y + _idx_y((2 * i))*8, iv1);
  }
  __threadfence();
  nfinished = 0;
  __syncthreads();
  if ((tx + ty) == 2) {
    nfinished = atomicInc(&finished, (nblocks - 1));
  }
  __syncthreads();
  if ((nfinished < (nblocks - 1))) { 
      return; 
  } // ! Begin finish_W_col work with last block
  tid = threadIdx.x + (threadIdx.y) * blockDim.x + 1;
  laneid = tid & 31;
  if (tid == 1) {
    alphar = 0.0 /*_8*/;
    alphai = 0.0 /*_8*/;
  }
  __syncthreads();
  rsum = 0.0 /*_8*/;
  isum = 0.0 /*_8*/;
  mytau = tau;
  nb = ceil((float(m) / (blockDim.x * blockDim.y)));
  // ! number of blocks down column
  i = tid;
  
  for (int j = 1; j <= nb; j += 1) {
    // ! All threads perform their product, zero if out of bounds
    if ((i <= m)) {
      tmp0 = mytau * y2[_idx_y2(i)];
      val1 = conj(tmp0) * x[_idx_x(i)];

    } else {
      val1 = make_hipDoubleComplex(0, 0);
    }
    rv1 = make_double(val1);
  iv1 = dimag(val1);
  rsum = (rsum + rv1);
  isum = (isum + iv1);
  i = (i + blockDim.x* blockDim.y);

  }// ! Partial sum within warps using shuffle
  rv1 = rsum;
  rv2 = __shfl_down(rv1, 1);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 2);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 4);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 8);
  rv1 = (rv1 + rv2);
  rv2 = __shfl_down(rv1, 16);
  rv1 = (rv1 + rv2);
  iv1 = isum;
  iv2 = __shfl_down(iv1, 1);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 2);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 4);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 8);
  iv1 = (iv1 + iv2);
  iv2 = __shfl_down(iv1, 16);
  iv1 = (iv1 + iv2);
  if (laneid == 1) {
    istat = atomicAdd(&alphar, rv1);
    istat = atomicAdd(&alphai, iv1);
  }
  __syncthreads();
  alpha = (-make_hipDoubleComplex(0.5, 0.0) * mytau * make_hipDoubleComplex(alphar, alphai));
  for (int i = tid; i <= m; i += (blockDim.x * blockDim.y)) {
    y2[_idx_y2(i)] = (mytau * y2[_idx_y2(i)] + alpha * x[_idx_x(i)]);
    // !zaxpy
  }
}

extern "C" void launch_stacked_zgemv_n_finish_w(dim3 *grid,
                                                dim3 *block,
                                                const int sharedMem,
                                                hipStream_t stream,
                                                int m,
                                                int n,
                                                int ldv,
                                                int ldw,
                                                hipDoubleComplex *v,
                                                const int v_n1,
                                                const int v_lb1,
                                                const int v_lb2,
                                                hipDoubleComplex *w,
                                                const int w_n1,
                                                const int w_lb1,
                                                const int w_lb2,
                                                hipDoubleComplex *z1,
                                                const int z1_n1,
                                                const int z1_lb1,
                                                hipDoubleComplex *z2,
                                                const int z2_n1,
                                                const int z2_lb1,
                                                double *y,
                                                const int y_n1,
                                                const int y_lb1,
                                                hipDoubleComplex tau,
                                                hipDoubleComplex *x,
                                                const int x_n1,
                                                const int x_lb1,
                                                hipDoubleComplex *y2,
                                                const int y2_n1,
                                                const int y2_lb1,
                                                unsigned int finished) {
  hipLaunchKernelGGL((stacked_zgemv_n_finish_w),
                     *grid,
                     *block,
                     sharedMem,
                     stream,
                     m,
                     n,
                     ldv,
                     ldw,
                     v,
                     v_n1,
                     v_lb1,
                     v_lb2,
                     w,
                     w_n1,
                     w_lb1,
                     w_lb2,
                     z1,
                     z1_n1,
                     z1_lb1,
                     z2,
                     z2_n1,
                     z2_lb1,
                     y,
                     y_n1,
                     y_lb1,
                     tau,
                     x,
                     x_n1,
                     x_lb1,
                     y2,
                     y2_n1,
                     y2_lb1,
                     finished);
}
// END stacked_zgemv_n_finish_w
