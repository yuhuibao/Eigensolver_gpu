// This file was generated by gpufort

#include "hip/hip_complex.h"
#include "hip/hip_runtime.h"
#include "hip/math_functions.h"
#include <cstdio>

namespace {
// make float
float make_float(short int a) { return static_cast<float>(a); }
float make_float(unsigned short int a) { return static_cast<float>(a); }
float make_float(unsigned int a) { return static_cast<float>(a); }
float make_float(int a) { return static_cast<float>(a); }
float make_float(long int a) { return static_cast<float>(a); }
float make_float(unsigned long int a) { return static_cast<float>(a); }
float make_float(long long int a) { return static_cast<float>(a); }
float make_float(unsigned long long int a) { return static_cast<float>(a); }
float make_float(signed char a) { return static_cast<float>(a); }
float make_float(unsigned char a) { return static_cast<float>(a); }
float make_float(float a) { return static_cast<float>(a); }
float make_float(double a) { return static_cast<float>(a); }
float make_float(long double a) { return static_cast<float>(a); }
float make_float(hipFloatComplex &a) { return static_cast<float>(a.x); }
float make_float(hipDoubleComplex &a) { return static_cast<float>(a.x); }
// make double
double make_double(short int a) { return static_cast<double>(a); }
double make_double(unsigned short int a) { return static_cast<double>(a); }
double make_double(unsigned int a) { return static_cast<double>(a); }
double make_double(int a) { return static_cast<double>(a); }
double make_double(long int a) { return static_cast<double>(a); }
double make_double(unsigned long int a) { return static_cast<double>(a); }
double make_double(long long int a) { return static_cast<double>(a); }
double make_double(unsigned long long int a) { return static_cast<double>(a); }
double make_double(signed char a) { return static_cast<double>(a); }
double make_double(unsigned char a) { return static_cast<double>(a); }
double make_double(float a) { return static_cast<double>(a); }
double make_double(double a) { return static_cast<double>(a); }
double make_double(long double a) { return static_cast<double>(a); }
double make_double(hipFloatComplex &a) { return static_cast<double>(a.x); }
double make_double(hipDoubleComplex &a) { return static_cast<double>(a.x); }
// conjugate complex type
hipFloatComplex conj(hipFloatComplex &c) { return hipConjf(c); }
hipDoubleComplex conj(hipDoubleComplex &z) { return hipConj(z); }

// TODO Add the following functions:
// - sign(x,y) = sign(y) * |x| - sign transfer function
// ...
} // namespace
#define divideAndRoundUp(x, y) ((x) / (y) + ((x) % (y) != 0))

// BEGIN krnl_2b8e8f_0
/* Fortran original:
      ! kernel do(1) <<<*,*>>>
      do j = 33, N
         !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
         d(j) = A(j, j)
      end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): [-1, -1, -1]. ('-1' means not specified)
// - Threads per block (CUDA): [-1, -1, -1]. ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_2b8e8f_0(TODO declaration not found a, TODO declaration not found d, int n) {

  unsigned int j = 1 + threadIdx.x + blockIdx.x * blockDim.x;
  if ((j <= n)) {
    // !A(j-1, j) = e(j-1) ! JR Not strictly needed so skipping this copy
    d[_idx(j)] = a[_idx_a(j, j)];
  }
}

extern "C" void launch_krnl_2b8e8f_0(dim3 *grid,
                                     dim3 *block,
                                     const int sharedMem,
                                     hipStream_t stream,
                                     TODO declaration not found a,
                                     TODO declaration not found d,
                                     int n) {
  hipLaunchKernelGGL((krnl_2b8e8f_0), *grid, *block, sharedMem, stream, a, d, n);
}
extern "C" void
launch_krnl_2b8e8f_0_auto(const int sharedMem, hipStream_t stream, TODO declaration not found a, TODO declaration not found d, int n) {
  const unsigned int krnl_2b8e8f_0_NX = n;

  const unsigned int krnl_2b8e8f_0_blockX = 256;

  const unsigned int krnl_2b8e8f_0_gridX = divideAndRoundUp(krnl_2b8e8f_0_NX, krnl_2b8e8f_0_blockX);

  dim3 grid(krnl_2b8e8f_0_gridX);
  dim3 block(krnl_2b8e8f_0_blockX);
  hipLaunchKernelGGL((krnl_2b8e8f_0), grid, block, sharedMem, stream, a, d, n);
}
// END krnl_2b8e8f_0

// BEGIN krnl_9c27cb_1
/* Fortran original:
        ! kernel do(1) <<<*,*>>>
        do k = 1, N - 1
           W(k, iw) = dcmplx(0, 0)
        end do

*/
// NOTE: The following information was given in the orignal Cuf kernel pragma:
// - Nested outer-most do-loops that are directly mapped to threads: 1
// - Number of blocks (CUDA): [-1, -1, -1]. ('-1' means not specified)
// - Threads per block (CUDA): [-1, -1, -1]. ('-1' means not specified)
// - Shared Memory: 0
// - Stream: 0

__global__ void krnl_9c27cb_1(int iw, int n, TODO declaration not found w) {

  unsigned int k = 1 + threadIdx.x + blockIdx.x * blockDim.x;
  if ((k <= (n - 1))) {
    w[_idx_w(k, iw)] = make_doubleComplex(0, 0);
  }
}

extern "C" void
launch_krnl_9c27cb_1(dim3 *grid, dim3 *block, const int sharedMem, hipStream_t stream, int iw, int n, TODO declaration not found w) {
  hipLaunchKernelGGL((krnl_9c27cb_1), *grid, *block, sharedMem, stream, iw, n, w);
}
extern "C" void launch_krnl_9c27cb_1_auto(const int sharedMem, hipStream_t stream, int iw, int n, TODO declaration not found w) {
  const unsigned int krnl_9c27cb_1_NX = (n - 1);

  const unsigned int krnl_9c27cb_1_blockX = 256;

  const unsigned int krnl_9c27cb_1_gridX = divideAndRoundUp(krnl_9c27cb_1_NX, krnl_9c27cb_1_blockX);

  dim3 grid(krnl_9c27cb_1_gridX);
  dim3 block(krnl_9c27cb_1_blockX);
  hipLaunchKernelGGL((krnl_9c27cb_1), grid, block, sharedMem, stream, iw, n, w);
}
// END krnl_9c27cb_1

// BEGIN zher2_mv_kernel
/* Fortran original:
      implicit none

      integer, value                                        :: N, M, ldv, ldw, ldw2

      complex(8), dimension(1:ldv, 1:M), device, intent(in) :: V

      complex(8), dimension(1:ldw, 1:M), device, intent(in) :: W

      complex(8), dimension(1:ldw2, 2), device               :: W2

      !DIR$ IGNORE_TKR x

      real(8), dimension(1:2*N), device                     :: x

      integer                                               :: i, j, istat

      complex(8)                                            :: val

      real(8)                                               :: rv, iv

      i = (blockIdx%x - 1)*blockDim%x + threadIdx%x

      j = (blockIdx%y - 1)*blockDim%y + threadIdx%y

      if (i <= N .and. j <= M) then

         val = -conjg(W(N, j))*V(i, j) - conjg(V(N, j))*W(i, j)

         rv = dble(val)

         iv = dimag(val)

         ! Zero out imaginary part on diagonal

         if (i == N) then

            iv = 0.d0

         endif

         ! Update x

         istat = atomicadd(x(2*i - 1), rv)

         istat = atomicadd(x(2*i), iv)

      endif

      if (threadIdx%y == 1) then

         ! Zero out column for zhemv call

         if (i <= N) W2(i, 1) = 0

         ! Zero out workspace for intermediate zgemv results

         if (i <= M) then

            W2(N + i, 1) = 0

            W2(N + i, 2) = 0

         endif

      endif


*/

__global__ void zher2_mv_kernel(int n, int m, int ldv, int ldw, int ldw2) {
  int i;
  int j;
  int istat;
  hipFloatComplex val;
  float rv;
  float iv;

  // ! TODO could not parse:        complex(8), dimension(1:ldv, 1:m), device, intent(in) :: v
  // ! TODO could not parse:        complex(8), dimension(1:ldw, 1:m), device, intent(in) :: w
  // ! TODO could not parse:        complex(8), dimension(1:ldw2, 2), device               :: w2
  // !DIR$ IGNORE_TKR x
  // ! TODO could not parse:        real(8), dimension(1:2*n), device                     :: x
  i = ((blockIdx.x - 1) * blockDim.x + threadIdx.x);
  j = ((blockIdx.y - 1) * blockDim.y + threadIdx.y);
  if ((i <= n & j <= m)) {
    val=(-ASTConjugate:{'match': '    attributes(global) subroutine zher2_mv_kernel(n, m, v, ldv, w, ldw, x, w2, ldw2)
         implicit none
         integer, value                                        :: n, m, ldv, ldw, ldw2
  ! TODO could not parse:        complex(8), dimension(1:ldv, 1:m), device, intent(in) :: v
  ! TODO could not parse:        complex(8), dimension(1:ldw, 1:m), device, intent(in) :: w
  ! TODO could not parse:        complex(8), dimension(1:ldw2, 2), device               :: w2
         !DIR$ IGNORE_TKR x
  ! TODO could not parse:        real(8), dimension(1:2*n), device                     :: x

         integer                                               :: i, j, istat
         complex(8)                                            :: val
         real(8)                                               :: rv, iv

         i = (blockIdx%x - 1)*blockDim%x + threadIdx%x
         j = (blockIdx%y - 1)*blockDim%y + threadIdx%y

         if (i <= n .and. j <= m) then

            val = -conjg(w(n, j))*v(i, j) - conjg(v(n, j))*w(i, j)
            rv = dble(val)
            iv = dimag(val)

            ! Zero out imaginary part on diagonal
            if (i == n) then
               iv = 0.d0
            endif

            ! Update x
            istat = atomicAdd(x(2*i - 1), rv)
            istat = atomicAdd(x(2*i), iv)
         endif

         if (threadIdx%y == 1) then
            ! Zero out column for zhemv call
            if (i <= n) w2(i, 1) = 0
            ! Zero out workspace for intermediate zgemv results
            if (i <= m) then
               w2(n + i, 1) = 0
               w2(n + i, 2) = 0
            endif
         endif

      end subroutine zher2_mv_kernel

  ', 'location': 958, 'ref': <fort2hip.translator.ASTArithmeticExpression object at 0x7f5901f519e8>, 'kind': None}*v[_idx_v(i,j)]-ASTConjugate:{'match': '    attributes(global) subroutine zher2_mv_kernel(n, m, v, ldv, w, ldw, x, w2, ldw2)
         implicit none
         integer, value                                        :: n, m, ldv, ldw, ldw2
  ! TODO could not parse:        complex(8), dimension(1:ldv, 1:m), device, intent(in) :: v
  ! TODO could not parse:        complex(8), dimension(1:ldw, 1:m), device, intent(in) :: w
  ! TODO could not parse:        complex(8), dimension(1:ldw2, 2), device               :: w2
         !DIR$ IGNORE_TKR x
  ! TODO could not parse:        real(8), dimension(1:2*n), device                     :: x

         integer                                               :: i, j, istat
         complex(8)                                            :: val
         real(8)                                               :: rv, iv

         i = (blockIdx%x - 1)*blockDim%x + threadIdx%x
         j = (blockIdx%y - 1)*blockDim%y + threadIdx%y

         if (i <= n .and. j <= m) then

            val = -conjg(w(n, j))*v(i, j) - conjg(v(n, j))*w(i, j)
            rv = dble(val)
            iv = dimag(val)

            ! Zero out imaginary part on diagonal
            if (i == n) then
               iv = 0.d0
            endif

            ! Update x
            istat = atomicAdd(x(2*i - 1), rv)
            istat = atomicAdd(x(2*i), iv)
         endif

         if (threadIdx%y == 1) then
            ! Zero out column for zhemv call
            if (i <= n) w2(i, 1) = 0
            ! Zero out workspace for intermediate zgemv results
            if (i <= m) then
               w2(n + i, 1) = 0
               w2(n + i, 2) = 0
            endif
         endif

      end subroutine zher2_mv_kernel

  ', 'location': 983, 'ref': <fort2hip.translator.ASTArithmeticExpression object at 0x7f5901e4dda0>, 'kind': None}*w[_idx_w(i,j)]);
  rv=make_double(val);
      iv = dimag[_idx(val)];
      // ! Zero out imaginary part on diagonal
      if ((i == n)) {
        iv = 0.e0;
      }
      // ! Update x
      istat = atomicAdd(x[_idx((2 * i - 1))], rv);
      istat = atomicAdd(x[_idx((2 * i))], iv);

  }
  if ((threadIdx.y==1)) {
      // ! Zero out column for zhemv call
      if ((i <= n)) {
        w2[_idx_w2(i, 1)] = 0;
        // ! Zero out workspace for intermediate zgemv results
        if ((i <= m)) {
          w2[_idx_w2((n + i), 1)] = 0;
          w2[_idx_w2((n + i), 2)] = 0;
        }
      }
  }
  }

  extern "C" void
  launch_zher2_mv_kernel(dim3 * grid, dim3 * block, const int sharedMem, hipStream_t stream, int n, int m, int ldv, int ldw, int ldw2) {
    hipLaunchKernelGGL((zher2_mv_kernel), *grid, *block, sharedMem, stream, n, m, ldv, ldw, ldw2);
  }
  // END zher2_mv_kernel

  // BEGIN zlarfg_kernel
  /* Fortran original:
        implicit none

        integer, value                   :: N

        complex(8), device               :: tau

        real(8), device                  :: e

        complex(8), dimension(N), device :: x

        integer                          :: tid, i, j, nb, istat, laneID

        real(8)                          :: rv1, rv2, rv3, scal, invscal, alphar, alphai, beta, rsum, isum

        complex(8)                       :: cv1

        real(8), shared                  :: xnorm

        complex(8), shared               :: alpha_s

        tid = threadIdx%x

        laneID = iand(tid, 31)

        if (tid == 1) then

           alpha_s = x(N)

           xnorm = 0.0_8

        endif

        call syncthreads()

        alphar = dble(alpha_s)

        alphai = dimag(alpha_s)

        rsum = 0.0_8

        nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

        i = tid

        do j = 1, nb

           ! All threads perform their product, zero if out of bounds

           if (i <= N - 1) then

              cv1 = x(i)

              rv2 = dble(cv1); rv3 = dimag(cv1)

              rv1 = rv2*rv2 + rv3*rv3

           else

              rv1 = 0.0_8

           endif

           rsum = rsum + rv1

           i = i + blockDim%x

        end do

        ! Partial sum within warps using shuffle

        rv1 = rsum

        rv2 = __shfl_down(rv1, 1)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 2)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 4)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 8)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 16)

        rv1 = rv1 + rv2

        if (laneID == 1) then

           istat = atomicadd(xnorm, rv1)

        endif

        call syncthreads()

        if (xnorm == 0.0_8 .and. alphai == 0.0_8) then

           if (tid == 1) then

              tau = 0.0_8

           endif

        else

           if (tid == 1) then

              xnorm = sqrt(xnorm)

              rv1 = abs(alphar)

              rv2 = abs(alphai)

              ! not taking abs of xnorm

              scal = max(rv1, rv2, xnorm)

              invscal = 1.d0/scal

              rv1 = rv1*invscal

              rv2 = rv2*invscal

              xnorm = xnorm*invscal

              beta = -sign(scal*sqrt(rv1*rv1 + rv2*rv2 + xnorm*xnorm), alphar)

              tau = dcmplx((beta - alphar)/beta, -alphai/beta)

              !zladiv

              rv1 = dble(alpha_s - beta)

              rv2 = dimag(alpha_s - beta)

              if (abs(rv2) .lt. abs(rv1)) then

                 xnorm = rv2/rv1

                 invscal = 1.d0/(rv1 + rv2*xnorm)

                 alpha_s = dcmplx(invscal, -xnorm*invscal)

              else

                 xnorm = rv1/rv2

                 invscal = 1.d0/(rv2 + rv1*xnorm)

                 alpha_s = dcmplx(xnorm*invscal, -invscal)

              endif

              e = beta ! store beta in e vector

           endif

           call syncthreads()

           do i = tid, N, blockDim%x

              cv1 = x(i)

              if (i <= N - 1) then

                 cv1 = alpha_s*cv1

              elseif (i == N) then

                 !x(i) = 1.0_8

                 cv1 = dcmplx(1.0_8, 0.0_8)

              endif

              x(i) = cv1

           end do

        endif


  */

  __global__ void zlarfg_kernel(int n, hipFloatComplex tau, float e, hipFloatComplex *x) {
    int tid;
    int i;
    int j;
    int nb;
    int istat;
    int laneid;
    float rv1;
    float rv2;
    float rv3;
    float scal;
    float invscal;
    float alphar;
    float alphai;
    float beta;
    float rsum;
    float isum;
    hipFloatComplex cv1;
    float xnorm;
    hipFloatComplex alpha_s;

    tid = threadIdx.x;
    laneid = iand(tid, 31);
    if ((tid == 1)) {
      alpha_s = x[_idx(n)];
      xnorm = 0.0 /*_8*/;
    }
    __syncthreads() alphar = make_double(alpha_s);
    alphai = dimag[_idx(alpha_s)];
    rsum = 0.0 /*_8*/;
    nb = ceiling((make_float(n) / blockDim.x));
    // ! number of blocks down column
    i = tid;
    for (int j = 1; j <= nb; j += 1) {
      // ! All threads perform their product, zero if out of bounds
      if ((i <= (n - 1))) {
        cv1 = x[_idx(i)];
        rv2 = make_double(cv1);
        rv3 = dimag[_idx(cv1)];
        rv1 = (rv2 * rv2 + rv3 * rv3);

      } else {
        rv1 = 0.0 /*_8*/;
      }
      rsum = (rsum + rv1);
      i = (i + blockDim.x);

    } // ! Partial sum within warps using shuffle
    rv1 = rsum;
    rv2 = __shfl_down(rv1, 1);
    rv1 = (rv1 + rv2);
    rv2 = __shfl_down(rv1, 2);
    rv1 = (rv1 + rv2);
    rv2 = __shfl_down(rv1, 4);
    rv1 = (rv1 + rv2);
    rv2 = __shfl_down(rv1, 8);
    rv1 = (rv1 + rv2);
    rv2 = __shfl_down(rv1, 16);
    rv1 = (rv1 + rv2);
    if ((laneid == 1)) {
      istat = atomicAdd(xnorm, rv1);
    }
    __syncthreads() if ((xnorm == 0.0 /*_8*/ & alphai == 0.0 /*_8*/)) {
      if ((tid == 1)) {
        tau = 0.0 /*_8*/;
      }
    }
    else if ((tid == 1)) {
      xnorm = sqrt(xnorm);
      rv1 = abs(alphar);
      rv2 = abs(alphai);
      // ! not taking abs of xnorm
      scal = max(rv1, rv2, xnorm);
      invscal = (1.e0 / scal);
      rv1 = (rv1 * invscal);
      rv2 = (rv2 * invscal);
      xnorm = (xnorm * invscal);
      beta = -sign((scal * sqrt((rv1 * rv1 + rv2 * rv2 + xnorm * xnorm))), alphar);
      tau = make_doubleComplex(((beta - alphar) / beta), (-alphai / beta));
      // !zladiv
      rv1 = make_double((alpha_s - beta));
      rv2 = dimag[_idx((alpha_s - beta))];
      if ((abs(rv2) < abs(rv1))) {
        xnorm = (rv2 / rv1);
        invscal = (1.e0 / (rv1 + rv2 * xnorm));
        alpha_s = make_doubleComplex(invscal, (-xnorm * invscal));

      } else {
        xnorm = (rv1 / rv2);
        invscal = (1.e0 / (rv2 + rv1 * xnorm));
        alpha_s = make_doubleComplex((xnorm * invscal), -invscal);
      }
      e = beta;
      // ! store beta in e vector
    }
    __syncthreads() for (int i = tid; i <= n; i += blockDim.x) {
      cv1 = x[_idx(i)];
      if ((i <= (n - 1))) {
        cv1 = (alpha_s * cv1);

      } else if ((i == n)) {
        // !x(i) = 1.0_8
        cv1 = make_doubleComplex(1.0 /*_8*/, 0.0 /*_8*/);
      }
      x[_idx(i)] = cv1;

    } // ! TODO could not parse:        endif
  }

  extern "C" void launch_zlarfg_kernel(dim3 * grid,
                                       dim3 * block,
                                       const int sharedMem,
                                       hipStream_t stream,
                                       int n,
                                       hipFloatComplex tau,
                                       float e,
                                       hipFloatComplex *x) {
    hipLaunchKernelGGL((zlarfg_kernel), *grid, *block, sharedMem, stream, n, tau, e, x);
  }
  // END zlarfg_kernel

  // BEGIN zher2_mv_zlarfg_kernel
  /* Fortran original:
        implicit none

        integer, value                                        :: N, M, ldv, ldw, ldw2

        complex(8), dimension(1:ldv, 1:M), device, intent(in) :: V

        complex(8), dimension(1:ldw, 1:M), device, intent(in) :: W

        complex(8), dimension(1:ldw2, 2), device              :: W2

        !DIR$ IGNORE_TKR x

        real(8), dimension(1:2*N), device                     :: x

        complex(8), dimension(1:N), device                    :: x2

        complex(8), device                                    :: tau

        real(8), device                                       :: e

        integer                                               :: i, j, tx, ty, tid, nb, laneid, istat, nBlocks

        integer, device                                       :: finished

        integer, shared                                       :: nFinished

        complex(8)                                            :: val

        real(8)                                               :: rv, iv

        real(8)                                               :: rv1, rv2, rv3, scal, invscal, alphar, alphai, beta, rsum, isum

        complex(8)                                            :: cv1

        real(8), shared                                       :: xnorm

        complex(8), shared                                    :: alpha_s

        tx = threadIdx%x

        ty = threadIdx%y

        i = (blockIdx%x - 1)*blockDim%x + tx

        j = (blockIdx%y - 1)*blockDim%y + ty

        nBlocks = gridDim%x*gridDim%y

        !if (i > N .or. j > M) return

        if (i <= N .and. j <= M) then

           val = -conjg(W(N, j))*V(i, j) - conjg(V(N, j))*W(i, j)

           rv = dble(val)

           iv = dimag(val)

           ! Zero out imaginary part on diagonal

           if (i == N) then

              iv = 0.d0

           endif

           ! Update x

           istat = atomicadd(x(2*i - 1), rv)

           istat = atomicadd(x(2*i), iv)

        endif

        if (ty == 1) then

           ! Zero out column for zhemv call

           if (i <= N) W2(i, 1) = 0

           ! Zero out workspace for intermediate zgemv results

           if (i <= M) then

              W2(N + i, 1) = 0

              W2(N + i, 2) = 0

           endif

        endif

        call threadfence()

        nFinished = 0

        call syncthreads()

        if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)

        call syncthreads()

        if (nFinished < nBlocks - 1) return

        ! Begin zlarfg work with last block

        if (N == 1) return

        tid = tx + (ty - 1)*blockDim%x

        laneID = iand(tid, 31)

        if (tid == 1) then

           alpha_s = x2(N - 1)

           xnorm = 0.0_8

        endif

        call syncthreads()

        alphar = dble(alpha_s)

        alphai = dimag(alpha_s)

        rsum = 0.0_8

        nb = ceiling(real(N - 1)/(blockDim%x*blockDim%y)) ! number of blocks down column

        i = tid

        do j = 1, nb

           ! All threads perform their product, zero if out of bounds

           if (i <= N - 2) then

              cv1 = x2(i)

              rv2 = dble(cv1); rv3 = dimag(cv1)

              rv1 = rv2*rv2 + rv3*rv3

           else

              rv1 = 0.0_8

           endif

           rsum = rsum + rv1

           i = i + blockDim%x*blockDim%y

        end do

        ! Partial sum within warps using shuffle

        rv1 = rsum

        rv2 = __shfl_down(rv1, 1)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 2)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 4)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 8)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 16)

        rv1 = rv1 + rv2

        if (laneID == 1) then

           istat = atomicadd(xnorm, rv1)

        endif

        call syncthreads()

        if (xnorm == 0.0_8 .and. alphai == 0.0_8) then

           if (tid == 1) then

              tau = 0.0_8

           endif

        else

           if (tid == 1) then

              xnorm = sqrt(xnorm)

              rv1 = abs(alphar)

              rv2 = abs(alphai)

              ! not taking abs of xnorm

              scal = max(rv1, rv2, xnorm)

              invscal = 1.d0/scal

              rv1 = rv1*invscal

              rv2 = rv2*invscal

              xnorm = xnorm*invscal

              beta = -sign(scal*sqrt(rv1*rv1 + rv2*rv2 + xnorm*xnorm), alphar)

              tau = dcmplx((beta - alphar)/beta, -alphai/beta)

              !zladiv

              rv1 = dble(alpha_s - beta)

              rv2 = dimag(alpha_s - beta)

              if (abs(rv2) .lt. abs(rv1)) then

                 xnorm = rv2/rv1

                 invscal = 1.d0/(rv1 + rv2*xnorm)

                 alpha_s = dcmplx(invscal, -xnorm*invscal)

              else

                 xnorm = rv1/rv2

                 invscal = 1.d0/(rv2 + rv1*xnorm)

                 alpha_s = dcmplx(xnorm*invscal, -invscal)

              endif

              e = beta ! store beta in e vector

           endif

           call syncthreads()

           do i = tid, N - 1, blockDim%x*blockDim%y

              cv1 = x2(i)

              if (i <= N - 2) then

                 cv1 = alpha_s*cv1

              elseif (i == N - 1) then

                 !x(i) = 1.0_8

                 cv1 = dcmplx(1.0_8, 0.0_8)

              endif

              x2(i) = cv1

           end do

        endif


  */

  __global__ void zher2_mv_zlarfg_kernel(int n, int m, int ldv, int ldw, int ldw2, hipFloatComplex tau, float e, int finished) {
    int i;
    int j;
    int tx;
    int ty;
    int tid;
    int nb;
    int laneid;
    int istat;
    int nblocks;
    int nfinished;
    hipFloatComplex val;
    float rv;
    float iv;
    float rv1;
    float rv2;
    float rv3;
    float scal;
    float invscal;
    float alphar;
    float alphai;
    float beta;
    float rsum;
    float isum;
    hipFloatComplex cv1;
    float xnorm;
    hipFloatComplex alpha_s;

    // ! TODO could not parse:        complex(8), dimension(1:ldv, 1:m), device, intent(in) :: v
    // ! TODO could not parse:        complex(8), dimension(1:ldw, 1:m), device, intent(in) :: w
    // ! TODO could not parse:        complex(8), dimension(1:ldw2, 2), device              :: w2
    // !DIR$ IGNORE_TKR x
    // ! TODO could not parse:        real(8), dimension(1:2*n), device                     :: x
    // ! TODO could not parse:        complex(8), dimension(1:n), device                    :: x2
    tx = threadIdx.x;
    ty = threadIdx.y;
    i = ((blockIdx.x - 1) * blockDim.x + tx);
    j = ((blockIdx.y - 1) * blockDim.y + ty);
    nblocks = (gridDim.x * gridDim.y);
    // !if (i > N .or. j > M) return
    if ((i <= n & j <= m)) {
    val=(-ASTConjugate:{'match': '    attributes(global) subroutine zher2_mv_zlarfg_kernel(n, m, v, ldv, w, ldw, x, w2, ldw2, e, tau, x2, finished)
         implicit none
         integer, value                                        :: n, m, ldv, ldw, ldw2
  ! TODO could not parse:        complex(8), dimension(1:ldv, 1:m), device, intent(in) :: v
  ! TODO could not parse:        complex(8), dimension(1:ldw, 1:m), device, intent(in) :: w
  ! TODO could not parse:        complex(8), dimension(1:ldw2, 2), device              :: w2
         !DIR$ IGNORE_TKR x
  ! TODO could not parse:        real(8), dimension(1:2*n), device                     :: x
  ! TODO could not parse:        complex(8), dimension(1:n), device                    :: x2
         complex(8), device                                    :: tau
         real(8), device                                       :: e

         integer                                               :: i, j, tx, ty, tid, nb, laneid, istat, nblocks
         integer, device                                       :: finished
         integer, shared                                       :: nfinished
         complex(8)                                            :: val
         real(8)                                               :: rv, iv
         real(8)                                               :: rv1, rv2, rv3, scal, invscal, alphar, alphai, beta, rsum, isum
         complex(8)                                            :: cv1
         real(8), shared                                       :: xnorm
         complex(8), shared                                    :: alpha_s

         tx = threadIdx%x
         ty = threadIdx%y
         i = (blockIdx%x - 1)*blockDim%x + tx
         j = (blockIdx%y - 1)*blockDim%y + ty

         nblocks = gridDim%x*gridDim%y

         !if (i > N .or. j > M) return
         if (i <= n .and. j <= m) then

            val = -conjg(w(n, j))*v(i, j) - conjg(v(n, j))*w(i, j)
            rv = dble(val)
            iv = dimag(val)

            ! Zero out imaginary part on diagonal
            if (i == n) then
               iv = 0.d0
            endif

            ! Update x
            istat = atomicAdd(x(2*i - 1), rv)
            istat = atomicAdd(x(2*i), iv)
         endif

         if (ty == 1) then
            ! Zero out column for zhemv call
            if (i <= n) w2(i, 1) = 0
            ! Zero out workspace for intermediate zgemv results
            if (i <= m) then
               w2(n + i, 1) = 0
               w2(n + i, 2) = 0
            endif
         endif

         call threadfence()

         nfinished = 0
         call __syncthreads()
         if (tx + ty == 2) nfinished = atomicInc(finished, nblocks - 1)
         call __syncthreads()

         if (nfinished < nblocks - 1) return

         ! Begin zlarfg work with last block
         if (n == 1) return

         tid = tx + (ty - 1)*blockDim%x
         laneid = iand(tid, 31)

         if (tid == 1) then
            alpha_s = x2(n - 1)
            xnorm = 0.0_8
         endif

         call __syncthreads()

         alphar = dble(alpha_s)
         alphai = dimag(alpha_s)
         rsum = 0.0_8

         nb = ceiling(real(n - 1)/(blockDim%x*blockDim%y)) ! number of blocks down column

         i = tid
         do j = 1, nb

            ! All threads perform their product, zero if out of bounds
            if (i <= n - 2) then
               cv1 = x2(i)
               rv2 = dble(cv1); rv3 = dimag(cv1)
               rv1 = rv2*rv2 + rv3*rv3
            else
               rv1 = 0.0_8
            endif

            rsum = rsum + rv1

            i = i + blockDim%x*blockDim%y
         end do

         ! Partial sum within warps using shuffle
         rv1 = rsum
         rv2 = __shfl_down(rv1, 1)
         rv1 = rv1 + rv2
         rv2 = __shfl_down(rv1, 2)
         rv1 = rv1 + rv2
         rv2 = __shfl_down(rv1, 4)
         rv1 = rv1 + rv2
         rv2 = __shfl_down(rv1, 8)
         rv1 = rv1 + rv2
         rv2 = __shfl_down(rv1, 16)
         rv1 = rv1 + rv2

         if (laneid == 1) then
            istat = atomicAdd(xnorm, rv1)
         endif

         call __syncthreads()

         if (xnorm == 0.0_8 .and. alphai == 0.0_8) then
            if (tid == 1) then
               tau = 0.0_8
            endif
         else
            if (tid == 1) then
               xnorm = sqrt(xnorm)

               rv1 = abs(alphar)
               rv2 = abs(alphai)
               ! not taking abs of xnorm
               scal = max(rv1, rv2, xnorm)
               invscal = 1.d0/scal

               rv1 = rv1*invscal
               rv2 = rv2*invscal
               xnorm = xnorm*invscal

               beta = -sign(scal*sqrt(rv1*rv1 + rv2*rv2 + xnorm*xnorm), alphar)

               tau = dcmplx((beta - alphar)/beta, -alphai/beta)

               !zladiv
               rv1 = dble(alpha_s - beta)
               rv2 = dimag(alpha_s - beta)

               if (abs(rv2) .lt. abs(rv1)) then
                  xnorm = rv2/rv1
                  invscal = 1.d0/(rv1 + rv2*xnorm)
                  alpha_s = dcmplx(invscal, -xnorm*invscal)
               else
                  xnorm = rv1/rv2
                  invscal = 1.d0/(rv2 + rv1*xnorm)
                  alpha_s = dcmplx(xnorm*invscal, -invscal)
               endif

               e = beta ! store beta in e vector
            endif

            call __syncthreads()

            do i = tid, n - 1, blockDim%x*blockDim%y
               cv1 = x2(i)

               if (i <= n - 2) then
                  cv1 = alpha_s*cv1
               else if (i == n - 1) then
                  !x(i) = 1.0_8
                  cv1 = dcmplx(1.0_8, 0.0_8)
               endif

               x2(i) = cv1
            end do

  ! TODO could not parse:        endif

      end subroutine zher2_mv_zlarfg_kernel

  ', 'location': 1834, 'ref': <fort2hip.translator.ASTArithmeticExpression object at 0x7f5901e5beb8>, 'kind': None}*v[_idx_v(i,j)]-ASTConjugate:{'match': '    attributes(global) subroutine zher2_mv_zlarfg_kernel(n, m, v, ldv, w, ldw, x, w2, ldw2, e, tau, x2, finished)
         implicit none
         integer, value                                        :: n, m, ldv, ldw, ldw2
  ! TODO could not parse:        complex(8), dimension(1:ldv, 1:m), device, intent(in) :: v
  ! TODO could not parse:        complex(8), dimension(1:ldw, 1:m), device, intent(in) :: w
  ! TODO could not parse:        complex(8), dimension(1:ldw2, 2), device              :: w2
         !DIR$ IGNORE_TKR x
  ! TODO could not parse:        real(8), dimension(1:2*n), device                     :: x
  ! TODO could not parse:        complex(8), dimension(1:n), device                    :: x2
         complex(8), device                                    :: tau
         real(8), device                                       :: e

         integer                                               :: i, j, tx, ty, tid, nb, laneid, istat, nblocks
         integer, device                                       :: finished
         integer, shared                                       :: nfinished
         complex(8)                                            :: val
         real(8)                                               :: rv, iv
         real(8)                                               :: rv1, rv2, rv3, scal, invscal, alphar, alphai, beta, rsum, isum
         complex(8)                                            :: cv1
         real(8), shared                                       :: xnorm
         complex(8), shared                                    :: alpha_s

         tx = threadIdx%x
         ty = threadIdx%y
         i = (blockIdx%x - 1)*blockDim%x + tx
         j = (blockIdx%y - 1)*blockDim%y + ty

         nblocks = gridDim%x*gridDim%y

         !if (i > N .or. j > M) return
         if (i <= n .and. j <= m) then

            val = -conjg(w(n, j))*v(i, j) - conjg(v(n, j))*w(i, j)
            rv = dble(val)
            iv = dimag(val)

            ! Zero out imaginary part on diagonal
            if (i == n) then
               iv = 0.d0
            endif

            ! Update x
            istat = atomicAdd(x(2*i - 1), rv)
            istat = atomicAdd(x(2*i), iv)
         endif

         if (ty == 1) then
            ! Zero out column for zhemv call
            if (i <= n) w2(i, 1) = 0
            ! Zero out workspace for intermediate zgemv results
            if (i <= m) then
               w2(n + i, 1) = 0
               w2(n + i, 2) = 0
            endif
         endif

         call threadfence()

         nfinished = 0
         call __syncthreads()
         if (tx + ty == 2) nfinished = atomicInc(finished, nblocks - 1)
         call __syncthreads()

         if (nfinished < nblocks - 1) return

         ! Begin zlarfg work with last block
         if (n == 1) return

         tid = tx + (ty - 1)*blockDim%x
         laneid = iand(tid, 31)

         if (tid == 1) then
            alpha_s = x2(n - 1)
            xnorm = 0.0_8
         endif

         call __syncthreads()

         alphar = dble(alpha_s)
         alphai = dimag(alpha_s)
         rsum = 0.0_8

         nb = ceiling(real(n - 1)/(blockDim%x*blockDim%y)) ! number of blocks down column

         i = tid
         do j = 1, nb

            ! All threads perform their product, zero if out of bounds
            if (i <= n - 2) then
               cv1 = x2(i)
               rv2 = dble(cv1);
        rv3 = dimag(cv1) rv1 =
            rv2 *rv2 + rv3 *rv3 else rv1 = 0.0_8 endif

                rsum = rsum + rv1

                                  i =
                           i + blockDim % x *blockDim %
                                   y end do

                                   !Partial sum within warps using shuffle rv1 = rsum rv2 = __shfl_down(rv1, 1) rv1 =
                               rv1 + rv2 rv2 = __shfl_down(rv1, 2) rv1 =
                                   rv1 + rv2 rv2 = __shfl_down(rv1, 4) rv1 =
                                       rv1 + rv2 rv2 = __shfl_down(rv1, 8) rv1 =
                                           rv1 + rv2 rv2 = __shfl_down(rv1, 16) rv1 =
                                               rv1 + rv2

                                               if (laneid == 1) then istat = atomicAdd(xnorm, rv1) endif

                                                   call
                                                   __syncthreads()

                                                       if (xnorm == 0.0_8.and.alphai == 0.0_8) then if (tid == 1) then tau =
                                                           0.0_8 endif else if (tid == 1) then xnorm = sqrt(xnorm)

                                                               rv1 = abs(alphar) rv2 = abs(
                                                                   alphai) !not taking abs of xnorm scal = max(rv1, rv2, xnorm) invscal =
                                                                   1.d0 / scal

                                                                              rv1 = rv1 *invscal rv2 = rv2 *invscal xnorm = xnorm *invscal

                                                                       beta = -sign(scal * sqrt(rv1 * rv1 + rv2 * rv2 + xnorm * xnorm),
                                                                                    alphar)

                                                                                  tau = dcmplx((beta - alphar) / beta, -alphai / beta)

                                                                           !zladiv rv1 = dble(alpha_s - beta) rv2 = dimag(alpha_s - beta)

                                                                               if (abs(rv2).lt.abs(rv1)) then xnorm =
                                                                                   rv2 / rv1 invscal =
                                                                                       1.d0 / (rv1 + rv2 * xnorm) alpha_s =
                                                                                           dcmplx(invscal, -xnorm * invscal) else xnorm =
                                                                                               rv1 / rv2 invscal =
                                                                                                   1.d0 / (rv2 + rv1 * xnorm) alpha_s =
                                                                                                       dcmplx(xnorm * invscal, -invscal)
                                                                                                           endif

                                                                                                               e = beta !store beta in e
                                                                                                                   vector endif

                                                                                                                       call
                                                                                                                       __syncthreads()

                                                                                                                           do i = tid,
        n - 1,
        blockDim % x *blockDim % y cv1 = x2(i)

            if (i <= n - 2) then cv1 = alpha_s * cv1 else if (i == n - 1) then !x(i) = 1.0_8 cv1 = dcmplx(1.0_8, 0.0_8) endif

                                           x2(i) = cv1 end do

            !TODO could not parse
            : endif

                  end subroutine zher2_mv_zlarfg_kernel

              ', ' location ': 1859, ' ref ': <fort2hip.translator.ASTArithmeticExpression object at 0x7f5901e967b8>, ' kind': None}*w[_idx_w(i,j)]); rv = make_double(
                  val);
        iv = dimag[_idx(val)];
        // ! Zero out imaginary part on diagonal
        if ((i == n)) {
          iv = 0.e0;
        }
        // ! Update x
        istat = atomicAdd(x[_idx((2 * i - 1))], rv);
        istat = atomicAdd(x[_idx((2 * i))], iv);

  }
  if ((ty==1)) {
        // ! Zero out column for zhemv call
        if ((i <= n)) {
          w2[_idx_w2(i, 1)] = 0;
          // ! Zero out workspace for intermediate zgemv results
          if ((i <= m)) {
            w2[_idx_w2((n + i), 1)] = 0;
            w2[_idx_w2((n + i), 2)] = 0;
          }
        }
  }
  threadfence()nfinished=0;
  __syncthreads()if (((tx+ty)==2)) {
        nfinished = atomicInc(finished, (nblocks - 1));
        __syncthreads() if ((nfinished < (nblocks - 1))) {
          return; // ! Begin zlarfg work with last block
          if ((n == 1)) {
            return;
            tid = (tx + (ty - 1) * blockDim.x);
            laneid = iand(tid, 31);
            if ((tid == 1)) {
              alpha_s = x2[_idx((n - 1))];
              xnorm = 0.0 /*_8*/;
            }
            __syncthreads() alphar = make_double(alpha_s);
            alphai = dimag[_idx(alpha_s)];
            rsum = 0.0 /*_8*/;
            nb = ceiling((make_float((n - 1)) / (blockDim.x * blockDim.y)));
            // ! number of blocks down column
            i = tid;
            for (int j = 1; j <= nb; j += 1) {
              // ! All threads perform their product, zero if out of bounds
              if ((i <= (n - 2))) {
                cv1 = x2[_idx(i)];
                rv2 = make_double(cv1);
                rv3 = dimag[_idx(cv1)];
                rv1 = (rv2 * rv2 + rv3 * rv3);

              } else {
                rv1 = 0.0 /*_8*/;
              }
              rsum = (rsum + rv1);
              i = (i + blockDim.x * blockDim.y);

            } // ! Partial sum within warps using shuffle
            rv1 = rsum;
            rv2 = __shfl_down(rv1, 1);
            rv1 = (rv1 + rv2);
            rv2 = __shfl_down(rv1, 2);
            rv1 = (rv1 + rv2);
            rv2 = __shfl_down(rv1, 4);
            rv1 = (rv1 + rv2);
            rv2 = __shfl_down(rv1, 8);
            rv1 = (rv1 + rv2);
            rv2 = __shfl_down(rv1, 16);
            rv1 = (rv1 + rv2);
            if ((laneid == 1)) {
              istat = atomicAdd(xnorm, rv1);
            }
            __syncthreads() if ((xnorm == 0.0 /*_8*/ & alphai == 0.0 /*_8*/)) {
              if ((tid == 1)) {
                tau = 0.0 /*_8*/;
              }
            }
            else if ((tid == 1)) {
              xnorm = sqrt(xnorm);
              rv1 = abs(alphar);
              rv2 = abs(alphai);
              // ! not taking abs of xnorm
              scal = max(rv1, rv2, xnorm);
              invscal = (1.e0 / scal);
              rv1 = (rv1 * invscal);
              rv2 = (rv2 * invscal);
              xnorm = (xnorm * invscal);
              beta = -sign((scal * sqrt((rv1 * rv1 + rv2 * rv2 + xnorm * xnorm))), alphar);
              tau = make_doubleComplex(((beta - alphar) / beta), (-alphai / beta));
              // !zladiv
              rv1 = make_double((alpha_s - beta));
              rv2 = dimag[_idx((alpha_s - beta))];
              if ((abs(rv2) < abs(rv1))) {
                xnorm = (rv2 / rv1);
                invscal = (1.e0 / (rv1 + rv2 * xnorm));
                alpha_s = make_doubleComplex(invscal, (-xnorm * invscal));

              } else {
                xnorm = (rv1 / rv2);
                invscal = (1.e0 / (rv2 + rv1 * xnorm));
                alpha_s = make_doubleComplex((xnorm * invscal), -invscal);
              }
              e = beta;
              // ! store beta in e vector
            }
            __syncthreads() for (int i = tid; i <= (n - 1); i += (blockDim.x * blockDim.y)) {
              cv1 = x2[_idx(i)];
              if ((i <= (n - 2))) {
                cv1 = (alpha_s * cv1);

              } else if ((i == (n - 1))) {
                // !x(i) = 1.0_8
                cv1 = make_doubleComplex(1.0 /*_8*/, 0.0 /*_8*/);
              }
              x2[_idx(i)] = cv1;

            } // ! TODO could not parse:        endif
          }
        }
  }
    }

    extern "C" void launch_zher2_mv_zlarfg_kernel(dim3 * grid,
                                                  dim3 * block,
                                                  const int sharedMem,
                                                  hipStream_t stream,
                                                  int n,
                                                  int m,
                                                  int ldv,
                                                  int ldw,
                                                  int ldw2,
                                                  hipFloatComplex tau,
                                                  float e,
                                                  int finished) {
      hipLaunchKernelGGL((zher2_mv_zlarfg_kernel), *grid, *block, sharedMem, stream, n, m, ldv, ldw, ldw2, tau, e, finished);
    }
    // END zher2_mv_zlarfg_kernel

    // BEGIN stacked_zgemv_c
    /* Fortran original:
          use cudafor

          implicit none

          integer, value                                     :: M, N, ldv, ldw

          complex(8), dimension(ldv, M), device, intent(in)  :: V

          complex(8), dimension(ldw, M), device, intent(in)  :: W

          complex(8), dimension(N), device, intent(in)       :: x

          !DIR$ IGNORE_TKR z1, z2

          real(8), dimension(2*M), device                    :: z1, z2

          !complex(8), dimension(M), device, intent(in)        :: z1, z2

          !real(8), dimension(32), shared                     :: r_s

          !real(8), dimension(32), shared                     :: i_s

          integer :: i, j, tx, ty, istat

          complex(8) :: val

          real(8) :: rv1, rv2, iv1, iv2, xr, xi

          tx = threadIdx%x

          ty = threadIdx%y

          i = (blockIdx%y - 1)*blockDim%y + ty

          j = (blockIdx%x - 1)*blockDim%x + tx

          !if (i > 2*M .or. j > N) return

          if (i > 2*M) return

          val = x(j)

          xr = dble(val); xi = dimag(val)

          if (j > N) then

             !val = dcmplx(0,0)

             rv1 = 0.d0; iv1 = 0.d0

          else

             if (i > M) then

                val = W(j, i - M)

             else

                val = V(j, i)

             endif

             rv2 = dble(val); iv2 = dimag(val)

             rv1 = rv2*xr + iv2*xi

             iv1 = rv2*xi - iv2*xr

          endif

          !Partial sum within warps using shuffle

          rv2 = __shfl_down(rv1, 1)

          rv1 = rv1 + rv2

          rv2 = __shfl_down(rv1, 2)

          rv1 = rv1 + rv2

          rv2 = __shfl_down(rv1, 4)

          rv1 = rv1 + rv2

          rv2 = __shfl_down(rv1, 8)

          rv1 = rv1 + rv2

          rv2 = __shfl_down(rv1, 16)

          rv1 = rv1 + rv2

          !if (tx == 1) then

          !r_s(ty + k*blockDim%y) = rv1

          !r_s(ty) = rv1

          !endif

          !Partial sum within warps using shuffle

          iv2 = __shfl_down(iv1, 1)

          iv1 = iv1 + iv2

          iv2 = __shfl_down(iv1, 2)

          iv1 = iv1 + iv2

          iv2 = __shfl_down(iv1, 4)

          iv1 = iv1 + iv2

          iv2 = __shfl_down(iv1, 8)

          iv1 = iv1 + iv2

          iv2 = __shfl_down(iv1, 16)

          iv1 = iv1 + iv2

          !if (tx == 1) then

          !i_s(ty + k*blockDim%y) = iv1

          !i_s(ty) = iv1

          !endif

          !call syncthreads()

          !if (ty == 1 .and. i+tx-1 <= 2*M) then

          !  if (i+tx-1 > M) then

          !    istat = atomicadd(z2(2*(i+tx-1-M) - 1), r_s(tx))

          !    istat = atomicadd(z2(2*(i+tx-1-M)), i_s(tx))

          !  else

          !    istat = atomicadd(z1(2*(i+tx-1) - 1), r_s(tx))

          !    istat = atomicadd(z1(2*(i+tx-1)), i_s(tx))

          !  endif

          !endif

          if (tx == 1) then

             if (i > M) then

                istat = atomicadd(z2(2*(i - M) - 1), rv1)

                istat = atomicadd(z2(2*(i - M)), iv1)

             else

                istat = atomicadd(z1(2*i - 1), rv1)

                istat = atomicadd(z1(2*i), iv1)

             endif

          endif

          return

    */

    __global__ void stacked_zgemv_c(int m, int n, int ldv, int ldw, hipFloatComplex *v, hipFloatComplex *w, hipFloatComplex *x) {
      int i;
      int j;
      int tx;
      int ty;
      int istat;
      hipFloatComplex val;
      float rv1;
      float rv2;
      float iv1;
      float iv2;
      float xr;
      float xi;

      // !DIR$ IGNORE_TKR z1, z2
      // ! TODO could not parse:        real(8), dimension(2*m), device                    :: z1, z2
      // !complex(8), dimension(M), device, intent(in)        :: z1, z2
      // !real(8), dimension(32), shared                     :: r_s
      // !real(8), dimension(32), shared                     :: i_s
      tx = threadIdx.x;
      ty = threadIdx.y;
      i = ((blockIdx.y - 1) * blockDim.y + ty);
      j = ((blockIdx.x - 1) * blockDim.x + tx);
      // !if (i > 2*M .or. j > N) return
      if ((i > (2 * m))) {
        return;
        val = x[_idx(j)];
        xr = make_double(val);
        xi = dimag[_idx(val)];
        if ((j > n)) {
          // !val = dcmplx(0,0)
          rv1 = 0.e0;
          iv1 = 0.e0;

        } else if ((i > m)) {
          val = w[_idx_w(j, (i - m))];

        } else {
          val = v[_idx_v(j, i)];
        }
        rv2 = make_double(val);
        iv2 = dimag[_idx(val)];
        rv1 = (rv2 * xr + iv2 * xi);
        iv1 = (rv2 * xi - iv2 * xr);
        // ! TODO could not parse:        endif
        // !Partial sum within warps using shuffle
        rv2 = __shfl_down(rv1, 1);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 2);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 4);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 8);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 16);
        rv1 = (rv1 + rv2);
        // !if (tx == 1) then
        // !r_s(ty + k*blockDim%y) = rv1
        // !r_s(ty) = rv1
        // !endif
        // !Partial sum within warps using shuffle
        iv2 = __shfl_down(iv1, 1);
        iv1 = (iv1 + iv2);
        iv2 = __shfl_down(iv1, 2);
        iv1 = (iv1 + iv2);
        iv2 = __shfl_down(iv1, 4);
        iv1 = (iv1 + iv2);
        iv2 = __shfl_down(iv1, 8);
        iv1 = (iv1 + iv2);
        iv2 = __shfl_down(iv1, 16);
        iv1 = (iv1 + iv2);
        // !if (tx == 1) then
        // !i_s(ty + k*blockDim%y) = iv1
        // !i_s(ty) = iv1
        // !endif
        // !call __syncthreads()
        // !if (ty == 1 .and. i+tx-1 <= 2*M) then
        // !  if (i+tx-1 > M) then
        // !    istat = atomicAdd(z2(2*(i+tx-1-M) - 1), r_s(tx))
        // !    istat = atomicAdd(z2(2*(i+tx-1-M)), i_s(tx))
        // !  else
        // !    istat = atomicAdd(z1(2*(i+tx-1) - 1), r_s(tx))
        // !    istat = atomicAdd(z1(2*(i+tx-1)), i_s(tx))
        // !  endif
        // !endif
        if ((tx == 1)) {
          if ((i > m)) {
            istat = atomicAdd(z2[_idx((2 * (i - m) - 1))], rv1);
            istat = atomicAdd(z2[_idx((2 * (i - m)))], iv1);

          } else {
            istat = atomicAdd(z1[_idx((2 * i - 1))], rv1);
            istat = atomicAdd(z1[_idx((2 * i))], iv1);
          }
        }
        return;
      }
    }

    extern "C" void launch_stacked_zgemv_c(dim3 * grid,
                                           dim3 * block,
                                           const int sharedMem,
                                           hipStream_t stream,
                                           int m,
                                           int n,
                                           int ldv,
                                           int ldw,
                                           hipFloatComplex *v,
                                           hipFloatComplex *w,
                                           hipFloatComplex *x) {
      hipLaunchKernelGGL((stacked_zgemv_c), *grid, *block, sharedMem, stream, m, n, ldv, ldw, v, w, x);
    }
    // END stacked_zgemv_c

    // BEGIN stacked_zgemv_n
    /* Fortran original:
          use cudafor

          implicit none

          integer, value                                     :: M, N, ldv, ldw

          complex(8), dimension(ldv, N), device, intent(in)  :: V

          complex(8), dimension(ldw, N), device, intent(in)  :: W

          complex(8), dimension(N), device, intent(in)       :: z1, z2

          !DIR$ IGNORE_TKR y

          real(8), dimension(2*M), device                    :: y

          integer :: i, j, tx, ty, istat

          complex(8) :: val1, val2

          real(8) :: rv1, rv2, iv1, iv2, xr, xi

          tx = threadIdx%x

          ty = threadIdx%y

          i = (blockIdx%x - 1)*blockDim%x + tx

          j = (blockIdx%y - 1)*blockDim%y + ty

          if (i > M .or. j > 2*N) return

          if (j > N) then

             val1 = z2(j - N)

             val2 = V(i, j - N)

          else

             val1 = z1(j)

             val2 = W(i, j)

          endif

          xr = dble(val1); xi = dimag(val1)

          rv2 = dble(val2); iv2 = dimag(val2)

          rv1 = -rv2*xr + iv2*xi

          iv1 = -rv2*xi - iv2*xr

          istat = atomicadd(y(2*i - 1), rv1)

          istat = atomicadd(y(2*i), iv1)

          return


    */

    __global__ void
    stacked_zgemv_n(int m, int n, int ldv, int ldw, hipFloatComplex *v, hipFloatComplex *w, hipFloatComplex *z1, hipFloatComplex *z2) {
      int i;
      int j;
      int tx;
      int ty;
      int istat;
      hipFloatComplex val1;
      hipFloatComplex val2;
      float rv1;
      float rv2;
      float iv1;
      float iv2;
      float xr;
      float xi;

      // !DIR$ IGNORE_TKR y
      // ! TODO could not parse:        real(8), dimension(2*m), device                    :: y
      tx = threadIdx.x;
      ty = threadIdx.y;
      i = ((blockIdx.x - 1) * blockDim.x + tx);
      j = ((blockIdx.y - 1) * blockDim.y + ty);
      if ((i > m | j > (2 * n))) {
        return;
        if ((j > n)) {
          val1 = z2[_idx((j - n))];
          val2 = v[_idx_v(i, (j - n))];

        } else {
          val1 = z1[_idx(j)];
          val2 = w[_idx_w(i, j)];
        }
        xr = make_double(val1);
        xi = dimag[_idx(val1)];
        rv2 = make_double(val2);
        iv2 = dimag[_idx(val2)];
        rv1 = (-rv2 * xr + iv2 * xi);
        iv1 = (-rv2 * xi - iv2 * xr);
        istat = atomicAdd(y[_idx((2 * i - 1))], rv1);
        istat = atomicAdd(y[_idx((2 * i))], iv1);
        return;
      }
    }

    extern "C" void launch_stacked_zgemv_n(dim3 * grid,
                                           dim3 * block,
                                           const int sharedMem,
                                           hipStream_t stream,
                                           int m,
                                           int n,
                                           int ldv,
                                           int ldw,
                                           hipFloatComplex *v,
                                           hipFloatComplex *w,
                                           hipFloatComplex *z1,
                                           hipFloatComplex *z2) {
      hipLaunchKernelGGL((stacked_zgemv_n), *grid, *block, sharedMem, stream, m, n, ldv, ldw, v, w, z1, z2);
    }
    // END stacked_zgemv_n

    // BEGIN finish_w_col_kernel
    /* Fortran original:
          implicit none

          integer, value                               :: N

          complex(8), device                           :: tau

          complex(8), dimension(N), device, intent(in) :: x

          complex(8), dimension(N), device             :: y

          integer                                      :: tid, i, j, k, nb, istat, laneID

          real(8)                                      :: rv1, rv2, iv1, iv2, rsum, isum

          complex(8)                                   :: val, cv1, mytau

          real(8), shared                              :: alphar, alphai

          !complex(8), shared                          :: alpha

          complex(8)                                   :: alpha

          tid = threadIdx%x

          laneID = iand(tid, 31)

          if (tid == 1) then

             alphar = 0.0_8

             alphai = 0.0_8

          endif

          call syncthreads()

          rsum = 0.0_8

          isum = 0.0_8

          mytau = tau

          nb = ceiling(real(N)/blockDim%x) ! number of blocks down column

          i = tid

          do j = 1, nb

             ! All threads perform their product, zero if out of bounds

             if (i <= N) then

                val = dconjg(mytau*y(i))*x(i)

             else

                val = dcmplx(0., 0.)

             endif

             rv1 = dble(val); iv1 = dimag(val)

             rsum = rsum + rv1

             isum = isum + iv1

             i = i + blockDim%x

          end do

          ! Partial sum within warps using shuffle

          rv1 = rsum

          rv2 = __shfl_down(rv1, 1)

          rv1 = rv1 + rv2

          rv2 = __shfl_down(rv1, 2)

          rv1 = rv1 + rv2

          rv2 = __shfl_down(rv1, 4)

          rv1 = rv1 + rv2

          rv2 = __shfl_down(rv1, 8)

          rv1 = rv1 + rv2

          rv2 = __shfl_down(rv1, 16)

          rv1 = rv1 + rv2

          iv1 = isum

          iv2 = __shfl_down(iv1, 1)

          iv1 = iv1 + iv2

          iv2 = __shfl_down(iv1, 2)

          iv1 = iv1 + iv2

          iv2 = __shfl_down(iv1, 4)

          iv1 = iv1 + iv2

          iv2 = __shfl_down(iv1, 8)

          iv1 = iv1 + iv2

          iv2 = __shfl_down(iv1, 16)

          iv1 = iv1 + iv2

          if (laneID == 1) then

             istat = atomicadd(alphar, rv1)

             istat = atomicadd(alphai, iv1)

          endif

          call syncthreads()

          alpha = -dcmplx(0.5, 0.0)*mytau*dcmplx(alphar, alphai)

          do i = tid, N, blockDim%x

             y(i) = mytau*y(i) + alpha*x(i) !zaxpy

          end do


    */

    __global__ void finish_w_col_kernel(int n, hipFloatComplex tau, hipFloatComplex *x, hipFloatComplex *y) {
      int tid;
      int i;
      int j;
      int k;
      int nb;
      int istat;
      int laneid;
      float rv1;
      float rv2;
      float iv1;
      float iv2;
      float rsum;
      float isum;
      hipFloatComplex val;
      hipFloatComplex cv1;
      hipFloatComplex mytau;
      float alphar;
      float alphai;
      hipFloatComplex alpha;

      // !complex(8), shared                          :: alpha
      tid = threadIdx.x;
      laneid = iand(tid, 31);
      if ((tid == 1)) {
        alphar = 0.0 /*_8*/;
        alphai = 0.0 /*_8*/;
      }
      __syncthreads() rsum = 0.0 /*_8*/;
      isum = 0.0 /*_8*/;
      mytau = tau;
      nb = ceiling((make_float(n) / blockDim.x));
      // ! number of blocks down column
      i = tid;
      // ! TODO could not parse:        do j = 1, nb
      // ! All threads perform their product, zero if out of bounds
      // ! TODO could not parse:           if (i <= n) then
  val=(ASTConjugate:{
        'match' : '    attributes(global) subroutine finish_w_col_kernel(n, tau, x, y) implicit none integer, value ::n complex(8),
            device ::tau complex(8),
            dimension(n),
            device,
            intent(in)::x complex(8),
            dimension(n),
            device ::y

                integer ::tid,
            i,
            j,
            k,
            nb,
            istat,
            laneid real(8)::rv1,
            rv2,
            iv1,
            iv2,
            rsum,
            isum complex(8)::val,
            cv1,
            mytau

                real(8),
            shared ::alphar,
            alphai !complex(8),
            shared ::alpha complex(8)::alpha

                tid = threadIdx % x laneid = iand(tid, 31)

                          if (tid == 1) then alphar = 0.0_8 alphai = 0.0_8 endif

                              call
                              __syncthreads()

                                  rsum = 0.0_8 isum = 0.0_8 mytau = tau

                                      nb = ceiling(real(n) / blockDim % x) !number of blocks down column

                                          i = tid !TODO could not parse : do j = 1,
                nb

            !All threads perform their product,
                zero if out of bounds !TODO could not parse : if (i <= n) then val = dconjg(mytau * y(i)) * x(i) !TODO could not parse
            : else !TODO could not parse : val = dcmplx(0., 0.) !TODO could not parse : endif

                                                                                            rv1 = dble(val);
        iv1 = dimag(val)

            rsum = rsum + rv1 isum =
                       isum + iv1

                                  i = i + blockDim % x

                                              !TODO could not parse
            : end do

              !Partial sum within warps using shuffle rv1 = rsum rv2 = __shfl_down(rv1, 1) rv1 =
                                          rv1 + rv2 rv2 = __shfl_down(rv1, 2) rv1 =
                                              rv1 + rv2 rv2 = __shfl_down(rv1, 4) rv1 =
                                                  rv1 + rv2 rv2 = __shfl_down(rv1, 8) rv1 =
                                                      rv1 + rv2 rv2 = __shfl_down(rv1, 16) rv1 =
                                                          rv1 + rv2

                                                                    iv1 = isum iv2 = __shfl_down(iv1, 1) iv1 =
                                                              iv1 + iv2 iv2 = __shfl_down(iv1, 2) iv1 =
                                                                  iv1 + iv2 iv2 = __shfl_down(iv1, 4) iv1 =
                                                                      iv1 + iv2 iv2 = __shfl_down(iv1, 8) iv1 =
                                                                          iv1 + iv2 iv2 = __shfl_down(iv1, 16) iv1 =
                                                                              iv1 + iv2

                                                                              if (laneid == 1) then istat = atomicAdd(alphar, rv1) istat =
                                                                                  atomicAdd(alphai, iv1) endif

                                                                                      call
                                                                                      __syncthreads()

                                                                                          alpha = -dcmplx(0.5, 0.0) * mytau *
                                                                                                  dcmplx(alphar, alphai)

                                                                                                      do i = tid,
        n,
        blockDim % x y(i) =
            mytau * y(i) +
            alpha * x(i) !zaxpy end do

                end subroutine finish_w_col_kernel

                ', ' location ': 1262, ' ref ': <fort2hip.translator.ASTArithmeticExpression object at 0x7f5901ff2da0>, ' kind': None}*x[_idx(i)]);
                // ! TODO could not parse:           else
                // ! TODO could not parse:              val = dcmplx(0., 0.)
                // ! TODO could not parse:           endif
                rv1 = make_double(val);
        iv1 = dimag[_idx(val)];
        rsum = (rsum + rv1);
        isum = (isum + iv1);
        i = (i + blockDim.x);
        // ! TODO could not parse:        end do
        // ! Partial sum within warps using shuffle
        rv1 = rsum;
        rv2 = __shfl_down(rv1, 1);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 2);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 4);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 8);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 16);
        rv1 = (rv1 + rv2);
        iv1 = isum;
        iv2 = __shfl_down(iv1, 1);
        iv1 = (iv1 + iv2);
        iv2 = __shfl_down(iv1, 2);
        iv1 = (iv1 + iv2);
        iv2 = __shfl_down(iv1, 4);
        iv1 = (iv1 + iv2);
        iv2 = __shfl_down(iv1, 8);
        iv1 = (iv1 + iv2);
        iv2 = __shfl_down(iv1, 16);
        iv1 = (iv1 + iv2);
        if ((laneid == 1)) {
          istat = atomicAdd(alphar, rv1);
          istat = atomicAdd(alphai, iv1);
        }
        __syncthreads() alpha = (-make_doubleComplex(0.5, 0.0) * mytau * make_doubleComplex(alphar, alphai));
        for (int i = tid; i <= n; i += blockDim.x) {
          y[_idx(i)] = (mytau * y[_idx(i)] + alpha * x[_idx(i)]);
          // !zaxpy
        }
}

extern "C" void launch_finish_w_col_kernel(dim3* grid, dim3* block, const int sharedMem, hipStream_t stream,int n,hipFloatComplex tau,hipFloatComplex * x,hipFloatComplex * y) {
        hipLaunchKernelGGL((finish_w_col_kernel), *grid, *block, sharedMem, stream, n, tau, x, y);
}
// END finish_w_col_kernel

 

// BEGIN stacked_zgemv_n_finish_w
  /* Fortran original: 
        use cudafor

        implicit none

        integer, value                                     :: M, N, ldv, ldw

        complex(8), dimension(ldv, N), device, intent(in)  :: V

        complex(8), dimension(ldw, N), device, intent(in)  :: W

        complex(8), dimension(N), device, intent(in)       :: z1, z2

        !DIR$ IGNORE_TKR y

        real(8), dimension(2*M), device                    :: y

        complex(8), device                                 :: tau

        complex(8), dimension(M), device, intent(in)       :: x

        complex(8), dimension(M), device                   :: y2

        integer, device                                    :: finished

        integer :: i, j, tx, ty, istat, nBlocks, tid, laneID, nb

        integer, shared :: nFinished

        complex(8) :: val1, val2, mytau, alpha

        real(8) :: rv1, rv2, iv1, iv2, xr, xi, rsum, isum

        real(8), shared :: alphar, alphai

        tx = threadIdx%x

        ty = threadIdx%y

        i = (blockIdx%x - 1)*blockDim%x + tx

        j = (blockIdx%y - 1)*blockDim%y + ty

        nBlocks = gridDim%x*gridDim%y

        if (i <= M .and. j <= 2*N) then

           if (j > N) then

              val1 = z2(j - N)

              val2 = V(i, j - N)

           else

              val1 = z1(j)

              val2 = W(i, j)

           endif

           xr = dble(val1); xi = dimag(val1)

           rv2 = dble(val2); iv2 = dimag(val2)

           rv1 = -rv2*xr + iv2*xi

           iv1 = -rv2*xi - iv2*xr

           istat = atomicadd(y(2*i - 1), rv1)

           istat = atomicadd(y(2*i), iv1)

        endif

        call threadfence()

        nFinished = 0

        call syncthreads()

        if (tx + ty == 2) nFinished = atomicinc(finished, nBlocks - 1)

        call syncthreads()

        if (nFinished < nBlocks - 1) return

        ! Begin finish_W_col work with last block

        tid = threadIdx%x + (threadIdx%y - 1)*blockDim%x

        laneID = iand(tid, 31)

        if (tid == 1) then

           alphar = 0.0_8

           alphai = 0.0_8

        endif

        call syncthreads()

        rsum = 0.0_8

        isum = 0.0_8

        mytau = tau

        nb = ceiling(real(M)/(blockDim%x*blockDim%y)) ! number of blocks down column

        i = tid

        do j = 1, nb

           ! All threads perform their product, zero if out of bounds

           if (i <= M) then

              val1 = dconjg(mytau*y2(i))*x(i)

           else

              val1 = dcmplx(0., 0.)

           endif

           rv1 = dble(val1); iv1 = dimag(val1)

           rsum = rsum + rv1

           isum = isum + iv1

           i = i + blockDim%x*blockDim%y

        end do

        ! Partial sum within warps using shuffle

        rv1 = rsum

        rv2 = __shfl_down(rv1, 1)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 2)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 4)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 8)

        rv1 = rv1 + rv2

        rv2 = __shfl_down(rv1, 16)

        rv1 = rv1 + rv2

        iv1 = isum

        iv2 = __shfl_down(iv1, 1)

        iv1 = iv1 + iv2

        iv2 = __shfl_down(iv1, 2)

        iv1 = iv1 + iv2

        iv2 = __shfl_down(iv1, 4)

        iv1 = iv1 + iv2

        iv2 = __shfl_down(iv1, 8)

        iv1 = iv1 + iv2

        iv2 = __shfl_down(iv1, 16)

        iv1 = iv1 + iv2

        if (laneID == 1) then

           istat = atomicadd(alphar, rv1)

           istat = atomicadd(alphai, iv1)

        endif

        call syncthreads()

        alpha = -dcmplx(0.5, 0.0)*mytau*dcmplx(alphar, alphai)

        do i = tid, M, blockDim%x*blockDim%y

           y2(i) = mytau*y2(i) + alpha*x(i) !zaxpy

        end do


  */
  

__global__ void stacked_zgemv_n_finish_w(int m,int n,int ldv,int ldw,hipFloatComplex * v,hipFloatComplex * w,hipFloatComplex * z1,hipFloatComplex * z2,hipFloatComplex tau,hipFloatComplex * x,hipFloatComplex * y2,int finished) {
        int i;
        int j;
        int tx;
        int ty;
        int istat;
        int nblocks;
        int tid;
        int laneid;
        int nb;
        int nfinished;
        hipFloatComplex val1;
        hipFloatComplex val2;
        hipFloatComplex mytau;
        hipFloatComplex alpha;
        float rv1;
        float rv2;
        float iv1;
        float iv2;
        float xr;
        float xi;
        float rsum;
        float isum;
        float alphar;
        float alphai;

        // !DIR$ IGNORE_TKR y
        // ! TODO could not parse:        real(8), dimension(2*m), device                    :: y
        tx = threadIdx.x;
        ty = threadIdx.y;
        i = ((blockIdx.x - 1) * blockDim.x + tx);
        j = ((blockIdx.y - 1) * blockDim.y + ty);
        nblocks = (gridDim.x * gridDim.y);
        if ((i <= m & j <= (2 * n))) {
          if ((j > n)) {
            val1 = z2[_idx((j - n))];
            val2 = v[_idx_v(i, (j - n))];

          } else {
            val1 = z1[_idx(j)];
            val2 = w[_idx_w(i, j)];
          }
          xr = make_double(val1);
          xi = dimag[_idx(val1)];
          rv2 = make_double(val2);
          iv2 = dimag[_idx(val2)];
          rv1 = (-rv2 * xr + iv2 * xi);
          iv1 = (-rv2 * xi - iv2 * xr);
          istat = atomicAdd(y[_idx((2 * i - 1))], rv1);
          istat = atomicAdd(y[_idx((2 * i))], iv1);
        }
        threadfence() nfinished = 0;
        __syncthreads() if (((tx + ty) == 2)) {
          nfinished = atomicInc(finished, (nblocks - 1));
          __syncthreads() if ((nfinished < (nblocks - 1))) {
            return; // ! Begin finish_W_col work with last block
            tid = (threadIdx.x + (threadIdx.y - 1) * blockDim.x);
            laneid = iand(tid, 31);
            if ((tid == 1)) {
              alphar = 0.0 /*_8*/;
              alphai = 0.0 /*_8*/;
            }
            __syncthreads() rsum = 0.0 /*_8*/;
            isum = 0.0 /*_8*/;
            mytau = tau;
            nb = ceiling((make_float(m) / (blockDim.x * blockDim.y)));
            // ! number of blocks down column
            i = tid;
            // ! TODO could not parse:        do j = 1, nb
            // ! All threads perform their product, zero if out of bounds
            // ! TODO could not parse:           if (i <= m) then
  val1=(ASTConjugate:{
              'match'
                  : '    attributes(global) subroutine stacked_zgemv_n_finish_w(m, n, v, ldv, w, ldw, z1, z2, y, tau, x, y2, finished) use cudafor implicit
                      none integer,
                  value ::m,
                  n,
                  ldv,
                  ldw complex(8),
                  dimension(ldv, n),
                  device,
                  intent(in)::v complex(8),
                  dimension(ldw, n),
                  device,
                  intent(in)::w complex(8),
                  dimension(n),
                  device,
                  intent(in)::z1,
                  z2 !DIR$ IGNORE_TKR y !TODO could not parse
                  : real(8),
                    dimension(2 * m),
                    device ::y complex(8),
                    device ::tau complex(8),
                    dimension(m),
                    device,
                    intent(in)::x complex(8),
                    dimension(m),
                    device ::y2 integer,
                    device ::finished

                        integer ::i,
                    j,
                    tx,
                    ty,
                    istat,
                    nblocks,
                    tid,
                    laneid,
                    nb integer,
                    shared ::nfinished complex(8)::val1,
                    val2,
                    mytau,
                    alpha real(8)::rv1,
                    rv2,
                    iv1,
                    iv2,
                    xr,
                    xi,
                    rsum,
                    isum real(8),
                    shared ::alphar,
                    alphai

                        tx = threadIdx % x ty = threadIdx % y

                                                                i = (blockIdx % x - 1) *blockDim % x + tx j =
                                                                        (blockIdx % y - 1) *blockDim % y + ty

                                                                                                               nblocks =
                                                                            gridDim % x * gridDim %
                                                                            y

                                                                            if (i <= m.and.j <= 2 * n) then if (j > n) then val1 =
                                                                                z2(j - n) val2 = v(i, j - n) else val1 = z1(j) val2 =
                                                                                    w(i, j) endif xr = dble(val1);
              xi = dimag(val1) rv2 = dble(val2);
              iv2 = dimag(val2)

                  rv1 = -rv2 *xr + iv2 *xi iv1 = -rv2 *xi - iv2 *xr

                                                                istat = atomicAdd(y(2 * i - 1), rv1) istat = atomicAdd(y(2 * i), iv1) endif

                                                     call
                                                     threadfence()

                                                         nfinished =
                                                             0 call __syncthreads() if (tx + ty == 2) nfinished =
                                                                 atomicInc(finished, nblocks - 1) call __syncthreads()

                                                                     if (nfinished < nblocks - 1) return

                  !Begin finish_W_col work with last block tid =
                      threadIdx % x + (threadIdx % y - 1) *blockDim % x laneid = iand(tid, 31)

                          if (tid == 1) then alphar = 0.0_8 alphai = 0.0_8 endif

                              call
                              __syncthreads()

                                  rsum = 0.0_8 isum = 0.0_8 mytau = tau

                                      nb = ceiling(real(m) / (blockDim % x * blockDim % y)) !number of blocks down column

                                          i = tid !TODO could not parse : do j = 1,
              nb

                  !All threads perform their product,
              zero if out of bounds !TODO could not parse : if (i <= m) then val1 = dconjg(mytau * y2(i)) * x(i) !TODO could not parse
                  : else !TODO could not parse : val1 = dcmplx(0., 0.) !TODO could not parse : endif

                                                                                                   rv1 = dble(val1);
              iv1 = dimag(val1)

                  rsum = rsum + rv1 isum =
                             isum + iv1

                                        i = i + blockDim % x *blockDim %
                                                    y

                                                    !TODO could not parse : end do

                                                                            !Partial sum within warps using shuffle rv1 = rsum rv2 =
                                                __shfl_down(rv1, 1) rv1 =
                                                    rv1 + rv2 rv2 = __shfl_down(rv1, 2) rv1 =
                                                        rv1 + rv2 rv2 = __shfl_down(rv1, 4) rv1 =
                                                            rv1 + rv2 rv2 = __shfl_down(rv1, 8) rv1 =
                                                                rv1 + rv2 rv2 = __shfl_down(rv1, 16) rv1 =
                                                                    rv1 + rv2

                                                                              iv1 = isum iv2 = __shfl_down(iv1, 1) iv1 =
                                                                        iv1 + iv2 iv2 = __shfl_down(iv1, 2) iv1 =
                                                                            iv1 + iv2 iv2 = __shfl_down(iv1, 4) iv1 =
                                                                                iv1 + iv2 iv2 = __shfl_down(iv1, 8) iv1 =
                                                                                    iv1 + iv2 iv2 = __shfl_down(iv1, 16) iv1 =
                                                                                        iv1 + iv2

                                                                                        if (laneid == 1) then istat = atomicAdd(alphar, rv1)
                                                                                            istat = atomicAdd(alphai, iv1) endif

                                                                                                call
                                                                                                __syncthreads()

                                                                                                    alpha = -dcmplx(0.5, 0.0) * mytau *
                                                                                                            dcmplx(alphar, alphai)

                                                                                                                do i = tid,
              m,
              blockDim % x * blockDim % y y2(i) =
                  mytau * y2(i) +
                  alpha * x(i) !zaxpy end do

                      end subroutine stacked_zgemv_n_finish_w

                      ', ' location ': 2504, ' ref ': <fort2hip.translator.ASTArithmeticExpression object at 0x7f5902023780>, ' kind': None}*x[_idx(i)]);
                      // ! TODO could not parse:           else
                      // ! TODO could not parse:              val1 = dcmplx(0., 0.)
                      // ! TODO could not parse:           endif
                      rv1 = make_double(val1);
              iv1 = dimag[_idx(val1)];
              rsum = (rsum + rv1);
              isum = (isum + iv1);
              i = (i + blockDim.x * blockDim.y);
              // ! TODO could not parse:        end do
              // ! Partial sum within warps using shuffle
              rv1 = rsum;
              rv2 = __shfl_down(rv1, 1);
              rv1 = (rv1 + rv2);
              rv2 = __shfl_down(rv1, 2);
              rv1 = (rv1 + rv2);
              rv2 = __shfl_down(rv1, 4);
              rv1 = (rv1 + rv2);
              rv2 = __shfl_down(rv1, 8);
              rv1 = (rv1 + rv2);
              rv2 = __shfl_down(rv1, 16);
              rv1 = (rv1 + rv2);
              iv1 = isum;
              iv2 = __shfl_down(iv1, 1);
              iv1 = (iv1 + iv2);
              iv2 = __shfl_down(iv1, 2);
              iv1 = (iv1 + iv2);
              iv2 = __shfl_down(iv1, 4);
              iv1 = (iv1 + iv2);
              iv2 = __shfl_down(iv1, 8);
              iv1 = (iv1 + iv2);
              iv2 = __shfl_down(iv1, 16);
              iv1 = (iv1 + iv2);
              if ((laneid == 1)) {
                istat = atomicAdd(alphar, rv1);
                istat = atomicAdd(alphai, iv1);
              }
              __syncthreads() alpha = (-make_doubleComplex(0.5, 0.0) * mytau * make_doubleComplex(alphar, alphai));
              for (int i = tid; i <= m; i += (blockDim.x * blockDim.y)) {
                y2[_idx(i)] = (mytau * y2[_idx(i)] + alpha * x[_idx(i)]);
                // !zaxpy
              }
  }
          }
        }

        extern "C" void launch_stacked_zgemv_n_finish_w(dim3 * grid,
                                                        dim3 * block,
                                                        const int sharedMem,
                                                        hipStream_t stream,
                                                        int m,
                                                        int n,
                                                        int ldv,
                                                        int ldw,
                                                        hipFloatComplex *v,
                                                        hipFloatComplex *w,
                                                        hipFloatComplex *z1,
                                                        hipFloatComplex *z2,
                                                        hipFloatComplex tau,
                                                        hipFloatComplex *x,
                                                        hipFloatComplex *y2,
                                                        int finished) {
          hipLaunchKernelGGL((stacked_zgemv_n_finish_w),
                             *grid,
                             *block,
                             sharedMem,
                             stream,
                             m,
                             n,
                             ldv,
                             ldw,
                             v,
                             w,
                             z1,
                             z2,
                             tau,
                             x,
                             y2,
                             finished);
        }
        // END stacked_zgemv_n_finish_w
