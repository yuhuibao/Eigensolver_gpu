// This file was generated by gpufort

#include "hip/hip_complex.h"
#include "hip/hip_runtime.h"
#include "hip/math_functions.h"
#include <cstdio>

namespace {
// make float
float make_float(short int a) { return static_cast<float>(a); }
float make_float(unsigned short int a) { return static_cast<float>(a); }
float make_float(unsigned int a) { return static_cast<float>(a); }
float make_float(int a) { return static_cast<float>(a); }
float make_float(long int a) { return static_cast<float>(a); }
float make_float(unsigned long int a) { return static_cast<float>(a); }
float make_float(long long int a) { return static_cast<float>(a); }
float make_float(unsigned long long int a) { return static_cast<float>(a); }
float make_float(signed char a) { return static_cast<float>(a); }
float make_float(unsigned char a) { return static_cast<float>(a); }
float make_float(float a) { return static_cast<float>(a); }
float make_float(double a) { return static_cast<float>(a); }
float make_float(long double a) { return static_cast<float>(a); }
float make_float(hipFloatComplex &a) { return static_cast<float>(a.x); }
float make_float(hipDoubleComplex &a) { return static_cast<float>(a.x); }
// make double
double make_double(short int a) { return static_cast<double>(a); }
double make_double(unsigned short int a) { return static_cast<double>(a); }
double make_double(unsigned int a) { return static_cast<double>(a); }
double make_double(int a) { return static_cast<double>(a); }
double make_double(long int a) { return static_cast<double>(a); }
double make_double(unsigned long int a) { return static_cast<double>(a); }
double make_double(long long int a) { return static_cast<double>(a); }
double make_double(unsigned long long int a) { return static_cast<double>(a); }
double make_double(signed char a) { return static_cast<double>(a); }
double make_double(unsigned char a) { return static_cast<double>(a); }
double make_double(float a) { return static_cast<double>(a); }
double make_double(double a) { return static_cast<double>(a); }
double make_double(long double a) { return static_cast<double>(a); }
double make_double(hipFloatComplex &a) { return static_cast<double>(a.x); }
__device__ double make_double(hipDoubleComplex &a) { return static_cast<double>(a.x); }
// conjugate complex type
hipFloatComplex conj(hipFloatComplex &c) { return hipConjf(c); }
hipDoubleComplex conj(hipDoubleComplex &z) { return hipConj(z); }
__device__ double dimag(hipDoubleComplex &a) { return static_cast<double>(a.y); }
// TODO Add the following functions:
// - sign(x,y) = sign(y) * |x| - sign transfer function
// ...
} // namespace
#define divideAndRoundUp(x, y) ((x) / (y) + ((x) % (y) != 0))

// BEGIN zhemv_gpu
/* Fortran original:
    use cudafor

  contains

#define BX 32
#define BY 8
#define NTILES 4

  attributes(global) subroutine zhemv_gpu(N, A, lda, x, y)
    use cudafor
    implicit none

    integer, value                                    :: N, lda
    complex(8), dimension(lda, N), device, intent(in) :: A
    complex(8), dimension(N), device, intent(in)      :: x
    !DIR$ IGNORE_TKR y
    real(8), dimension(2*N), device                   :: y 

    real(8), dimension(BX+1, BX), shared              :: Ar_s
    real(8), dimension(BX+1, BX), shared              :: Ai_s
    real(8), dimension(BX), shared                    :: r_s
    real(8), dimension(BX), shared                    :: i_s

    integer                                           :: tx, ty, ii, jj, i, j, k, istat
    real(8)                                           :: rv1, rv2, iv1, iv2, myrsum, myisum
    real(8)                                           :: Ar, Ai, xrl, xil
    complex(8)                                        :: val

    ! ii,jj is index of top left corner of block
    ii = (blockIdx%y-1) * blockDim%x + 1
    !print*, "ii ", ii

    myrsum = 0.0_8
    myisum = 0.0_8

    tx = threadIdx%x
    ty = threadIdx%y

    if (ii + (blockIdx%x-1)*blockDim%x > N) return


    i = ii + tx - 1
    if (i <= N) then
      val =  x(i) ! read part of x for lower triangular multiply
    endif
    xrl = dble(val)
    xil = dimag(val)

    ! Loop over columns (skip all lower triangular blocks)
    do jj = ii + (blockIdx%x-1)*blockDim%x, N, gridDim%x*blockDim%x
      j = jj + ty - 1

      ! Load block into shared memory
      ! CASE 1: Diagonal block
      if (ii == jj) then

        ! Load full block into shared memory
        do k = 0,NTILES-1
          if (i <= N .and. j + k * blockDim%y <= N) then
            val = A(i, j + k*blockDim%y)
            Ar_s(tx, ty + k * blockDim%y) = dble(val)
            Ai_s(tx, ty + k * blockDim%y) = dimag(val)
          endif
        end do
        
        call syncthreads()

        ! Reflect to populate lower triangular part with true values of A
        do k = 0,NTILES-1
          if (tx > ty + k * blockDim%y) then
            Ar_s(tx, ty + k * blockDim%y) = Ar_s(ty + k * blockDim%y, tx)
            Ai_s(tx, ty + k * blockDim%y) = -Ai_s(ty + k * blockDim%y, tx)
          endif
        end do

        call syncthreads()

        do k = 0,NTILES-1
          if (i <= N .and. j + k * blockDim%y <= N ) then
            Ar = Ar_s(tx, ty + k * blockDim%y); Ai = Ai_s(tx, ty + k * blockDim%y)
            val = x(j + k*blockDim%y)
            rv1 = dble(val) ; iv1 = dimag(val)
            myrsum = myrsum + Ar * rv1 - Ai * iv1
            myisum = myisum + Ar * iv1 + Ai * rv1
          endif
        end do

        !call syncthreads()

      ! CASE 2: Upper triangular block
      else if (ii < jj) then
        do k = 0,NTILES-1
          if (j + k * blockDim%y <= N) then
            val = A(i, j + k * blockDim%y)
            Ar = dble(val)
            Ai = dimag(val)
          endif

          if (i <= N .and. j + k * blockDim%y <= N ) then
            val = x(j + k*blockDim%y)
            rv1 = dble(val) ; iv1 = dimag(val)
            myrsum = myrsum + Ar * rv1 - Ai * iv1
            myisum = myisum + Ar * iv1 + Ai * rv1
          endif

          ! Perform product for symmetric lower block here
          ! Don't need sync threads since thread is accessing own value
          !call syncthreads()
          if (i <= N .and. j + k*blockDim%y <= N) then
            rv1 = Ar * xrl + Ai * xil
            iv1 = Ar * xil - Ai * xrl
          else
            rv1 = 0.0_8
            iv1 = 0.0_8
          endif

          !Partial sum within warps using shuffle
          rv2 = __shfl_down(rv1,1)
          rv1 = rv1 + rv2
          rv2 = __shfl_down(rv1,2)
          rv1 = rv1 + rv2
          rv2 = __shfl_down(rv1,4)
          rv1 = rv1 + rv2
          rv2 = __shfl_down(rv1,8)
          rv1 = rv1 + rv2
          rv2 = __shfl_down(rv1,16)
          rv1 = rv1 + rv2

          if (tx == 1) then
            r_s(ty + k*blockDim%y) = rv1
          endif

          !Partial sum within warps using shuffle
          iv2 = __shfl_down(iv1,1)
          iv1 = iv1 + iv2
          iv2 = __shfl_down(iv1,2)
          iv1 = iv1 + iv2
          iv2 = __shfl_down(iv1,4)
          iv1 = iv1 + iv2
          iv2 = __shfl_down(iv1,8)
          iv1 = iv1 + iv2
          iv2 = __shfl_down(iv1,16)
          iv1 = iv1 + iv2

          if (tx == 1) then
            i_s(ty + k*blockDim%y) = iv1
          endif
        enddo

        call syncthreads()

        if (ty == 1 .and. jj+tx-1 <= N) then
          istat = atomicadd(y(2*(jj + tx -1)-1), r_s(tx))
          istat = atomicadd(y(2*(jj + tx -1)), i_s(tx))
        endif
        !call syncthreads()

      endif

      call syncthreads()

    end do

    if (i <= N) then
      istat = atomicadd(y(2*i - 1), myrsum)
      istat = atomicadd(y(2*i), myisum)
    endif
    
  end subroutine zhemv_gpu


*/

__global__ void zhemv_gpu(int n,
                          int lda,
                          hipDoubleComplex *a,
                          const int a_n1,
                          const int a_n2,
                          const int a_lb1,
                          const int a_lb2,
                          hipDoubleComplex *x,
                          const int x_n1,
                          const int x_lb1,
                          double *y,
                          const int y_n1,
                          const int y_lb1) {

int ar_s_n1, ar_s_lb1, ar_s_n2, ar_s_lb2, r_s_n1, r_s_lb1;
int ai_s_n1, ai_s_lb1, ai_s_n2, ai_s_lb2, i_s_n1, i_s_lb1;
#undef _idx_a
#define _idx_a(a, b) ((a - (a_lb1)) + a_n1 * (b - (a_lb2)))
#undef _idx_x
#define _idx_x(a) ((a - (x_lb1)))
#undef _idx_y
#define _idx_y(a) ((a - (y_lb1)))
#undef _idx_ar_s
#define _idx_ar_s(a, b) ((a - (ar_s_lb1)) + ar_s_n1 * (b - (ar_s_lb2)))
#undef _idx_r_s
#define _idx_r_s(a) ((a - (r_s_lb1))) 
#undef _idx_ai_s
#define _idx_ai_s(a, b) ((a - (ai_s_lb1)) + ai_s_n1 * (b - (ai_s_lb2)))
#undef _idx_i_s
#define _idx_i_s(a) ((a - (i_s_lb1))) 
#define BX 32
#define BY 8
#define NTILES 4
  // ! TODO could not parse:      real(8), dimension(bx + 1, bx), shared              :: ar_s
  // ! TODO could not parse:      real(8), dimension(bx), shared                    :: r_s
  __shared__ double r_s[BX];
  __shared__ double* ar_s;
  __shared__ double i_s[BX];
  __shared__ double* ai_s;
  ar_s_n1 = BX + 1;
  ar_s_n2 = BX;
  ar_s_lb1 = 1;
  ar_s_lb2 = 1;
  r_s_n1 = BX;
  r_s_lb1 = 1;
  int tx;
  int ty;
  int ii;
  int jj;
  int i;
  int j;
  int k;
  int istat;
  double rv1,iv1;
  double rv2,iv2;
  double myrsum,myisum;
  double ar,ai;
  double xrl,xil;
  hipDoubleComplex val;
  // ! ii,jj is index of top left corner of block
  ii = ((blockIdx.y) * blockDim.x + 1);
  myrsum = 0.0 /*_8*/;
  myisum = 0.0 /*_8*/; 
  tx = threadIdx.x + 1;
  ty = threadIdx.y + 1;
  if (((ii + (blockIdx.x) * blockDim.x) > n)) {
    return;
  }
  i = (ii + tx - 1);
  if ((i <= n)) {
    val = x[_idx_x(i)];
    // ! read part of x for lower triangular multiply
  }
  xrl = make_double(val);
  xil = dimag(val);
  // ! Loop over columns (skip all lower triangular blocks)
  for (int jj = (ii + (blockIdx.x - 1) * blockDim.x); jj <= n; jj += (gridDim.x * blockDim.x)) {
    j = (jj + ty - 1);
    // ! Load block into shared memory
    // ! CASE 1: Diagonal block
    if (ii == jj) {
      // ! Load full block into shared memory
      for (int k = 0; k <= (NTILES - 1); k += 1) {
        if ((i <= n && (j + k * blockDim.y) <= n)) {
            val = a[_idx_a(i,j + k*blockDim.y)];
          ar_s[_idx_ar_s(tx, (ty + k * blockDim.y))] = make_double(val);
         ai_s[_idx_ai_s(tx, (ty + k * blockDim.y))] = dimag(val); 
        }
      }
      __syncthreads(); // ! Reflect to populate lower triangular part with true values of A
          for (int k = 0; k <= (NTILES - 1); k += 1) {
        if ((tx > (ty + k * blockDim.y))) {
          ar_s[_idx_ar_s(tx, (ty + k * blockDim.y))] = ar_s[_idx_ar_s((ty + k * blockDim.y), tx)];
          ai_s[_idx_ai_s(tx, (ty + k * blockDim.y))] = -ai_s[_idx_ar_s((ty + k * blockDim.y), tx)];
        }
      }
      __syncthreads();
       for (int k = 0; k <= (NTILES - 1); k += 1) {
        if ((i <= n && (j + k * blockDim.y) <= n)) {
            ar = ar_s[_idx_ar_s(tx, ty + k * blockDim.y)];
            ai =ai_s[_idx_ai_s(tx, ty + k * blockDim.y)];
            rv1 =make_double(val); 
            iv1 =dimag(val); 
          myrsum = myrsum + ar * rv1 - ai * iv1;
          myisum = myisum + ar * iv1 + ai * rv1;
        }

      } // !call __syncthreads()
      // ! CASE 2: Upper triangular block

    } else if ((ii < jj)) {
      for (int k = 0; k <= (NTILES - 1); k += 1) {
        if (((j + k * blockDim.y) <= n)) {
          val = a[_idx_a(i, j + k * blockDim.y)];
          ar =make_double(val); 
          ai =dimag(val); 
        }
        if ((i <= n && (j + k * blockDim.y) <= n)) {
          val = x[_idx_x(j + k*blockDim.y)];
          rv1 =make_double(val); 
          iv1 = dimag(val); 
          myrsum = myrsum + ar * rv1 - ai * iv1;
          myisum = myisum + ar * iv1 + ai * rv1;

        }
        // ! Perform product for symmetric lower block here
        if ((i <= n && (j + k * blockDim.y) <= n)) {
          rv1 = ar * xrl + ai * xil;
          iv1 = ar * xil - ai * xrl;

        } else {
          rv1 = 0.0 /*_8*/;
          iv1 = 0.0 /*_8*/;
        }
        // !Partial sum within warps using shuffle
        rv2 = __shfl_down(rv1, 1);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 2);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 4);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 8);
        rv1 = (rv1 + rv2);
        rv2 = __shfl_down(rv1, 16);
        rv1 = (rv1 + rv2);
        if (tx == 1) {
          r_s[_idx_r_s((ty + k * blockDim.y))] = rv1;
        }
        iv2 = __shfl_down(iv1,1);
          iv1 = iv1 + iv2;
          iv2 = __shfl_down(iv1,2);
          iv1 = iv1 + iv2;
          iv2 = __shfl_down(iv1,4);
          iv1 = iv1 + iv2;
          iv2 = __shfl_down(iv1,8);
          iv1 = iv1 + iv2;
          iv2 = __shfl_down(iv1,16);
          iv1 = iv1 + iv2;
         if (tx == 1) {
          i_s[_idx_i_s((ty + k * blockDim.y))] = iv1;
        } 
      }
      __syncthreads(); 
      if ((ty == 1 && (jj + tx - 1) <= n)) { 
          istat = atomicAdd(y + _idx_y((2*(jj + tx - 1)-1)*8), r_s[_idx_r_s(tx)]); 
          istat = atomicAdd(y + _idx_y(2*(jj + tx - 1)*8), i_s[_idx_i_s(tx)]); 
      }
      // !call __syncthreads()
    }
    __syncthreads();
  }
  if ((i <= n)) {
    istat = atomicAdd(y + _idx_y(2*i-1)*8, myrsum);
    istat = atomicAdd(y + _idx_y(2*i)*8, myisum);
  }
}

extern "C" void launch_zhemv_gpu(dim3 *grid,
                                 dim3 *block,
                                 const int sharedMem,
                                 hipStream_t stream,
                                 int n,
                                 int lda,
                                 hipDoubleComplex *a,
                                 const int a_n1,
                                 const int a_n2,
                                 const int a_lb1,
                                 const int a_lb2,
                                 hipDoubleComplex *x,
                                 const int x_n1,
                                 const int x_lb1,
                                 double *y,
                                 const int y_n1,
                                 const int y_lb1) {
  hipLaunchKernelGGL((zhemv_gpu), *grid, *block, sharedMem, stream, n, lda, a, a_n1, a_n2, a_lb1, a_lb2, x, x_n1, x_lb1, y, y_n1, y_lb1);
}
// END dsymv_gpu
